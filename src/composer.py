#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Neural composer: Play and edit music generated by the trained model.
"""

import os
import argparse
import math
import wave

import numpy as np
import pyaudio
import pygame

import src.midi_utils as midi_utils

# User constants
base_folder = '..'
folder_name = os.path.join(*('results/history-20190515121400'.split('/')))
sub_dir_name = 'e1'
sample_rate = 48000
note_dt = 2000  # num samples
note_duration = 20000  # num samples
note_decay = 5.0 / sample_rate
num_params = 120
num_measures = 16
num_sigmas = 5.0
note_threshold = 32
use_pca = True
is_ae = True

# colors
background_color = (210, 210, 210)
edge_color = (60, 60, 60)
slider_colors = [(90, 20, 20), (90, 90, 20), (20, 90, 20), (20, 90, 90), (20, 20, 90), (90, 20, 90)]

note_w = 96
note_h = 96
note_pad = 2

notes_rows = int(num_measures / 8)
notes_cols = 8

slider_num = min(40, num_params)
slider_h = 200
slider_pad = 5
tick_pad = 4

control_w = 210
control_h = 30
control_pad = 5
control_num = 3
control_colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255)]
control_inits = [0.75, 0.5, 0.5]

# derived constants
notes_w = notes_cols * (note_w + note_pad * 2)
notes_h = notes_rows * (note_h + note_pad * 2)
sliders_w = notes_w
sliders_h = slider_h + slider_pad * 2
controls_w = control_w * control_num
controls_h = control_h
window_w = notes_w
window_h = notes_h + sliders_h + controls_h
slider_w = int((window_w - slider_pad * 2) / slider_num)
notes_x = 0
notes_y = sliders_h
sliders_x = slider_pad
sliders_y = slider_pad
controls_x = int((window_w - controls_w) / 2)
controls_y = notes_h + sliders_h

# global variables
prev_mouse_pos = None
mouse_pressed = 0
cur_slider_ix = 0
cur_control_ix = 0
volume = 3000
instrument = 0
needs_update = True
current_params = np.zeros((num_params,), dtype=np.float32)
current_notes = np.zeros((num_measures, note_h, note_w), dtype=np.uint8)
cur_controls = np.array(control_inits, dtype=np.float32)

# setup audio stream
audio = pyaudio.PyAudio()
audio_notes = []
audio_time = 0
note_time = 0
note_time_dt = 0
audio_reset = False
audio_pause = False


def audio_callback(in_data, frame_count, time_info, status):
    """
    Audio call-back to influence playback of music with input.
    :param in_data:
    :param frame_count:
    :param time_info:
    :param status:
    :return:
    """
    global audio_time
    global audio_notes
    global audio_reset
    global note_time
    global note_time_dt

    # check if needs restart
    if audio_reset:
        audio_notes = []
        audio_time = 0
        note_time = 0
        note_time_dt = 0
        audio_reset = False

    # check if paused
    if audio_pause and status is not None:
        data = np.zeros((frame_count,), dtype=np.float32)
        return data.tobytes(), pyaudio.paContinue

    # find and add any notes in this time window
    cur_dt = note_dt
    while note_time_dt < audio_time + frame_count:
        measure_ix = int(note_time / note_h)
        if measure_ix >= num_measures:
            break
        note_ix = note_time % note_h
        notes = np.where(current_notes[measure_ix, note_ix] >= note_threshold)[0]
        for note in notes:
            freq = 2 * 38.89 * pow(2.0, note / 12.0) / sample_rate
            audio_notes.append((note_time_dt, freq))
        note_time += 1
        note_time_dt += cur_dt

    # generate the tones
    data = np.zeros((frame_count,), dtype=np.float32)
    for t, f in audio_notes:
        x = np.arange(audio_time - t, audio_time + frame_count - t)
        x = np.maximum(x, 0)

        if instrument == 0:
            w = np.sign(1 - np.mod(x * f, 2))  # Square
        elif instrument == 1:
            w = np.mod(x * f - 1, 2) - 1  # Sawtooth
        elif instrument == 2:
            w = 2 * np.abs(np.mod(x * f - 0.5, 2) - 1) - 1  # Triangle
        elif instrument == 3:
            w = np.sin(x * f * math.pi)  # Sine

        # w = np.floor(w*8)/8
        w[x == 0] = 0
        w *= volume * np.exp(-x * note_decay)
        data += w
    data = np.clip(data, -32000, 32000).astype(np.int16)

    # remove notes that are too old
    audio_time += frame_count
    audio_notes = [(t, f) for t, f in audio_notes if audio_time < t + note_duration]

    # reset if loop occurs
    if note_time / note_h >= num_measures:
        audio_time = 0
        note_time = 0
        note_time_dt = 0
        audio_notes = []

    # return the sound clip
    return data.tobytes(), pyaudio.paContinue


def update_mouse_click(mouse_pos):
    """
    Update control stated based on where the mouse clicked.
    :param mouse_pos:
    :return:
    """
    global cur_slider_ix
    global cur_control_ix
    global mouse_pressed
    x = (mouse_pos[0] - sliders_x)
    y = (mouse_pos[1] - sliders_y)

    if 0 <= x < sliders_w and 0 <= y < sliders_h:
        cur_slider_ix = int(x / slider_w)
        mouse_pressed = 1

    x = (mouse_pos[0] - controls_x)
    y = (mouse_pos[1] - controls_y)
    if 0 <= x < controls_w and 0 <= y < controls_h:
        cur_control_ix = int(x / control_w)
        mouse_pressed = 2


def apply_controls():
    """
    Change parameters based on controls.
    :return:
    """
    global note_threshold
    global note_dt
    global volume

    note_threshold = (1.0 - cur_controls[0]) * 200 + 10
    note_dt = (1.0 - cur_controls[1]) * 1800 + 200
    volume = cur_controls[2] * 6000


def update_mouse_move(mouse_pos):
    """
    Update sliders/controls based on mouse input.
    :param mouse_pos:
    :return:
    """
    global needs_update

    if mouse_pressed == 1:
        # change sliders
        y = (mouse_pos[1] - sliders_y)
        if 0 <= y <= slider_h:
            val = (float(y) / slider_h - 0.5) * (num_sigmas * 2)
            current_params[int(cur_slider_ix)] = val
            needs_update = True
    elif mouse_pressed == 2:
        # change controls
        x = (mouse_pos[0] - (controls_x + cur_control_ix * control_w))
        if control_pad <= x <= control_w - control_pad:
            val = float(x - control_pad) / (control_w - control_pad * 2)
            cur_controls[int(cur_control_ix)] = val
            apply_controls()


def draw_controls(screen):
    """
    Draw volume and threshold controls to screen.
    :param screen:
    :return:
    """
    for i in range(control_num):
        x = controls_x + i * control_w + control_pad
        y = controls_y + control_pad
        w = control_w - control_pad * 2
        h = control_h - control_pad * 2
        col = control_colors[i]

        pygame.draw.rect(screen, col, (x, y, int(w * cur_controls[i]), h))
        pygame.draw.rect(screen, (0, 0, 0), (x, y, w, h), 1)


def draw_sliders(screen):
    """
    Draw sliders to screen.
    :param screen:
    :return:
    """
    for i in range(slider_num):
        slider_color = slider_colors[i % len(slider_colors)]
        x = sliders_x + i * slider_w
        y = sliders_y

        cx = x + slider_w / 2
        cy_1 = y
        cy_2 = y + slider_h
        pygame.draw.line(screen, slider_color, (cx, cy_1), (cx, cy_2))

        cx_1 = x + tick_pad
        cx_2 = x + slider_w - tick_pad
        for j in range(int(num_sigmas * 2 + 1)):
            ly = y + slider_h / 2.0 + (j - num_sigmas) * slider_h / (num_sigmas * 2.0)
            ly = int(ly)
            col = (0, 0, 0) if j - num_sigmas == 0 else slider_color
            pygame.draw.line(screen, col, (cx_1, ly), (cx_2, ly))

        py = y + int((current_params[i] / (num_sigmas * 2) + 0.5) * slider_h)
        pygame.draw.circle(screen, slider_color, (int(cx), int(py)), int((slider_w - tick_pad) / 2))


def get_pianoroll_from_notes(notes):
    """
    Draw piano roll of notes.
    :param notes:
    :return:
    """

    output = np.full((3, int(notes_h), int(notes_w)), 64, dtype=np.uint8)

    for i in range(notes_rows):
        for j in range(notes_cols):
            x = note_pad + j * (note_w + note_pad * 2)
            y = note_pad + i * (note_h + note_pad * 2)
            ix = i * notes_cols + j

            measure = np.rot90(notes[ix])
            played_only = np.where(measure >= note_threshold, 255, 0)
            output[0, y:y + note_h, x:x + note_w] = np.minimum(measure * (255.0 / note_threshold), 255.0)
            output[1, y:y + note_h, x:x + note_w] = played_only
            output[2, y:y + note_h, x:x + note_w] = played_only

    return np.transpose(output, (2, 1, 0))


def draw_notes(screen, notes_surface):
    """
    Draw pianoroll notes to screen.
    :param screen:
    :param notes_surface:
    :return:
    """

    pygame.surfarray.blit_array(notes_surface, get_pianoroll_from_notes(current_notes))

    measure_ix = int(note_time / note_h)
    note_ix = note_time % note_h
    x = notes_x + note_pad + (measure_ix % notes_cols) * (note_w + note_pad * 2) + note_ix
    y = notes_y + note_pad + int(measure_ix / notes_cols) * (note_h + note_pad * 2)

    pygame.draw.rect(screen, (255, 255, 0), (x, y, 4, note_h), 0)


def play(framework, model_folder_name):
    global mouse_pressed
    global current_notes
    global audio_pause
    global needs_update
    global current_params
    global prev_mouse_pos
    global audio_reset
    global instrument

    print("Loading encoder...")
    if framework.lower() == 'keras':
        from src.composer.keras_model import KerasModelLoader
        model_loader = KerasModelLoader(os.path.join(base_folder, folder_name))
        encoder = model_loader.get_submodel('encoder')
        decoder = model_loader.get_submodel('decoder')
    elif 'torch' in framework.lower():
        from src.composer.pytorch_model import PyTorchModelLoader
        model_loader = PyTorchModelLoader(os.path.join(base_folder, folder_name))
        encoder = model_loader.get_submodel('encoder')
        decoder = model_loader.get_submodel('decoder')
    else:
        raise Exception('Model could not be loaded from {} with framework {}'.format(folder_name, framework))

    print("Loading gaussian/pca statistics...")
    latent_means = np.load(os.path.join(base_folder, folder_name, model_folder_name, 'latent_means.npy'))
    latent_stds = np.load(os.path.join(base_folder, folder_name, model_folder_name, 'latent_stds.npy'))
    latent_pca_values = np.load(os.path.join(base_folder, folder_name, model_folder_name, 'latent_pca_values.npy'))
    latent_pca_vectors = np.load(os.path.join(base_folder, folder_name, model_folder_name, 'latent_pca_vectors.npy'))

    print("Loading songs...")
    y_samples = np.load(os.path.join(base_folder, *'data/interim/samples.npy'.split('/')))
    y_lengths = np.load(os.path.join(base_folder, *'data/interim/lengths.npy'.split('/')))

    # open a window
    pygame.init()
    pygame.font.init()
    screen = pygame.display.set_mode((int(window_w), int(window_h)))
    notes_surface = screen.subsurface((notes_x, notes_y, notes_w, notes_h))
    pygame.display.set_caption('Neural Composer')

    # start the audio stream
    audio_stream = audio.open(
        format=audio.get_format_from_width(2),
        channels=1,
        rate=sample_rate,
        output=True,
        stream_callback=audio_callback)
    audio_stream.start_stream()

    # main loop
    running = True
    random_song_ix = 0
    cur_len = 0
    apply_controls()
    while running:
        # process events
        for event in pygame.event.get():
            if event.type == pygame.QUIT:  # QUIT BUTTON HIT
                running = False
                break

            elif event.type == pygame.MOUSEBUTTONDOWN:  # MOUSE BUTTON DOWN
                if pygame.mouse.get_pressed()[0]:
                    prev_mouse_pos = pygame.mouse.get_pos()
                    update_mouse_click(prev_mouse_pos)
                    update_mouse_move(prev_mouse_pos)
                elif pygame.mouse.get_pressed()[2]:
                    current_params = np.zeros((num_params,), dtype=np.float32)
                    needs_update = True

            elif event.type == pygame.MOUSEBUTTONUP:   # MOUSE BUTTON UP
                mouse_pressed = 0
                prev_mouse_pos = None

            elif event.type == pygame.MOUSEMOTION and mouse_pressed > 0:  # MOUSE MOTION WHILE PRESSED
                update_mouse_move(pygame.mouse.get_pos())

            elif event.type == pygame.KEYDOWN:
                if event.key == pygame.K_r:  # KEYDOWN R
                    # generate random song
                    current_params = np.clip(np.random.normal(0.0, 1.0, (num_params,)), -num_sigmas, num_sigmas)
                    needs_update = True
                    audio_reset = True

                if event.key == pygame.K_e:  # KEYDOWN E
                    # generate random song with larger variance
                    current_params = np.clip(np.random.normal(0.0, 2.0, (num_params,)), -num_sigmas, num_sigmas)
                    needs_update = True
                    audio_reset = True

                if event.key == pygame.K_o:  # KEYDOWN O
                    # check how well the autoencoder can reconstruct a random song
                    print("Random Song Index: " + str(random_song_ix))
                    if is_ae:
                        example_song = y_samples[cur_len:cur_len + num_measures]
                        current_notes = example_song * 255
                        latent_x = encoder.predict(np.expand_dims(example_song, 0), batch_size=1)[0]
                        cur_len += y_lengths[random_song_ix]
                        random_song_ix += 1
                    else:
                        random_song_ix = np.array([random_song_ix], dtype=np.int64)
                        latent_x = encoder.predict(random_song_ix, batch_size=1)[0]
                        random_song_ix = (random_song_ix + 1) % model.layers[0].input_dim

                    if use_pca:
                        current_params = np.dot(latent_x - latent_means, latent_pca_vectors.T) / latent_pca_values
                    else:
                        current_params = (latent_x - latent_means) / latent_stds

                    needs_update = True
                    audio_reset = True

                if event.key == pygame.K_m:  # KEYDOWN M
                    # save song as midi
                    audio_pause = True
                    audio_reset = True
                    midi_utils.samples_to_midi(current_notes, os.path.join(base_folder, *'results/live.mid'.split('/')), note_threshold)
                    audio_pause = False

                if event.key == pygame.K_w:  # KEYDOWN W
                    # save song as wave
                    audio_pause = True
                    audio_reset = True
                    save_audio = b''
                    while True:
                        save_audio += audio_callback(None, 1024, None, None)[0]
                        if audio_time == 0:
                            break
                    wave_output = wave.open(os.path.join(base_folder, *'results/live.wav'.split('/')), 'w')
                    wave_output.setparams((1, 2, sample_rate, 0, 'NONE', 'not compressed'))
                    wave_output.writeframes(save_audio)
                    wave_output.close()
                    audio_pause = False

                if event.key == pygame.K_ESCAPE:  # KEYDOWN ESCAPE
                    # exit application
                    running = False
                    break

                if event.key == pygame.K_SPACE:  # KEYDOWN SPACE
                    # toggle pause/play audio
                    audio_pause = not audio_pause

                if event.key == pygame.K_TAB:  # KEYDOWN TAB
                    # reset audio playing
                    audio_reset = True

                if event.key == pygame.K_1:  # KEYDOWN 1
                    # play instrument 0
                    instrument = 0

                if event.key == pygame.K_2:  # KEYDOWN 2
                    # play instrument 1
                    instrument = 1

                if event.key == pygame.K_3:  # KEYDOWN 3
                    # play instrument 2
                    instrument = 2

                if event.key == pygame.K_4:  # KEYDOWN 4
                    # play instrument 3
                    instrument = 3

                if event.key == pygame.K_c:  # KEYDOWN C
                    # test if the autoencoder did a good job ad reproducing one of the original songs
                    y = np.expand_dims(np.where(current_notes > note_threshold, 1, 0), 0)
                    latent_x = encoder.predict(y)[0]
                    if use_pca:
                        current_params = np.dot(latent_x - latent_means, latent_pca_vectors.T) / latent_pca_values
                    else:
                        current_params = (latent_x - latent_means) / latent_stds
                    needs_update = True

        # check if params were changed so that a new song should be generated
        if needs_update:
            if use_pca:
                latent_x = latent_means + np.dot(current_params * latent_pca_values, latent_pca_vectors)
            else:
                latent_x = latent_means + latent_stds * current_params
            latent_x = np.expand_dims(latent_x, axis=0)
            y = decoder([latent_x, 0])[0][0]
            current_notes = (y * 255.0).astype(np.uint8)
            needs_update = False

        # draw GUI to the screen
        screen.fill(background_color)
        draw_notes(screen, notes_surface)
        draw_sliders(screen)
        draw_controls(screen)

        # flip the screen buffer
        pygame.display.flip()
        pygame.time.wait(10)

    # if app is exited, close the audio stream
    audio_stream.stop_stream()
    audio_stream.close()
    audio.terminate()


if __name__ == "__main__":
    # configure parser and parse arguments
    parser = argparse.ArgumentParser(description='Neural Composer: Play and edit music of a trained model.')
    parser.add_argument('--framework', default="keras", type=str, help='The framework the model was trained with. Either keras or pytorch')
    parser.add_argument('--model', default=sub_dir_name, type=str, help='The folder the model is stored in.')

    args = parser.parse_args()
    framework = args.framework
    sub_dir_name = args.model
    play(framework, sub_dir_name)
