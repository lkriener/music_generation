{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_autoencoder.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "Ou4d9tXByD7W"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "xwklt7Ux0tIr",
        "colab_type": "text"
      },
      "source": [
        "# Run the full pipeline to train a composer model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OEd0I150tIu",
        "colab_type": "code",
        "outputId": "32673968-c7de-40de-ce29-c7bf16e56d7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# Uncomment the line below and run this cell to get your data from github into colab (only runnable in colab, not ordinary jupyter notebook):\n",
        "! git clone https://github.com/lkriener/music_generation.git && mv music_generation/* . && rm music_generation -r"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'music_generation'...\n",
            "remote: Enumerating objects: 202, done.\u001b[K\n",
            "remote: Counting objects: 100% (202/202), done.\u001b[K\n",
            "remote: Compressing objects: 100% (149/149), done.\u001b[K\n",
            "remote: Total 896 (delta 124), reused 124 (delta 53), pack-reused 694\u001b[K\n",
            "Receiving objects: 100% (896/896), 4.85 MiB | 23.42 MiB/s, done.\n",
            "Resolving deltas: 100% (304/304), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Px4qJApoDwn2",
        "colab_type": "code",
        "outputId": "13309f2a-2f1b-4d56-dada-b33101bc2d33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        }
      },
      "source": [
        "!pip install -r colab_requirements.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting py_midicsv==1.9.0 (from -r colab_requirements.txt (line 4))\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/eb/3133f65bd34dafcbae37508d290ebf540832430cbe2aef23629cc6a6197f/py_midicsv-1.9.0-py3-none-any.whl\n",
            "Collecting pypianoroll==0.5.0 (from -r colab_requirements.txt (line 5))\n",
            "  Downloading https://files.pythonhosted.org/packages/aa/33/fa38c07909e425add987146cb0f8d5ad80262f6a72cc820bf7e5f690d527/pypianoroll-0.5.0.tar.gz\n",
            "Collecting mido==1.2.9 (from -r colab_requirements.txt (line 6))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/0a/81beb587b1ae832ea6a1901dc7c6faa380e8dd154e0a862f0a9f3d2afab9/mido-1.2.9-py2.py3-none-any.whl (52kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.7MB/s \n",
            "\u001b[?25hCollecting pygame==1.9.6 (from -r colab_requirements.txt (line 7))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/24/ede6428359f913ed9cd1643dd5533aefeb5a2699cc95bea089de50ead586/pygame-1.9.6-cp36-cp36m-manylinux1_x86_64.whl (11.4MB)\n",
            "\u001b[K     |████████████████████████████████| 11.4MB 9.8MB/s \n",
            "\u001b[?25hCollecting pyaudio==0.2.11 (from -r colab_requirements.txt (line 8))\n",
            "  Downloading https://files.pythonhosted.org/packages/ab/42/b4f04721c5c5bfc196ce156b3c768998ef8c0ae3654ed29ea5020c749a6b/PyAudio-0.2.11.tar.gz\n",
            "Requirement already satisfied: six<2.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pypianoroll==0.5.0->-r colab_requirements.txt (line 5)) (1.12.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pypianoroll==0.5.0->-r colab_requirements.txt (line 5)) (1.16.3)\n",
            "Requirement already satisfied: scipy<2.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pypianoroll==0.5.0->-r colab_requirements.txt (line 5)) (1.2.1)\n",
            "Requirement already satisfied: pretty_midi<1.0,>=0.2.8 in /usr/local/lib/python3.6/dist-packages (from pypianoroll==0.5.0->-r colab_requirements.txt (line 5)) (0.2.8)\n",
            "Building wheels for collected packages: pypianoroll, pyaudio\n",
            "  Building wheel for pypianoroll (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/f6/fb/5d070524ecf7ba9ed201247a293c01945cfd7f840f8ef338c0\n",
            "  Building wheel for pyaudio (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for pyaudio\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for pyaudio\n",
            "Successfully built pypianoroll\n",
            "Failed to build pyaudio\n",
            "\u001b[31mERROR: magenta 0.3.19 has requirement mido==1.2.6, but you'll have mido 1.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: py-midicsv, pypianoroll, mido, pygame, pyaudio\n",
            "  Found existing installation: mido 1.2.6\n",
            "    Uninstalling mido-1.2.6:\n",
            "      Successfully uninstalled mido-1.2.6\n",
            "  Running setup.py install for pyaudio ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31mERROR: Command \"/usr/bin/python3 -u -c 'import setuptools, tokenize;__file__='\"'\"'/tmp/pip-install-3p59f94a/pyaudio/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-61mdtmj5/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in /tmp/pip-install-3p59f94a/pyaudio/\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2rHdUNG1Z8J",
        "colab_type": "code",
        "outputId": "5700a41a-d28f-4ff4-892b-8c8d521b6514",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')  # force_remount=True"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou4d9tXByD7W",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6mEeMjK2x-k",
        "colab_type": "code",
        "outputId": "f0ff0868-edb9-45b7-e673-55fb9d265bc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!mkdir data/raw/bach\n",
        "!cp \"/content/drive/My Drive/music_datasets/bach\" data/raw -r"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘data/raw/bach’: File exists\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zP2YGsog0tIz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from preprocess_songs import preprocess_songs\n",
        "\n",
        "data_folder = './data/raw/bach'\n",
        "#preprocess_songs(data_folder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdK-QG2cyLAE",
        "colab_type": "text"
      },
      "source": [
        "## Training Keras model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSsbEe-ryZo8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp \"/content/drive/My Drive/music_datasets/bach_interim/.\" data/interim -r"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IpcrwFI0tI3",
        "colab_type": "code",
        "outputId": "1866c9c9-928a-412c-d299-e8b32542935a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138576
        }
      },
      "source": [
        "from src.train_keras import train as train_keras\n",
        "\n",
        "trained_keras_model, training_loss = train_keras('data/interim/samples.npy', 'data/interim/lengths.npy', use_double_autoencoder=True, epochs_qty=2000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Data...\n",
            "Loaded 5856 samples from 296 songs.\n",
            "5856\n",
            "Preparing song samples, padding songs...\n",
            "Building model...\n",
            "(None, 16, 96, 96)\n",
            "(None, 16, 9216)\n",
            "(None, 16, 2000)\n",
            "(None, 16, 200)\n",
            "(None, 3200)\n",
            "(None, 1600)\n",
            "(None, 120)\n",
            "(None, 1600)\n",
            "(None, 3200)\n",
            "(None, 16, 200)\n",
            "(None, 16, 2000)\n",
            "(None, 16, 9216)\n",
            "(None, 16, 96, 96)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 16, 96, 96)        0         \n",
            "_________________________________________________________________\n",
            "reshape_3 (Reshape)          (None, 16, 9216)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 16, 2000)          18434000  \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 16, 200)           400200    \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 3200)              0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 1600)              5121600   \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 120)               192120    \n",
            "_________________________________________________________________\n",
            "encoder (BatchNormalization) (None, 120)               480       \n",
            "_________________________________________________________________\n",
            "decoder (Dense)              (None, 1600)              193600    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 1600)              6400      \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 3200)              5123200   \n",
            "_________________________________________________________________\n",
            "reshape_4 (Reshape)          (None, 16, 200)           0         \n",
            "_________________________________________________________________\n",
            "time_distributed_3 (TimeDist (None, 16, 200)           800       \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 16, 200)           0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 16, 200)           0         \n",
            "_________________________________________________________________\n",
            "time_distributed_4 (TimeDist (None, 16, 2000)          402000    \n",
            "_________________________________________________________________\n",
            "time_distributed_5 (TimeDist (None, 16, 2000)          8000      \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 16, 2000)          0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 16, 2000)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed_6 (TimeDist (None, 16, 9216)          18441216  \n",
            "_________________________________________________________________\n",
            "reshape_5 (Reshape)          (None, 16, 96, 96)        0         \n",
            "=================================================================\n",
            "Total params: 48,323,616\n",
            "Trainable params: 48,315,776\n",
            "Non-trainable params: 7,840\n",
            "_________________________________________________________________\n",
            "None\n",
            "Referencing sub-models...\n",
            "Training model...\n",
            "Training epoch:  0 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 2s 8ms/step - loss: 0.7172\n",
            "Train loss: 0.7172319889068604\n",
            "Training epoch:  1 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0965\n",
            "Train loss: 0.09652562439441681\n",
            "Training epoch:  2 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0788\n",
            "Train loss: 0.07875019311904907\n",
            "Training epoch:  3 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0868\n",
            "Train loss: 0.08678223192691803\n",
            "Training epoch:  4 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0430\n",
            "Train loss: 0.043028708547353745\n",
            "Training epoch:  5 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0289\n",
            "Train loss: 0.02894110232591629\n",
            "Training epoch:  6 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0250\n",
            "Train loss: 0.024952800944447517\n",
            "Training epoch:  7 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0220\n",
            "Train loss: 0.021963823586702347\n",
            "Training epoch:  8 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0203\n",
            "Train loss: 0.020289171487092972\n",
            "Training epoch:  9 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0174\n",
            "Train loss: 0.017382550984621048\n",
            "Training epoch:  10 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0153\n",
            "Train loss: 0.015307722613215446\n",
            "Training epoch:  11 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0141\n",
            "Train loss: 0.01409489568322897\n",
            "Training epoch:  12 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0132\n",
            "Train loss: 0.013192076236009598\n",
            "Training epoch:  13 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0125\n",
            "Train loss: 0.012515762820839882\n",
            "Training epoch:  14 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0119\n",
            "Train loss: 0.01186244748532772\n",
            "Training epoch:  15 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0114\n",
            "Train loss: 0.011362888850271702\n",
            "Training epoch:  16 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0109\n",
            "Train loss: 0.010898282751441002\n",
            "Training epoch:  17 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0106\n",
            "Train loss: 0.010584363713860512\n",
            "Training epoch:  18 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0102\n",
            "Train loss: 0.010191553272306919\n",
            "Training epoch:  19 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0099\n",
            "Train loss: 0.00989600270986557\n",
            "Training epoch:  20 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0096\n",
            "Train loss: 0.00963494461029768\n",
            "Training epoch:  21 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0094\n",
            "Train loss: 0.009414141066372395\n",
            "Training epoch:  22 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0092\n",
            "Train loss: 0.00919586792588234\n",
            "Training epoch:  23 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0090\n",
            "Train loss: 0.009007588028907776\n",
            "Training epoch:  24 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0088\n",
            "Train loss: 0.008825131691992283\n",
            "Training epoch:  25 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0087\n",
            "Train loss: 0.008677671663463116\n",
            "Training epoch:  26 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0086\n",
            "Train loss: 0.008563867770135403\n",
            "Training epoch:  27 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0084\n",
            "Train loss: 0.00844242237508297\n",
            "Training epoch:  28 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0083\n",
            "Train loss: 0.00830746628344059\n",
            "Training epoch:  29 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0082\n",
            "Train loss: 0.008204928599298\n",
            "Training epoch:  30 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0081\n",
            "Train loss: 0.008070554584264755\n",
            "Training epoch:  31 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0080\n",
            "Train loss: 0.007972302846610546\n",
            "Training epoch:  32 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0079\n",
            "Train loss: 0.007857581600546837\n",
            "Training epoch:  33 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0078\n",
            "Train loss: 0.007769071497023106\n",
            "Training epoch:  34 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0077\n",
            "Train loss: 0.007730052340775728\n",
            "Training epoch:  35 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0077\n",
            "Train loss: 0.007653594017028809\n",
            "Training epoch:  36 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0075\n",
            "Train loss: 0.007547209970653057\n",
            "Training epoch:  37 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0075\n",
            "Train loss: 0.007496885489672422\n",
            "Training epoch:  38 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0074\n",
            "Train loss: 0.007417955901473761\n",
            "Training epoch:  39 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0074\n",
            "Train loss: 0.007402551360428333\n",
            "Training epoch:  40 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0073\n",
            "Train loss: 0.0073240529745817184\n",
            "Training epoch:  41 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0073\n",
            "Train loss: 0.007292955182492733\n",
            "Training epoch:  42 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0072\n",
            "Train loss: 0.0072164214216172695\n",
            "Training epoch:  43 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0072\n",
            "Train loss: 0.0071999914944171906\n",
            "Training epoch:  44 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0071\n",
            "Train loss: 0.007138173095881939\n",
            "Training epoch:  45 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0071\n",
            "Train loss: 0.007122153416275978\n",
            "Training epoch:  46 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0071\n",
            "Train loss: 0.007058414164930582\n",
            "Training epoch:  47 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0070\n",
            "Train loss: 0.0070298234932124615\n",
            "Training epoch:  48 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0070\n",
            "Train loss: 0.006969927344471216\n",
            "Training epoch:  49 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0070\n",
            "Train loss: 0.0069601270370185375\n",
            "Training epoch:  50 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0069\n",
            "Train loss: 0.006912920158356428\n",
            "Training epoch:  51 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0069\n",
            "Train loss: 0.006935283076018095\n",
            "Training epoch:  52 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0069\n",
            "Train loss: 0.006873059086501598\n",
            "Training epoch:  53 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0068\n",
            "Train loss: 0.006830052938312292\n",
            "Training epoch:  54 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0068\n",
            "Train loss: 0.006763876415789127\n",
            "Training epoch:  55 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0067\n",
            "Train loss: 0.006732259877026081\n",
            "Training epoch:  56 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0067\n",
            "Train loss: 0.0066724070347845554\n",
            "Training epoch:  57 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0066\n",
            "Train loss: 0.006638110615313053\n",
            "Training epoch:  58 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0066\n",
            "Train loss: 0.006609228905290365\n",
            "Training epoch:  59 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0066\n",
            "Train loss: 0.006570836063474417\n",
            "Training epoch:  60 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0065\n",
            "Train loss: 0.006538223475217819\n",
            "Training epoch:  61 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0065\n",
            "Train loss: 0.006497784052044153\n",
            "Training epoch:  62 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0065\n",
            "Train loss: 0.006470630876719952\n",
            "Training epoch:  63 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0065\n",
            "Train loss: 0.006468023639172316\n",
            "Training epoch:  64 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Train loss: 0.006443459074944258\n",
            "Training epoch:  65 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0064\n",
            "Train loss: 0.006394712720066309\n",
            "Training epoch:  66 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0064\n",
            "Train loss: 0.006355477962642908\n",
            "Training epoch:  67 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0063\n",
            "Train loss: 0.006348709110170603\n",
            "Training epoch:  68 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0064\n",
            "Train loss: 0.006379447411745787\n",
            "Training epoch:  69 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0064\n",
            "Train loss: 0.006368106696754694\n",
            "Training epoch:  70 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0064\n",
            "Train loss: 0.006359596271067858\n",
            "Training epoch:  71 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0063\n",
            "Train loss: 0.006336299702525139\n",
            "Training epoch:  72 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0063\n",
            "Train loss: 0.006299624685198069\n",
            "Training epoch:  73 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0063\n",
            "Train loss: 0.0063233403488993645\n",
            "Training epoch:  74 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0063\n",
            "Train loss: 0.006258423440158367\n",
            "Training epoch:  75 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0063\n",
            "Train loss: 0.006273987703025341\n",
            "Training epoch:  76 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0062\n",
            "Train loss: 0.006228261161595583\n",
            "Training epoch:  77 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0063\n",
            "Train loss: 0.006289141718298197\n",
            "Training epoch:  78 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0062\n",
            "Train loss: 0.00624234089627862\n",
            "Training epoch:  79 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0063\n",
            "Train loss: 0.006308515556156635\n",
            "Training epoch:  80 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0063\n",
            "Train loss: 0.006258724723011255\n",
            "Training epoch:  81 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0063\n",
            "Train loss: 0.006314220372587442\n",
            "Training epoch:  82 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0063\n",
            "Train loss: 0.006257741712033749\n",
            "Training epoch:  83 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0062\n",
            "Train loss: 0.006248475518077612\n",
            "Training epoch:  84 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0062\n",
            "Train loss: 0.0061952355317771435\n",
            "Training epoch:  85 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0062\n",
            "Train loss: 0.0062131695449352264\n",
            "Training epoch:  86 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0062\n",
            "Train loss: 0.00617698859423399\n",
            "Training epoch:  87 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0062\n",
            "Train loss: 0.006193035282194614\n",
            "Training epoch:  88 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0062\n",
            "Train loss: 0.0061728269793093204\n",
            "Training epoch:  89 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0062\n",
            "Train loss: 0.006214791443198919\n",
            "Training epoch:  90 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0061\n",
            "Train loss: 0.006133364513516426\n",
            "Training epoch:  91 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0062\n",
            "Train loss: 0.0061991577968001366\n",
            "Training epoch:  92 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0061\n",
            "Train loss: 0.0061179944314062595\n",
            "Training epoch:  93 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0062\n",
            "Train loss: 0.006243218667805195\n",
            "Training epoch:  94 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0062\n",
            "Train loss: 0.006218458991497755\n",
            "Training epoch:  95 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0063\n",
            "Train loss: 0.006258963607251644\n",
            "Training epoch:  96 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0061\n",
            "Train loss: 0.006099266465753317\n",
            "Training epoch:  97 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0061\n",
            "Train loss: 0.006120047997683287\n",
            "Training epoch:  98 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0061\n",
            "Train loss: 0.006099655758589506\n",
            "Training epoch:  99 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0061\n",
            "Train loss: 0.006136074662208557\n",
            "...Saved PCA statistics and last model.\n",
            "Latent Mean values:  [ 0.16442364  0.04212161 -0.4769328  -0.16812222  0.25800413  0.27584144]\n",
            "Latent PCA values:  [9.64420124 8.1807461  2.7385136  0.74042707 0.55797796 0.29177172]\n",
            "Training epoch:  100 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0061\n",
            "Train loss: 0.006099347956478596\n",
            "Training epoch:  101 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0062\n",
            "Train loss: 0.006179268006235361\n",
            "Training epoch:  102 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0062\n",
            "Train loss: 0.0061538126319646835\n",
            "Training epoch:  103 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0062\n",
            "Train loss: 0.0062476531602442265\n",
            "Training epoch:  104 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0061\n",
            "Train loss: 0.0060978541150689125\n",
            "Training epoch:  105 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0062\n",
            "Train loss: 0.006222187541425228\n",
            "Training epoch:  106 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0061\n",
            "Train loss: 0.00609346991404891\n",
            "Training epoch:  107 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0062\n",
            "Train loss: 0.0062079718336462975\n",
            "Training epoch:  108 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0061\n",
            "Train loss: 0.006110853049904108\n",
            "Training epoch:  109 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0062\n",
            "Train loss: 0.006182822398841381\n",
            "Training epoch:  110 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0060\n",
            "Train loss: 0.006035087630152702\n",
            "Training epoch:  111 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0061\n",
            "Train loss: 0.0060662622563540936\n",
            "Training epoch:  112 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0060\n",
            "Train loss: 0.006011404097080231\n",
            "Training epoch:  113 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0061\n",
            "Train loss: 0.006063675042241812\n",
            "Training epoch:  114 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0061\n",
            "Train loss: 0.006052493117749691\n",
            "Training epoch:  115 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0062\n",
            "Train loss: 0.006194465793669224\n",
            "Training epoch:  116 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0061\n",
            "Train loss: 0.006093356758356094\n",
            "Training epoch:  117 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0062\n",
            "Train loss: 0.006217870861291885\n",
            "Training epoch:  118 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0061\n",
            "Train loss: 0.006061358842998743\n",
            "Training epoch:  119 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0061\n",
            "Train loss: 0.006142282858490944\n",
            "Training epoch:  120 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0060\n",
            "Train loss: 0.006007594987750053\n",
            "Training epoch:  121 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0060\n",
            "Train loss: 0.0060349032282829285\n",
            "Training epoch:  122 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0060\n",
            "Train loss: 0.005981935653835535\n",
            "Training epoch:  123 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0060\n",
            "Train loss: 0.006027517840266228\n",
            "Training epoch:  124 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0060\n",
            "Train loss: 0.005988087970763445\n",
            "Training epoch:  125 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0061\n",
            "Train loss: 0.0060598659329116344\n",
            "Training epoch:  126 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0060\n",
            "Train loss: 0.0059969560243189335\n",
            "Training epoch:  127 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0061\n",
            "Train loss: 0.006078849546611309\n",
            "Training epoch:  128 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0060\n",
            "Train loss: 0.0060096848756074905\n",
            "Training epoch:  129 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0061\n",
            "Train loss: 0.006093231029808521\n",
            "Training epoch:  130 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0060\n",
            "Train loss: 0.006007937248796225\n",
            "Training epoch:  131 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0060\n",
            "Train loss: 0.006022547837346792\n",
            "Training epoch:  132 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0059\n",
            "Train loss: 0.005937881767749786\n",
            "Training epoch:  133 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0060\n",
            "Train loss: 0.005950875114649534\n",
            "Training epoch:  134 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0059\n",
            "Train loss: 0.0059294551610946655\n",
            "Training epoch:  135 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0060\n",
            "Train loss: 0.005970647558569908\n",
            "Training epoch:  136 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0060\n",
            "Train loss: 0.005985124036669731\n",
            "Training epoch:  137 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0061\n",
            "Train loss: 0.006063311360776424\n",
            "Training epoch:  138 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0060\n",
            "Train loss: 0.006007269956171513\n",
            "Training epoch:  139 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0062\n",
            "Train loss: 0.0061559053137898445\n",
            "Training epoch:  140 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0060\n",
            "Train loss: 0.006000563036650419\n",
            "Training epoch:  141 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0060\n",
            "Train loss: 0.0060472809709608555\n",
            "Training epoch:  142 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0059\n",
            "Train loss: 0.005901880096644163\n",
            "Training epoch:  143 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0059\n",
            "Train loss: 0.005921365227550268\n",
            "Training epoch:  144 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0059\n",
            "Train loss: 0.005855422466993332\n",
            "Training epoch:  145 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0059\n",
            "Train loss: 0.005892617162317038\n",
            "Training epoch:  146 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0059\n",
            "Train loss: 0.005859872326254845\n",
            "Training epoch:  147 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0059\n",
            "Train loss: 0.005932547617703676\n",
            "Training epoch:  148 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0059\n",
            "Train loss: 0.0059027522802352905\n",
            "Training epoch:  149 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0060\n",
            "Train loss: 0.006003103218972683\n",
            "Training epoch:  150 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0059\n",
            "Train loss: 0.0058944616466760635\n",
            "Training epoch:  151 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0060\n",
            "Train loss: 0.005978730972856283\n",
            "Training epoch:  152 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0059\n",
            "Train loss: 0.005916137248277664\n",
            "Training epoch:  153 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0060\n",
            "Train loss: 0.006029232405126095\n",
            "Training epoch:  154 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0059\n",
            "Train loss: 0.005926136858761311\n",
            "Training epoch:  155 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0059\n",
            "Train loss: 0.0058881607837975025\n",
            "Training epoch:  156 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0058\n",
            "Train loss: 0.005836531054228544\n",
            "Training epoch:  157 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0059\n",
            "Train loss: 0.005864648148417473\n",
            "Training epoch:  158 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0059\n",
            "Train loss: 0.005878302734345198\n",
            "Training epoch:  159 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0059\n",
            "Train loss: 0.005941417068243027\n",
            "Training epoch:  160 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0059\n",
            "Train loss: 0.0058949533849954605\n",
            "Training epoch:  161 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0060\n",
            "Train loss: 0.0059673842042684555\n",
            "Training epoch:  162 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0059\n",
            "Train loss: 0.005894533824175596\n",
            "Training epoch:  163 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0059\n",
            "Train loss: 0.005936708766967058\n",
            "Training epoch:  164 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0059\n",
            "Train loss: 0.005855889525264502\n",
            "Training epoch:  165 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0059\n",
            "Train loss: 0.005876157898455858\n",
            "Training epoch:  166 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0058\n",
            "Train loss: 0.005820936057716608\n",
            "Training epoch:  167 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0058\n",
            "Train loss: 0.0058355676010251045\n",
            "Training epoch:  168 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0058\n",
            "Train loss: 0.00580023555085063\n",
            "Training epoch:  169 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0058\n",
            "Train loss: 0.005839797668159008\n",
            "Training epoch:  170 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0058\n",
            "Train loss: 0.005815715994685888\n",
            "Training epoch:  171 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0059\n",
            "Train loss: 0.005865550599992275\n",
            "Training epoch:  172 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0058\n",
            "Train loss: 0.005842342972755432\n",
            "Training epoch:  173 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0059\n",
            "Train loss: 0.0059441388584673405\n",
            "Training epoch:  174 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0058\n",
            "Train loss: 0.005837997887283564\n",
            "Training epoch:  175 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0058\n",
            "Train loss: 0.005833546165376902\n",
            "Training epoch:  176 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0058\n",
            "Train loss: 0.005772198084741831\n",
            "Training epoch:  177 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0058\n",
            "Train loss: 0.005786456633359194\n",
            "Training epoch:  178 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0058\n",
            "Train loss: 0.005776394158601761\n",
            "Training epoch:  179 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0058\n",
            "Train loss: 0.005820880644023418\n",
            "Training epoch:  180 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0058\n",
            "Train loss: 0.005783452186733484\n",
            "Training epoch:  181 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0058\n",
            "Train loss: 0.005837737116962671\n",
            "Training epoch:  182 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0058\n",
            "Train loss: 0.005770668853074312\n",
            "Training epoch:  183 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0058\n",
            "Train loss: 0.005842290818691254\n",
            "Training epoch:  184 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0058\n",
            "Train loss: 0.005786594469100237\n",
            "Training epoch:  185 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0058\n",
            "Train loss: 0.005845624953508377\n",
            "Training epoch:  186 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0058\n",
            "Train loss: 0.005795932374894619\n",
            "Training epoch:  187 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0059\n",
            "Train loss: 0.005881552118808031\n",
            "Training epoch:  188 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0058\n",
            "Train loss: 0.005797391291707754\n",
            "Training epoch:  189 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0058\n",
            "Train loss: 0.005837286822497845\n",
            "Training epoch:  190 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0058\n",
            "Train loss: 0.005761627573519945\n",
            "Training epoch:  191 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0058\n",
            "Train loss: 0.005808877293020487\n",
            "Training epoch:  192 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0058\n",
            "Train loss: 0.005761992186307907\n",
            "Training epoch:  193 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0058\n",
            "Train loss: 0.005782808642834425\n",
            "Training epoch:  194 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0057\n",
            "Train loss: 0.005728767719119787\n",
            "Training epoch:  195 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0058\n",
            "Train loss: 0.005775523837655783\n",
            "Training epoch:  196 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0057\n",
            "Train loss: 0.005744229070842266\n",
            "Training epoch:  197 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0058\n",
            "Train loss: 0.005790979601442814\n",
            "Training epoch:  198 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0057\n",
            "Train loss: 0.005733940284699202\n",
            "Training epoch:  199 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0057\n",
            "Train loss: 0.0057221343740820885\n",
            "...Saved PCA statistics and last model.\n",
            "Latent Mean values:  [ 0.16871206  0.0856545   0.10043205  0.38132018 -0.00891586  0.2852222 ]\n",
            "Latent PCA values:  [9.10692546 4.92873036 3.28264639 2.81669214 2.1942622  0.87382467]\n",
            "Training epoch:  200 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0057\n",
            "Train loss: 0.00567009299993515\n",
            "Training epoch:  201 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0057\n",
            "Train loss: 0.00566406175494194\n",
            "Training epoch:  202 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0057\n",
            "Train loss: 0.005661866161972284\n",
            "Training epoch:  203 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0057\n",
            "Train loss: 0.005688899662345648\n",
            "Training epoch:  204 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0057\n",
            "Train loss: 0.005717448890209198\n",
            "Training epoch:  205 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0057\n",
            "Train loss: 0.005748116876929998\n",
            "Training epoch:  206 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0057\n",
            "Train loss: 0.005729990545660257\n",
            "Training epoch:  207 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0058\n",
            "Train loss: 0.005764463450759649\n",
            "Training epoch:  208 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0057\n",
            "Train loss: 0.005694975145161152\n",
            "Training epoch:  209 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0057\n",
            "Train loss: 0.005743428133428097\n",
            "Training epoch:  210 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0057\n",
            "Train loss: 0.005715005099773407\n",
            "Training epoch:  211 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0058\n",
            "Train loss: 0.00578045379370451\n",
            "Training epoch:  212 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0057\n",
            "Train loss: 0.005716665182262659\n",
            "Training epoch:  213 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0057\n",
            "Train loss: 0.005741640459746122\n",
            "Training epoch:  214 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0057\n",
            "Train loss: 0.005688942968845367\n",
            "Training epoch:  215 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0057\n",
            "Train loss: 0.005713966675102711\n",
            "Training epoch:  216 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0057\n",
            "Train loss: 0.00569489598274231\n",
            "Training epoch:  217 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0057\n",
            "Train loss: 0.005732773803174496\n",
            "Training epoch:  218 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0057\n",
            "Train loss: 0.005702449474483728\n",
            "Training epoch:  219 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0057\n",
            "Train loss: 0.005739298183470964\n",
            "Training epoch:  220 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0057\n",
            "Train loss: 0.005700251087546349\n",
            "Training epoch:  221 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0057\n",
            "Train loss: 0.005734659731388092\n",
            "Training epoch:  222 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0057\n",
            "Train loss: 0.005671547260135412\n",
            "Training epoch:  223 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0057\n",
            "Train loss: 0.005650344770401716\n",
            "Training epoch:  224 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0056\n",
            "Train loss: 0.005621102172881365\n",
            "Training epoch:  225 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0056\n",
            "Train loss: 0.005619327537715435\n",
            "Training epoch:  226 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0056\n",
            "Train loss: 0.005640852265059948\n",
            "Training epoch:  227 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0057\n",
            "Train loss: 0.005650763399899006\n",
            "Training epoch:  228 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0057\n",
            "Train loss: 0.0056917788460850716\n",
            "Training epoch:  229 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0057\n",
            "Train loss: 0.0057286107912659645\n",
            "Training epoch:  230 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0058\n",
            "Train loss: 0.005768881645053625\n",
            "Training epoch:  231 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0057\n",
            "Train loss: 0.005727118346840143\n",
            "Training epoch:  232 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0057\n",
            "Train loss: 0.00567241245880723\n",
            "Training epoch:  233 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0056\n",
            "Train loss: 0.005613998975604773\n",
            "Training epoch:  234 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0056\n",
            "Train loss: 0.005616501439362764\n",
            "Training epoch:  235 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0056\n",
            "Train loss: 0.005618393421173096\n",
            "Training epoch:  236 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0056\n",
            "Train loss: 0.005619177129119635\n",
            "Training epoch:  237 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0056\n",
            "Train loss: 0.005629260092973709\n",
            "Training epoch:  238 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0057\n",
            "Train loss: 0.005682469345629215\n",
            "Training epoch:  239 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0057\n",
            "Train loss: 0.005685679614543915\n",
            "Training epoch:  240 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0057\n",
            "Train loss: 0.0056781889870762825\n",
            "Training epoch:  241 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0057\n",
            "Train loss: 0.00567824998870492\n",
            "Training epoch:  242 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0056\n",
            "Train loss: 0.005634661298245192\n",
            "Training epoch:  243 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0056\n",
            "Train loss: 0.00563666969537735\n",
            "Training epoch:  244 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0056\n",
            "Train loss: 0.0055959029123187065\n",
            "Training epoch:  245 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0056\n",
            "Train loss: 0.005605037789791822\n",
            "Training epoch:  246 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0056\n",
            "Train loss: 0.005592441186308861\n",
            "Training epoch:  247 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0056\n",
            "Train loss: 0.005625155754387379\n",
            "Training epoch:  248 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0056\n",
            "Train loss: 0.005606535356491804\n",
            "Training epoch:  249 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0056\n",
            "Train loss: 0.005625772289931774\n",
            "Training epoch:  250 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0056\n",
            "Train loss: 0.00559889804571867\n",
            "Training epoch:  251 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0056\n",
            "Train loss: 0.005594606511294842\n",
            "Training epoch:  252 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0056\n",
            "Train loss: 0.005618393421173096\n",
            "Training epoch:  253 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0056\n",
            "Train loss: 0.005637494381517172\n",
            "Training epoch:  254 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0056\n",
            "Train loss: 0.005643817130476236\n",
            "Training epoch:  255 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0057\n",
            "Train loss: 0.005663767922669649\n",
            "Training epoch:  256 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0057\n",
            "Train loss: 0.005650042090564966\n",
            "Training epoch:  257 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0057\n",
            "Train loss: 0.005652138963341713\n",
            "Training epoch:  258 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0057\n",
            "Train loss: 0.005668319761753082\n",
            "Training epoch:  259 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0056\n",
            "Train loss: 0.005581464618444443\n",
            "Training epoch:  260 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0056\n",
            "Train loss: 0.005571416113525629\n",
            "Training epoch:  261 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0055\n",
            "Train loss: 0.005543870385736227\n",
            "Training epoch:  262 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0055\n",
            "Train loss: 0.0055489870719611645\n",
            "Training epoch:  263 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0056\n",
            "Train loss: 0.005570725537836552\n",
            "Training epoch:  264 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0056\n",
            "Train loss: 0.005601355340331793\n",
            "Training epoch:  265 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0056\n",
            "Train loss: 0.005600301548838615\n",
            "Training epoch:  266 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0057\n",
            "Train loss: 0.005652813706547022\n",
            "Training epoch:  267 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0057\n",
            "Train loss: 0.005660659167915583\n",
            "Training epoch:  268 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0057\n",
            "Train loss: 0.005663811694830656\n",
            "Training epoch:  269 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0056\n",
            "Train loss: 0.005590623244643211\n",
            "Training epoch:  270 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0055\n",
            "Train loss: 0.005532076582312584\n",
            "Training epoch:  271 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0055\n",
            "Train loss: 0.005500982515513897\n",
            "Training epoch:  272 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0055\n",
            "Train loss: 0.00552906421944499\n",
            "Training epoch:  273 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0055\n",
            "Train loss: 0.00553502980619669\n",
            "Training epoch:  274 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0055\n",
            "Train loss: 0.005541174206882715\n",
            "Training epoch:  275 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0056\n",
            "Train loss: 0.0055553605780005455\n",
            "Training epoch:  276 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0056\n",
            "Train loss: 0.005569775123149157\n",
            "Training epoch:  277 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0056\n",
            "Train loss: 0.005573894362896681\n",
            "Training epoch:  278 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0056\n",
            "Train loss: 0.005558749195188284\n",
            "Training epoch:  279 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0055\n",
            "Train loss: 0.005546324886381626\n",
            "Training epoch:  280 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0055\n",
            "Train loss: 0.005549157038331032\n",
            "Training epoch:  281 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0056\n",
            "Train loss: 0.005588721018284559\n",
            "Training epoch:  282 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0056\n",
            "Train loss: 0.005567596759647131\n",
            "Training epoch:  283 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0055\n",
            "Train loss: 0.005542834755033255\n",
            "Training epoch:  284 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0055\n",
            "Train loss: 0.005477695260196924\n",
            "Training epoch:  285 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0055\n",
            "Train loss: 0.0054764715023338795\n",
            "Training epoch:  286 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0055\n",
            "Train loss: 0.005493493285030127\n",
            "Training epoch:  287 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0055\n",
            "Train loss: 0.005517748184502125\n",
            "Training epoch:  288 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0056\n",
            "Train loss: 0.005550747737288475\n",
            "Training epoch:  289 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0056\n",
            "Train loss: 0.005585262551903725\n",
            "Training epoch:  290 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0056\n",
            "Train loss: 0.005591662600636482\n",
            "Training epoch:  291 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0056\n",
            "Train loss: 0.005577122326940298\n",
            "Training epoch:  292 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0056\n",
            "Train loss: 0.0055921683087944984\n",
            "Training epoch:  293 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0055\n",
            "Train loss: 0.0055360328406095505\n",
            "Training epoch:  294 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0055\n",
            "Train loss: 0.00554537121206522\n",
            "Training epoch:  295 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0055\n",
            "Train loss: 0.0055052475072443485\n",
            "Training epoch:  296 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0055\n",
            "Train loss: 0.005517839454114437\n",
            "Training epoch:  297 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0055\n",
            "Train loss: 0.005509969312697649\n",
            "Training epoch:  298 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0055\n",
            "Train loss: 0.00553010031580925\n",
            "Training epoch:  299 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0055\n",
            "Train loss: 0.0054814876057207584\n",
            "...Saved PCA statistics and last model.\n",
            "Latent Mean values:  [ 0.24685743 -0.12311206 -0.1970623   0.2594291  -0.14556628  0.3500043 ]\n",
            "Latent PCA values:  [7.46954083 4.77419566 3.2483735  3.16985572 2.79803784 1.35855212]\n",
            "Training epoch:  300 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0055\n",
            "Train loss: 0.005508347414433956\n",
            "Training epoch:  301 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0055\n",
            "Train loss: 0.0054671610705554485\n",
            "Training epoch:  302 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0055\n",
            "Train loss: 0.005490184761583805\n",
            "Training epoch:  303 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0054\n",
            "Train loss: 0.00544004375115037\n",
            "Training epoch:  304 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0055\n",
            "Train loss: 0.005464933812618256\n",
            "Training epoch:  305 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0054\n",
            "Train loss: 0.00544529827311635\n",
            "Training epoch:  306 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0055\n",
            "Train loss: 0.005501914769411087\n",
            "Training epoch:  307 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0055\n",
            "Train loss: 0.005474196281284094\n",
            "Training epoch:  308 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0054\n",
            "Train loss: 0.005418423097580671\n",
            "Training epoch:  309 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0054\n",
            "Train loss: 0.00542118214070797\n",
            "Training epoch:  310 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0054\n",
            "Train loss: 0.00541371013969183\n",
            "Training epoch:  311 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0054\n",
            "Train loss: 0.005411128047853708\n",
            "Training epoch:  312 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0054\n",
            "Train loss: 0.005409634672105312\n",
            "Training epoch:  313 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0054\n",
            "Train loss: 0.005404348019510508\n",
            "Training epoch:  314 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0054\n",
            "Train loss: 0.005418309476226568\n",
            "Training epoch:  315 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0054\n",
            "Train loss: 0.005441620014607906\n",
            "Training epoch:  316 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0054\n",
            "Train loss: 0.005438491702079773\n",
            "Training epoch:  317 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0054\n",
            "Train loss: 0.005434097722172737\n",
            "Training epoch:  318 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0054\n",
            "Train loss: 0.005438473075628281\n",
            "Training epoch:  319 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0055\n",
            "Train loss: 0.005462645087391138\n",
            "Training epoch:  320 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0055\n",
            "Train loss: 0.005460277199745178\n",
            "Training epoch:  321 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0055\n",
            "Train loss: 0.005473017226904631\n",
            "Training epoch:  322 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0054\n",
            "Train loss: 0.005429751239717007\n",
            "Training epoch:  323 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0055\n",
            "Train loss: 0.005461454391479492\n",
            "Training epoch:  324 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0055\n",
            "Train loss: 0.005455181002616882\n",
            "Training epoch:  325 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0054\n",
            "Train loss: 0.00543025741353631\n",
            "Training epoch:  326 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0054\n",
            "Train loss: 0.0054133133962750435\n",
            "Training epoch:  327 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0054\n",
            "Train loss: 0.005373152904212475\n",
            "Training epoch:  328 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0054\n",
            "Train loss: 0.005413668230175972\n",
            "Training epoch:  329 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0054\n",
            "Train loss: 0.005383496638387442\n",
            "Training epoch:  330 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0054\n",
            "Train loss: 0.005418114364147186\n",
            "Training epoch:  331 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0054\n",
            "Train loss: 0.005373811349272728\n",
            "Training epoch:  332 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0053\n",
            "Train loss: 0.005345153156667948\n",
            "Training epoch:  333 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0053\n",
            "Train loss: 0.00530913146212697\n",
            "Training epoch:  334 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0053\n",
            "Train loss: 0.005313534755259752\n",
            "Training epoch:  335 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0053\n",
            "Train loss: 0.005308456718921661\n",
            "Training epoch:  336 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0053\n",
            "Train loss: 0.005344845820218325\n",
            "Training epoch:  337 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0053\n",
            "Train loss: 0.0053366138599812984\n",
            "Training epoch:  338 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0053\n",
            "Train loss: 0.005328409373760223\n",
            "Training epoch:  339 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0053\n",
            "Train loss: 0.005289877764880657\n",
            "Training epoch:  340 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0053\n",
            "Train loss: 0.005337258335202932\n",
            "Training epoch:  341 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0053\n",
            "Train loss: 0.005330945365130901\n",
            "Training epoch:  342 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0054\n",
            "Train loss: 0.0053550745360553265\n",
            "Training epoch:  343 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0054\n",
            "Train loss: 0.0053961495868861675\n",
            "Training epoch:  344 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0054\n",
            "Train loss: 0.005374498665332794\n",
            "Training epoch:  345 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0054\n",
            "Train loss: 0.005386014934629202\n",
            "Training epoch:  346 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0053\n",
            "Train loss: 0.005344478413462639\n",
            "Training epoch:  347 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0053\n",
            "Train loss: 0.005335346795618534\n",
            "Training epoch:  348 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0053\n",
            "Train loss: 0.005299414973706007\n",
            "Training epoch:  349 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0053\n",
            "Train loss: 0.005282293073832989\n",
            "Training epoch:  350 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0053\n",
            "Train loss: 0.005268631502985954\n",
            "Training epoch:  351 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0053\n",
            "Train loss: 0.005276593845337629\n",
            "Training epoch:  352 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0053\n",
            "Train loss: 0.005285430699586868\n",
            "Training epoch:  353 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0053\n",
            "Train loss: 0.0053078653290867805\n",
            "Training epoch:  354 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0053\n",
            "Train loss: 0.005259941332042217\n",
            "Training epoch:  355 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0053\n",
            "Train loss: 0.005254546646028757\n",
            "Training epoch:  356 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0052\n",
            "Train loss: 0.005215381272137165\n",
            "Training epoch:  357 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0053\n",
            "Train loss: 0.005270694382488728\n",
            "Training epoch:  358 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0053\n",
            "Train loss: 0.005275798961520195\n",
            "Training epoch:  359 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0053\n",
            "Train loss: 0.0052755107171833515\n",
            "Training epoch:  360 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0053\n",
            "Train loss: 0.005290356930345297\n",
            "Training epoch:  361 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0053\n",
            "Train loss: 0.005265702027827501\n",
            "Training epoch:  362 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0052\n",
            "Train loss: 0.005249479785561562\n",
            "Training epoch:  363 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0052\n",
            "Train loss: 0.005217650439590216\n",
            "Training epoch:  364 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0052\n",
            "Train loss: 0.005227701272815466\n",
            "Training epoch:  365 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0052\n",
            "Train loss: 0.0052117821760475636\n",
            "Training epoch:  366 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0052\n",
            "Train loss: 0.005227287765592337\n",
            "Training epoch:  367 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0052\n",
            "Train loss: 0.005219728220254183\n",
            "Training epoch:  368 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0052\n",
            "Train loss: 0.005220403894782066\n",
            "Training epoch:  369 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0052\n",
            "Train loss: 0.005200464278459549\n",
            "Training epoch:  370 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0052\n",
            "Train loss: 0.005212970077991486\n",
            "Training epoch:  371 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0052\n",
            "Train loss: 0.00520817656069994\n",
            "Training epoch:  372 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0052\n",
            "Train loss: 0.005240497179329395\n",
            "Training epoch:  373 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0052\n",
            "Train loss: 0.005220048129558563\n",
            "Training epoch:  374 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0052\n",
            "Train loss: 0.0052457512356340885\n",
            "Training epoch:  375 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0052\n",
            "Train loss: 0.005205831024795771\n",
            "Training epoch:  376 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0052\n",
            "Train loss: 0.005170597229152918\n",
            "Training epoch:  377 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0052\n",
            "Train loss: 0.00516089703887701\n",
            "Training epoch:  378 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0052\n",
            "Train loss: 0.005167483352124691\n",
            "Training epoch:  379 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0052\n",
            "Train loss: 0.005198800470679998\n",
            "Training epoch:  380 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0052\n",
            "Train loss: 0.00522207748144865\n",
            "Training epoch:  381 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0052\n",
            "Train loss: 0.005232709925621748\n",
            "Training epoch:  382 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0053\n",
            "Train loss: 0.005256038624793291\n",
            "Training epoch:  383 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0052\n",
            "Train loss: 0.005236796569079161\n",
            "Training epoch:  384 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0052\n",
            "Train loss: 0.005204329267144203\n",
            "Training epoch:  385 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0052\n",
            "Train loss: 0.0051758186891674995\n",
            "Training epoch:  386 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0052\n",
            "Train loss: 0.0051624104380607605\n",
            "Training epoch:  387 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0052\n",
            "Train loss: 0.005161636509001255\n",
            "Training epoch:  388 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0051\n",
            "Train loss: 0.005137487314641476\n",
            "Training epoch:  389 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0051\n",
            "Train loss: 0.005117480643093586\n",
            "Training epoch:  390 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0051\n",
            "Train loss: 0.0051080333068966866\n",
            "Training epoch:  391 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0051\n",
            "Train loss: 0.005138256587088108\n",
            "Training epoch:  392 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0051\n",
            "Train loss: 0.005142335779964924\n",
            "Training epoch:  393 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0051\n",
            "Train loss: 0.005114068742841482\n",
            "Training epoch:  394 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0051\n",
            "Train loss: 0.005140671506524086\n",
            "Training epoch:  395 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0051\n",
            "Train loss: 0.005115070845931768\n",
            "Training epoch:  396 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0051\n",
            "Train loss: 0.005121930968016386\n",
            "Training epoch:  397 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0051\n",
            "Train loss: 0.005116529297083616\n",
            "Training epoch:  398 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0051\n",
            "Train loss: 0.005127064883708954\n",
            "Training epoch:  399 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0051\n",
            "Train loss: 0.005131129175424576\n",
            "...Saved PCA statistics and last model.\n",
            "Latent Mean values:  [0.08453271 0.03894323 0.2846269  0.18837395 0.0928702  0.16750969]\n",
            "Latent PCA values:  [5.66429015 4.03526065 3.25149957 3.02973545 2.83968936 1.89537201]\n",
            "Training epoch:  400 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0051\n",
            "Train loss: 0.005148133262991905\n",
            "Training epoch:  401 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0051\n",
            "Train loss: 0.0051258583553135395\n",
            "Training epoch:  402 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0051\n",
            "Train loss: 0.0051321410574018955\n",
            "Training epoch:  403 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0051\n",
            "Train loss: 0.005080827046185732\n",
            "Training epoch:  404 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0051\n",
            "Train loss: 0.0050858319737017155\n",
            "Training epoch:  405 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0050\n",
            "Train loss: 0.005047237500548363\n",
            "Training epoch:  406 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0051\n",
            "Train loss: 0.005071777850389481\n",
            "Training epoch:  407 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0051\n",
            "Train loss: 0.005065428093075752\n",
            "Training epoch:  408 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0051\n",
            "Train loss: 0.005125838797539473\n",
            "Training epoch:  409 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0051\n",
            "Train loss: 0.005114031955599785\n",
            "Training epoch:  410 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0051\n",
            "Train loss: 0.005111338570713997\n",
            "Training epoch:  411 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0051\n",
            "Train loss: 0.005089153535664082\n",
            "Training epoch:  412 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0051\n",
            "Train loss: 0.0050784447230398655\n",
            "Training epoch:  413 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0051\n",
            "Train loss: 0.005069748964160681\n",
            "Training epoch:  414 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0051\n",
            "Train loss: 0.005060555879026651\n",
            "Training epoch:  415 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0050\n",
            "Train loss: 0.005027948413044214\n",
            "Training epoch:  416 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0050\n",
            "Train loss: 0.005004882346838713\n",
            "Training epoch:  417 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0050\n",
            "Train loss: 0.004977537784725428\n",
            "Training epoch:  418 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0050\n",
            "Train loss: 0.004972436465322971\n",
            "Training epoch:  419 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0050\n",
            "Train loss: 0.00500074028968811\n",
            "Training epoch:  420 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0050\n",
            "Train loss: 0.005025740247219801\n",
            "Training epoch:  421 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0050\n",
            "Train loss: 0.00503881648182869\n",
            "Training epoch:  422 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0050\n",
            "Train loss: 0.005032016430050135\n",
            "Training epoch:  423 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0051\n",
            "Train loss: 0.005061814095824957\n",
            "Training epoch:  424 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0051\n",
            "Train loss: 0.005054984241724014\n",
            "Training epoch:  425 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0051\n",
            "Train loss: 0.005091716535389423\n",
            "Training epoch:  426 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0051\n",
            "Train loss: 0.0050830780528485775\n",
            "Training epoch:  427 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0050\n",
            "Train loss: 0.00502846110612154\n",
            "Training epoch:  428 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0050\n",
            "Train loss: 0.005032068118453026\n",
            "Training epoch:  429 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0050\n",
            "Train loss: 0.004984711762517691\n",
            "Training epoch:  430 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0050\n",
            "Train loss: 0.005013752728700638\n",
            "Training epoch:  431 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0050\n",
            "Train loss: 0.004998423159122467\n",
            "Training epoch:  432 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0050\n",
            "Train loss: 0.005014043767005205\n",
            "Training epoch:  433 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0050\n",
            "Train loss: 0.005009158980101347\n",
            "Training epoch:  434 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0050\n",
            "Train loss: 0.005012397654354572\n",
            "Training epoch:  435 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0050\n",
            "Train loss: 0.004985881503671408\n",
            "Training epoch:  436 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0050\n",
            "Train loss: 0.004961618687957525\n",
            "Training epoch:  437 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0049\n",
            "Train loss: 0.004915209952741861\n",
            "Training epoch:  438 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0049\n",
            "Train loss: 0.004940089304000139\n",
            "Training epoch:  439 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0049\n",
            "Train loss: 0.004912801086902618\n",
            "Training epoch:  440 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0049\n",
            "Train loss: 0.004927079193294048\n",
            "Training epoch:  441 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0049\n",
            "Train loss: 0.004902233835309744\n",
            "Training epoch:  442 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0050\n",
            "Train loss: 0.004973899573087692\n",
            "Training epoch:  443 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0050\n",
            "Train loss: 0.004972454160451889\n",
            "Training epoch:  444 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0050\n",
            "Train loss: 0.004973916336894035\n",
            "Training epoch:  445 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0050\n",
            "Train loss: 0.0049830093048512936\n",
            "Training epoch:  446 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0050\n",
            "Train loss: 0.004966260399669409\n",
            "Training epoch:  447 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0049\n",
            "Train loss: 0.0049487329088151455\n",
            "Training epoch:  448 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0049\n",
            "Train loss: 0.004940599203109741\n",
            "Training epoch:  449 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0049\n",
            "Train loss: 0.004926346242427826\n",
            "Training epoch:  450 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0049\n",
            "Train loss: 0.004924717824906111\n",
            "Training epoch:  451 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0049\n",
            "Train loss: 0.004931301344186068\n",
            "Training epoch:  452 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0049\n",
            "Train loss: 0.004915119148790836\n",
            "Training epoch:  453 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0049\n",
            "Train loss: 0.004918848164379597\n",
            "Training epoch:  454 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0049\n",
            "Train loss: 0.004916458856314421\n",
            "Training epoch:  455 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0049\n",
            "Train loss: 0.004882674664258957\n",
            "Training epoch:  456 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0049\n",
            "Train loss: 0.004879738669842482\n",
            "Training epoch:  457 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0049\n",
            "Train loss: 0.004891666118055582\n",
            "Training epoch:  458 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0049\n",
            "Train loss: 0.004901693668216467\n",
            "Training epoch:  459 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0049\n",
            "Train loss: 0.004918885417282581\n",
            "Training epoch:  460 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0049\n",
            "Train loss: 0.004883316345512867\n",
            "Training epoch:  461 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0049\n",
            "Train loss: 0.004851581994444132\n",
            "Training epoch:  462 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0049\n",
            "Train loss: 0.004853392019867897\n",
            "Training epoch:  463 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0049\n",
            "Train loss: 0.0048560407012701035\n",
            "Training epoch:  464 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0049\n",
            "Train loss: 0.004866001661866903\n",
            "Training epoch:  465 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0049\n",
            "Train loss: 0.004857090301811695\n",
            "Training epoch:  466 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0049\n",
            "Train loss: 0.004880635999143124\n",
            "Training epoch:  467 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0049\n",
            "Train loss: 0.004873464349657297\n",
            "Training epoch:  468 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0049\n",
            "Train loss: 0.004903801251202822\n",
            "Training epoch:  469 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0049\n",
            "Train loss: 0.004893438890576363\n",
            "Training epoch:  470 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0049\n",
            "Train loss: 0.004886473063379526\n",
            "Training epoch:  471 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0048\n",
            "Train loss: 0.004837716929614544\n",
            "Training epoch:  472 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0048\n",
            "Train loss: 0.004816112108528614\n",
            "Training epoch:  473 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0048\n",
            "Train loss: 0.004791374783962965\n",
            "Training epoch:  474 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0048\n",
            "Train loss: 0.004779882729053497\n",
            "Training epoch:  475 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0048\n",
            "Train loss: 0.0047753038816154\n",
            "Training epoch:  476 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0048\n",
            "Train loss: 0.004839329514652491\n",
            "Training epoch:  477 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0048\n",
            "Train loss: 0.004829561337828636\n",
            "Training epoch:  478 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0048\n",
            "Train loss: 0.004817832261323929\n",
            "Training epoch:  479 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0048\n",
            "Train loss: 0.00480644591152668\n",
            "Training epoch:  480 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0048\n",
            "Train loss: 0.004808719735592604\n",
            "Training epoch:  481 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0048\n",
            "Train loss: 0.0048090447671711445\n",
            "Training epoch:  482 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0048\n",
            "Train loss: 0.004786013159900904\n",
            "Training epoch:  483 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0048\n",
            "Train loss: 0.004776632413268089\n",
            "Training epoch:  484 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0048\n",
            "Train loss: 0.004754265304654837\n",
            "Training epoch:  485 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0048\n",
            "Train loss: 0.004764987155795097\n",
            "Training epoch:  486 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0048\n",
            "Train loss: 0.004785529803484678\n",
            "Training epoch:  487 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0048\n",
            "Train loss: 0.00481937313452363\n",
            "Training epoch:  488 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0048\n",
            "Train loss: 0.0048005771823227406\n",
            "Training epoch:  489 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0048\n",
            "Train loss: 0.004780681803822517\n",
            "Training epoch:  490 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0048\n",
            "Train loss: 0.004757216200232506\n",
            "Training epoch:  491 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0047\n",
            "Train loss: 0.004730104468762875\n",
            "Training epoch:  492 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0047\n",
            "Train loss: 0.0047350479289889336\n",
            "Training epoch:  493 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0047\n",
            "Train loss: 0.0047384994104504585\n",
            "Training epoch:  494 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0047\n",
            "Train loss: 0.004738368093967438\n",
            "Training epoch:  495 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0047\n",
            "Train loss: 0.004715537652373314\n",
            "Training epoch:  496 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0047\n",
            "Train loss: 0.00474067497998476\n",
            "Training epoch:  497 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0047\n",
            "Train loss: 0.004726744256913662\n",
            "Training epoch:  498 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0048\n",
            "Train loss: 0.004751298576593399\n",
            "Training epoch:  499 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0047\n",
            "Train loss: 0.004735597874969244\n",
            "...Saved PCA statistics and last model.\n",
            "Latent Mean values:  [ 0.34655705 -0.16624472  0.4109515   0.48196283  0.18130493 -0.244519  ]\n",
            "Latent PCA values:  [4.53188658 3.73652162 3.1059255  2.90656709 2.69801639 2.08490415]\n",
            "Training epoch:  500 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0047\n",
            "Train loss: 0.004746057093143463\n",
            "Training epoch:  501 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0047\n",
            "Train loss: 0.004743564873933792\n",
            "Training epoch:  502 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0047\n",
            "Train loss: 0.004722070414572954\n",
            "Training epoch:  503 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0047\n",
            "Train loss: 0.004684397019445896\n",
            "Training epoch:  504 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0047\n",
            "Train loss: 0.004686013795435429\n",
            "Training epoch:  505 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0047\n",
            "Train loss: 0.004708487540483475\n",
            "Training epoch:  506 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0047\n",
            "Train loss: 0.004731164313852787\n",
            "Training epoch:  507 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0048\n",
            "Train loss: 0.004758899100124836\n",
            "Training epoch:  508 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0047\n",
            "Train loss: 0.004732684697955847\n",
            "Training epoch:  509 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0047\n",
            "Train loss: 0.0047325813211500645\n",
            "Training epoch:  510 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0047\n",
            "Train loss: 0.004727156367152929\n",
            "Training epoch:  511 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0047\n",
            "Train loss: 0.004705999046564102\n",
            "Training epoch:  512 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0047\n",
            "Train loss: 0.004654791671782732\n",
            "Training epoch:  513 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0047\n",
            "Train loss: 0.0046507264487445354\n",
            "Training epoch:  514 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0046\n",
            "Train loss: 0.00462329713627696\n",
            "Training epoch:  515 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0047\n",
            "Train loss: 0.004651112947613001\n",
            "Training epoch:  516 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0046\n",
            "Train loss: 0.004624482709914446\n",
            "Training epoch:  517 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0046\n",
            "Train loss: 0.004625081084668636\n",
            "Training epoch:  518 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0046\n",
            "Train loss: 0.004621945321559906\n",
            "Training epoch:  519 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0046\n",
            "Train loss: 0.004620262887328863\n",
            "Training epoch:  520 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0046\n",
            "Train loss: 0.004633126314729452\n",
            "Training epoch:  521 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0046\n",
            "Train loss: 0.004646009299904108\n",
            "Training epoch:  522 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0046\n",
            "Train loss: 0.004635415505617857\n",
            "Training epoch:  523 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0046\n",
            "Train loss: 0.0046464065089821815\n",
            "Training epoch:  524 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0046\n",
            "Train loss: 0.0045967563055455685\n",
            "Training epoch:  525 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0046\n",
            "Train loss: 0.004599912092089653\n",
            "Training epoch:  526 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0046\n",
            "Train loss: 0.004565984010696411\n",
            "Training epoch:  527 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0046\n",
            "Train loss: 0.0045936680398881435\n",
            "Training epoch:  528 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0046\n",
            "Train loss: 0.004584906622767448\n",
            "Training epoch:  529 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0046\n",
            "Train loss: 0.004585935268551111\n",
            "Training epoch:  530 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0046\n",
            "Train loss: 0.0046014199033379555\n",
            "Training epoch:  531 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0046\n",
            "Train loss: 0.004593442659825087\n",
            "Training epoch:  532 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0046\n",
            "Train loss: 0.004612043499946594\n",
            "Training epoch:  533 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0046\n",
            "Train loss: 0.004606023896485567\n",
            "Training epoch:  534 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0046\n",
            "Train loss: 0.0045933849178254604\n",
            "Training epoch:  535 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0046\n",
            "Train loss: 0.0045672194100916386\n",
            "Training epoch:  536 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0045\n",
            "Train loss: 0.004536747932434082\n",
            "Training epoch:  537 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0045\n",
            "Train loss: 0.004529973026365042\n",
            "Training epoch:  538 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0046\n",
            "Train loss: 0.004553847014904022\n",
            "Training epoch:  539 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0046\n",
            "Train loss: 0.004562851972877979\n",
            "Training epoch:  540 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0046\n",
            "Train loss: 0.004562818445265293\n",
            "Training epoch:  541 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0045\n",
            "Train loss: 0.004532288759946823\n",
            "Training epoch:  542 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0045\n",
            "Train loss: 0.004524814430624247\n",
            "Training epoch:  543 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0045\n",
            "Train loss: 0.004514302592724562\n",
            "Training epoch:  544 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0046\n",
            "Train loss: 0.004550115671008825\n",
            "Training epoch:  545 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0045\n",
            "Train loss: 0.004510813858360052\n",
            "Training epoch:  546 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0045\n",
            "Train loss: 0.004486197605729103\n",
            "Training epoch:  547 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0045\n",
            "Train loss: 0.004488869570195675\n",
            "Training epoch:  548 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0045\n",
            "Train loss: 0.0044809505343437195\n",
            "Training epoch:  549 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0045\n",
            "Train loss: 0.004496659617871046\n",
            "Training epoch:  550 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0045\n",
            "Train loss: 0.004483575467020273\n",
            "Training epoch:  551 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0045\n",
            "Train loss: 0.00450535025447607\n",
            "Training epoch:  552 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0045\n",
            "Train loss: 0.004481191746890545\n",
            "Training epoch:  553 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0045\n",
            "Train loss: 0.004487872589379549\n",
            "Training epoch:  554 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0045\n",
            "Train loss: 0.004470545332878828\n",
            "Training epoch:  555 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0045\n",
            "Train loss: 0.004466639831662178\n",
            "Training epoch:  556 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0045\n",
            "Train loss: 0.004453055094927549\n",
            "Training epoch:  557 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0045\n",
            "Train loss: 0.004478540271520615\n",
            "Training epoch:  558 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0045\n",
            "Train loss: 0.00448104552924633\n",
            "Training epoch:  559 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0045\n",
            "Train loss: 0.004510586149990559\n",
            "Training epoch:  560 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0045\n",
            "Train loss: 0.004457210656255484\n",
            "Training epoch:  561 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0045\n",
            "Train loss: 0.004510210826992989\n",
            "Training epoch:  562 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0044\n",
            "Train loss: 0.004449101164937019\n",
            "Training epoch:  563 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0044\n",
            "Train loss: 0.0044172084890306\n",
            "Training epoch:  564 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0044\n",
            "Train loss: 0.004404126200824976\n",
            "Training epoch:  565 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0044\n",
            "Train loss: 0.004384273663163185\n",
            "Training epoch:  566 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0044\n",
            "Train loss: 0.004399681463837624\n",
            "Training epoch:  567 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0044\n",
            "Train loss: 0.00439430819824338\n",
            "Training epoch:  568 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0044\n",
            "Train loss: 0.004399617202579975\n",
            "Training epoch:  569 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0044\n",
            "Train loss: 0.004361829254776239\n",
            "Training epoch:  570 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0044\n",
            "Train loss: 0.004366826731711626\n",
            "Training epoch:  571 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0044\n",
            "Train loss: 0.0043585700914263725\n",
            "Training epoch:  572 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0044\n",
            "Train loss: 0.004366997163742781\n",
            "Training epoch:  573 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0044\n",
            "Train loss: 0.00436312984675169\n",
            "Training epoch:  574 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0044\n",
            "Train loss: 0.004389322362840176\n",
            "Training epoch:  575 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0044\n",
            "Train loss: 0.004364583641290665\n",
            "Training epoch:  576 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0044\n",
            "Train loss: 0.004369016271084547\n",
            "Training epoch:  577 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0043\n",
            "Train loss: 0.004348717629909515\n",
            "Training epoch:  578 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0044\n",
            "Train loss: 0.00437776418402791\n",
            "Training epoch:  579 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0044\n",
            "Train loss: 0.00435780081897974\n",
            "Training epoch:  580 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0044\n",
            "Train loss: 0.004364065360277891\n",
            "Training epoch:  581 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0044\n",
            "Train loss: 0.0043946607038378716\n",
            "Training epoch:  582 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0044\n",
            "Train loss: 0.0043866573832929134\n",
            "Training epoch:  583 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0044\n",
            "Train loss: 0.00437552947551012\n",
            "Training epoch:  584 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0043\n",
            "Train loss: 0.0043163904920220375\n",
            "Training epoch:  585 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0043\n",
            "Train loss: 0.004288363270461559\n",
            "Training epoch:  586 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0043\n",
            "Train loss: 0.004277319181710482\n",
            "Training epoch:  587 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0043\n",
            "Train loss: 0.004257760476320982\n",
            "Training epoch:  588 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0042\n",
            "Train loss: 0.00424245884642005\n",
            "Training epoch:  589 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0043\n",
            "Train loss: 0.004278307780623436\n",
            "Training epoch:  590 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0043\n",
            "Train loss: 0.004307202063500881\n",
            "Training epoch:  591 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0043\n",
            "Train loss: 0.004314221907407045\n",
            "Training epoch:  592 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0043\n",
            "Train loss: 0.004301088862121105\n",
            "Training epoch:  593 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0043\n",
            "Train loss: 0.004276664927601814\n",
            "Training epoch:  594 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0043\n",
            "Train loss: 0.004279658664017916\n",
            "Training epoch:  595 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0043\n",
            "Train loss: 0.004310532007366419\n",
            "Training epoch:  596 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0043\n",
            "Train loss: 0.004304385278373957\n",
            "Training epoch:  597 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0043\n",
            "Train loss: 0.0042583090253174305\n",
            "Training epoch:  598 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0043\n",
            "Train loss: 0.0042602382600307465\n",
            "Training epoch:  599 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0042\n",
            "Train loss: 0.004235825035721064\n",
            "...Saved PCA statistics and last model.\n",
            "Latent Mean values:  [-0.06421407 -0.6781393   0.23821044  0.28333196 -0.07244309  0.17355473]\n",
            "Latent PCA values:  [3.73674546 3.45184291 2.83783518 2.74779531 2.49969428 2.26202408]\n",
            "Training epoch:  600 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0043\n",
            "Train loss: 0.0042670052498579025\n",
            "Training epoch:  601 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0043\n",
            "Train loss: 0.004259871318936348\n",
            "Training epoch:  602 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0043\n",
            "Train loss: 0.004274268634617329\n",
            "Training epoch:  603 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0043\n",
            "Train loss: 0.004274951294064522\n",
            "Training epoch:  604 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0043\n",
            "Train loss: 0.004281261470168829\n",
            "Training epoch:  605 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0043\n",
            "Train loss: 0.004259295295923948\n",
            "Training epoch:  606 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0042\n",
            "Train loss: 0.004249768331646919\n",
            "Training epoch:  607 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0042\n",
            "Train loss: 0.004212681204080582\n",
            "Training epoch:  608 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0042\n",
            "Train loss: 0.004199272487312555\n",
            "Training epoch:  609 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0042\n",
            "Train loss: 0.004175698850303888\n",
            "Training epoch:  610 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0042\n",
            "Train loss: 0.004183746874332428\n",
            "Training epoch:  611 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0042\n",
            "Train loss: 0.004161102697253227\n",
            "Training epoch:  612 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0042\n",
            "Train loss: 0.0041967034339904785\n",
            "Training epoch:  613 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0042\n",
            "Train loss: 0.004217605106532574\n",
            "Training epoch:  614 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0042\n",
            "Train loss: 0.0041815792210400105\n",
            "Training epoch:  615 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0042\n",
            "Train loss: 0.004175599664449692\n",
            "Training epoch:  616 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0042\n",
            "Train loss: 0.004183987621217966\n",
            "Training epoch:  617 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0042\n",
            "Train loss: 0.004205462522804737\n",
            "Training epoch:  618 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0042\n",
            "Train loss: 0.004222868010401726\n",
            "Training epoch:  619 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0042\n",
            "Train loss: 0.0042269909754395485\n",
            "Training epoch:  620 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0042\n",
            "Train loss: 0.004226967226713896\n",
            "Training epoch:  621 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0042\n",
            "Train loss: 0.004187826998531818\n",
            "Training epoch:  622 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0042\n",
            "Train loss: 0.004159095231443644\n",
            "Training epoch:  623 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0041\n",
            "Train loss: 0.004124172031879425\n",
            "Training epoch:  624 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0041\n",
            "Train loss: 0.004111667163670063\n",
            "Training epoch:  625 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0041\n",
            "Train loss: 0.004122960846871138\n",
            "Training epoch:  626 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0041\n",
            "Train loss: 0.004101775586605072\n",
            "Training epoch:  627 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0041\n",
            "Train loss: 0.004117718432098627\n",
            "Training epoch:  628 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0041\n",
            "Train loss: 0.004096685908734798\n",
            "Training epoch:  629 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0041\n",
            "Train loss: 0.004125798121094704\n",
            "Training epoch:  630 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0041\n",
            "Train loss: 0.004131319001317024\n",
            "Training epoch:  631 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0041\n",
            "Train loss: 0.004139872267842293\n",
            "Training epoch:  632 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0041\n",
            "Train loss: 0.004123408813029528\n",
            "Training epoch:  633 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0041\n",
            "Train loss: 0.004117804579436779\n",
            "Training epoch:  634 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0041\n",
            "Train loss: 0.00412807846441865\n",
            "Training epoch:  635 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0041\n",
            "Train loss: 0.0040913657285273075\n",
            "Training epoch:  636 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0041\n",
            "Train loss: 0.004059897270053625\n",
            "Training epoch:  637 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0041\n",
            "Train loss: 0.0040518674068152905\n",
            "Training epoch:  638 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0040\n",
            "Train loss: 0.004044456873089075\n",
            "Training epoch:  639 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0041\n",
            "Train loss: 0.00405315775424242\n",
            "Training epoch:  640 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0041\n",
            "Train loss: 0.004060615785419941\n",
            "Training epoch:  641 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0041\n",
            "Train loss: 0.004077799618244171\n",
            "Training epoch:  642 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0041\n",
            "Train loss: 0.004107190296053886\n",
            "Training epoch:  643 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0040\n",
            "Train loss: 0.004048113711178303\n",
            "Training epoch:  644 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0040\n",
            "Train loss: 0.004024937283247709\n",
            "Training epoch:  645 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0040\n",
            "Train loss: 0.003965453710407019\n",
            "Training epoch:  646 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0040\n",
            "Train loss: 0.0040220716036856174\n",
            "Training epoch:  647 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0040\n",
            "Train loss: 0.004031195770949125\n",
            "Training epoch:  648 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0041\n",
            "Train loss: 0.004061459563672543\n",
            "Training epoch:  649 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0041\n",
            "Train loss: 0.004064048640429974\n",
            "Training epoch:  650 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0040\n",
            "Train loss: 0.004048254806548357\n",
            "Training epoch:  651 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0041\n",
            "Train loss: 0.004055452067404985\n",
            "Training epoch:  652 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0040\n",
            "Train loss: 0.004034597892314196\n",
            "Training epoch:  653 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0040\n",
            "Train loss: 0.004036933183670044\n",
            "Training epoch:  654 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0040\n",
            "Train loss: 0.004022051580250263\n",
            "Training epoch:  655 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0040\n",
            "Train loss: 0.003998412285000086\n",
            "Training epoch:  656 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0040\n",
            "Train loss: 0.003984744660556316\n",
            "Training epoch:  657 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0040\n",
            "Train loss: 0.003986522555351257\n",
            "Training epoch:  658 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0040\n",
            "Train loss: 0.003967483527958393\n",
            "Training epoch:  659 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0040\n",
            "Train loss: 0.0039772107265889645\n",
            "Training epoch:  660 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0039\n",
            "Train loss: 0.003937274683266878\n",
            "Training epoch:  661 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0039\n",
            "Train loss: 0.003942571114748716\n",
            "Training epoch:  662 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0040\n",
            "Train loss: 0.003950679674744606\n",
            "Training epoch:  663 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0040\n",
            "Train loss: 0.003987118601799011\n",
            "Training epoch:  664 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0039\n",
            "Train loss: 0.0039469655603170395\n",
            "Training epoch:  665 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0040\n",
            "Train loss: 0.003958657383918762\n",
            "Training epoch:  666 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0040\n",
            "Train loss: 0.003951904829591513\n",
            "Training epoch:  667 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0040\n",
            "Train loss: 0.0039507239125669\n",
            "Training epoch:  668 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0039\n",
            "Train loss: 0.003943264950066805\n",
            "Training epoch:  669 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0039\n",
            "Train loss: 0.003919890150427818\n",
            "Training epoch:  670 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0039\n",
            "Train loss: 0.003924689721316099\n",
            "Training epoch:  671 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0039\n",
            "Train loss: 0.003937022760510445\n",
            "Training epoch:  672 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0039\n",
            "Train loss: 0.0039384677074849606\n",
            "Training epoch:  673 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0039\n",
            "Train loss: 0.003929908387362957\n",
            "Training epoch:  674 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0039\n",
            "Train loss: 0.003894251771271229\n",
            "Training epoch:  675 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0039\n",
            "Train loss: 0.0038612685166299343\n",
            "Training epoch:  676 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0039\n",
            "Train loss: 0.0038786926306784153\n",
            "Training epoch:  677 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0039\n",
            "Train loss: 0.0038813650608062744\n",
            "Training epoch:  678 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0039\n",
            "Train loss: 0.0039061016868799925\n",
            "Training epoch:  679 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0039\n",
            "Train loss: 0.0038858898915350437\n",
            "Training epoch:  680 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0039\n",
            "Train loss: 0.003939938265830278\n",
            "Training epoch:  681 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0039\n",
            "Train loss: 0.003918255679309368\n",
            "Training epoch:  682 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0039\n",
            "Train loss: 0.0038782849442213774\n",
            "Training epoch:  683 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0039\n",
            "Train loss: 0.0038553779013454914\n",
            "Training epoch:  684 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0038\n",
            "Train loss: 0.0038470549043267965\n",
            "Training epoch:  685 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0039\n",
            "Train loss: 0.003893509041517973\n",
            "Training epoch:  686 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0039\n",
            "Train loss: 0.003895644098520279\n",
            "Training epoch:  687 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0039\n",
            "Train loss: 0.0038938699290156364\n",
            "Training epoch:  688 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0039\n",
            "Train loss: 0.0038682129234075546\n",
            "Training epoch:  689 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0039\n",
            "Train loss: 0.0038575062062591314\n",
            "Training epoch:  690 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0038\n",
            "Train loss: 0.003837862517684698\n",
            "Training epoch:  691 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0038\n",
            "Train loss: 0.0038264107424765825\n",
            "Training epoch:  692 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0038\n",
            "Train loss: 0.0038054981268942356\n",
            "Training epoch:  693 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0038\n",
            "Train loss: 0.003819800680503249\n",
            "Training epoch:  694 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0038\n",
            "Train loss: 0.0038009281270205975\n",
            "Training epoch:  695 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0038\n",
            "Train loss: 0.0038056583143770695\n",
            "Training epoch:  696 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0038\n",
            "Train loss: 0.0037781826686114073\n",
            "Training epoch:  697 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0038\n",
            "Train loss: 0.0038005278911441565\n",
            "Training epoch:  698 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0038\n",
            "Train loss: 0.0037866737693548203\n",
            "Training epoch:  699 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0038\n",
            "Train loss: 0.0037626668345183134\n",
            "...Saved PCA statistics and last model.\n",
            "Latent Mean values:  [-0.00218587 -0.10576602  0.23381795  0.06928539  0.02339952 -0.05771682]\n",
            "Latent PCA values:  [3.34898935 3.14809263 2.81816533 2.7464653  2.4986937  2.27451216]\n",
            "Training epoch:  700 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0038\n",
            "Train loss: 0.0037785598542541265\n",
            "Training epoch:  701 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0038\n",
            "Train loss: 0.003776851575821638\n",
            "Training epoch:  702 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0038\n",
            "Train loss: 0.003798879450187087\n",
            "Training epoch:  703 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0038\n",
            "Train loss: 0.0038297015707939863\n",
            "Training epoch:  704 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0038\n",
            "Train loss: 0.0038172497879713774\n",
            "Training epoch:  705 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0038\n",
            "Train loss: 0.0038109382148832083\n",
            "Training epoch:  706 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0038\n",
            "Train loss: 0.003817041637375951\n",
            "Training epoch:  707 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0038\n",
            "Train loss: 0.003797560930252075\n",
            "Training epoch:  708 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0038\n",
            "Train loss: 0.003813923103734851\n",
            "Training epoch:  709 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0038\n",
            "Train loss: 0.0037977590691298246\n",
            "Training epoch:  710 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0038\n",
            "Train loss: 0.003771372837945819\n",
            "Training epoch:  711 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0037\n",
            "Train loss: 0.0037105060182511806\n",
            "Training epoch:  712 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0037\n",
            "Train loss: 0.003707418218255043\n",
            "Training epoch:  713 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0037\n",
            "Train loss: 0.0036934558302164078\n",
            "Training epoch:  714 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0037\n",
            "Train loss: 0.0037255368661135435\n",
            "Training epoch:  715 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0037\n",
            "Train loss: 0.0037317799869924784\n",
            "Training epoch:  716 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0037\n",
            "Train loss: 0.0037042200565338135\n",
            "Training epoch:  717 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0037\n",
            "Train loss: 0.0037099889013916254\n",
            "Training epoch:  718 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0037\n",
            "Train loss: 0.0036954768002033234\n",
            "Training epoch:  719 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0037\n",
            "Train loss: 0.0037140045315027237\n",
            "Training epoch:  720 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0037\n",
            "Train loss: 0.0037283306010067463\n",
            "Training epoch:  721 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0037\n",
            "Train loss: 0.0037404350005090237\n",
            "Training epoch:  722 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0037\n",
            "Train loss: 0.0037371290381997824\n",
            "Training epoch:  723 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0037\n",
            "Train loss: 0.0037334070075303316\n",
            "Training epoch:  724 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0037\n",
            "Train loss: 0.003688477911055088\n",
            "Training epoch:  725 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0037\n",
            "Train loss: 0.003664033254608512\n",
            "Training epoch:  726 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0037\n",
            "Train loss: 0.0036553945392370224\n",
            "Training epoch:  727 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0037\n",
            "Train loss: 0.0036614376585930586\n",
            "Training epoch:  728 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0036\n",
            "Train loss: 0.003636040026322007\n",
            "Training epoch:  729 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0037\n",
            "Train loss: 0.0036598360165953636\n",
            "Training epoch:  730 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0037\n",
            "Train loss: 0.0036759264767169952\n",
            "Training epoch:  731 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0037\n",
            "Train loss: 0.003706606337800622\n",
            "Training epoch:  732 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0037\n",
            "Train loss: 0.0036880411207675934\n",
            "Training epoch:  733 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0037\n",
            "Train loss: 0.0036730743013322353\n",
            "Training epoch:  734 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0037\n",
            "Train loss: 0.0036550380755215883\n",
            "Training epoch:  735 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0036\n",
            "Train loss: 0.003624241566285491\n",
            "Training epoch:  736 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0036\n",
            "Train loss: 0.003619419876486063\n",
            "Training epoch:  737 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0036\n",
            "Train loss: 0.0036084039602428675\n",
            "Training epoch:  738 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0036\n",
            "Train loss: 0.0036099657882004976\n",
            "Training epoch:  739 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0036\n",
            "Train loss: 0.0036235773004591465\n",
            "Training epoch:  740 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0036\n",
            "Train loss: 0.003596344031393528\n",
            "Training epoch:  741 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0036\n",
            "Train loss: 0.0036142137832939625\n",
            "Training epoch:  742 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0036\n",
            "Train loss: 0.003630509600043297\n",
            "Training epoch:  743 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0036\n",
            "Train loss: 0.0036383531987667084\n",
            "Training epoch:  744 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0037\n",
            "Train loss: 0.003652755171060562\n",
            "Training epoch:  745 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0036\n",
            "Train loss: 0.003629934974014759\n",
            "Training epoch:  746 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0036\n",
            "Train loss: 0.0036232962738722563\n",
            "Training epoch:  747 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0036\n",
            "Train loss: 0.0035975018981844187\n",
            "Training epoch:  748 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0036\n",
            "Train loss: 0.0036003380082547665\n",
            "Training epoch:  749 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0036\n",
            "Train loss: 0.003593617118895054\n",
            "Training epoch:  750 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0036\n",
            "Train loss: 0.0035961938556283712\n",
            "Training epoch:  751 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0036\n",
            "Train loss: 0.003588219406083226\n",
            "Training epoch:  752 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0036\n",
            "Train loss: 0.0035634185187518597\n",
            "Training epoch:  753 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0036\n",
            "Train loss: 0.0035563712008297443\n",
            "Training epoch:  754 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0036\n",
            "Train loss: 0.0035577560774981976\n",
            "Training epoch:  755 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0036\n",
            "Train loss: 0.0035585337318480015\n",
            "Training epoch:  756 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0036\n",
            "Train loss: 0.0035892874002456665\n",
            "Training epoch:  757 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0036\n",
            "Train loss: 0.003581420285627246\n",
            "Training epoch:  758 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0036\n",
            "Train loss: 0.0035655596293509007\n",
            "Training epoch:  759 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0035\n",
            "Train loss: 0.003540005534887314\n",
            "Training epoch:  760 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0035\n",
            "Train loss: 0.00351673923432827\n",
            "Training epoch:  761 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0035\n",
            "Train loss: 0.003529330249875784\n",
            "Training epoch:  762 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0035\n",
            "Train loss: 0.0035238724667578936\n",
            "Training epoch:  763 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0035\n",
            "Train loss: 0.003529182868078351\n",
            "Training epoch:  764 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0035\n",
            "Train loss: 0.0035111461766064167\n",
            "Training epoch:  765 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0035\n",
            "Train loss: 0.0035329002421349287\n",
            "Training epoch:  766 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0035\n",
            "Train loss: 0.0035253779496997595\n",
            "Training epoch:  767 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0035\n",
            "Train loss: 0.0035102455876767635\n",
            "Training epoch:  768 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0035\n",
            "Train loss: 0.003505670465528965\n",
            "Training epoch:  769 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0035\n",
            "Train loss: 0.0035090583842247725\n",
            "Training epoch:  770 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0035\n",
            "Train loss: 0.0035023021046072245\n",
            "Training epoch:  771 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0035\n",
            "Train loss: 0.003488701768219471\n",
            "Training epoch:  772 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0035\n",
            "Train loss: 0.0034804190509021282\n",
            "Training epoch:  773 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0035\n",
            "Train loss: 0.0034554419107735157\n",
            "Training epoch:  774 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0034\n",
            "Train loss: 0.003448843490332365\n",
            "Training epoch:  775 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0034\n",
            "Train loss: 0.0034320084378123283\n",
            "Training epoch:  776 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0034\n",
            "Train loss: 0.00342162954621017\n",
            "Training epoch:  777 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0034\n",
            "Train loss: 0.003402514150366187\n",
            "Training epoch:  778 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0034\n",
            "Train loss: 0.003390588564798236\n",
            "Training epoch:  779 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0034\n",
            "Train loss: 0.0033880802802741528\n",
            "Training epoch:  780 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0034\n",
            "Train loss: 0.00341043877415359\n",
            "Training epoch:  781 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0034\n",
            "Train loss: 0.0034146495163440704\n",
            "Training epoch:  782 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0035\n",
            "Train loss: 0.0034538055770099163\n",
            "Training epoch:  783 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0035\n",
            "Train loss: 0.0034624761901795864\n",
            "Training epoch:  784 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0035\n",
            "Train loss: 0.0034542011562734842\n",
            "Training epoch:  785 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0034\n",
            "Train loss: 0.003438441315665841\n",
            "Training epoch:  786 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0034\n",
            "Train loss: 0.003412353340536356\n",
            "Training epoch:  787 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0034\n",
            "Train loss: 0.003424149937927723\n",
            "Training epoch:  788 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0034\n",
            "Train loss: 0.0034231676254421473\n",
            "Training epoch:  789 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0034\n",
            "Train loss: 0.0034370175562798977\n",
            "Training epoch:  790 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0034\n",
            "Train loss: 0.0034316733945161104\n",
            "Training epoch:  791 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0034\n",
            "Train loss: 0.0034253112971782684\n",
            "Training epoch:  792 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0034\n",
            "Train loss: 0.00341433216817677\n",
            "Training epoch:  793 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0034\n",
            "Train loss: 0.003440589178353548\n",
            "Training epoch:  794 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0034\n",
            "Train loss: 0.003405611263588071\n",
            "Training epoch:  795 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0034\n",
            "Train loss: 0.0033968526404350996\n",
            "Training epoch:  796 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0034\n",
            "Train loss: 0.0033526045735925436\n",
            "Training epoch:  797 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0033\n",
            "Train loss: 0.003327838843688369\n",
            "Training epoch:  798 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0033\n",
            "Train loss: 0.003341342555359006\n",
            "Training epoch:  799 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0034\n",
            "Train loss: 0.0033731914591044188\n",
            "...Saved PCA statistics and last model.\n",
            "Latent Mean values:  [0.18060212 0.40848073 0.03376585 0.16280006 0.0914807  0.13096811]\n",
            "Latent PCA values:  [3.23856937 2.93917449 2.80784049 2.68494093 2.47646496 2.2388481 ]\n",
            "Training epoch:  800 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0034\n",
            "Train loss: 0.0033807759173214436\n",
            "Training epoch:  801 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0034\n",
            "Train loss: 0.003374735126271844\n",
            "Training epoch:  802 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0034\n",
            "Train loss: 0.003382934955880046\n",
            "Training epoch:  803 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0034\n",
            "Train loss: 0.0033769183792173862\n",
            "Training epoch:  804 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0034\n",
            "Train loss: 0.003362740622833371\n",
            "Training epoch:  805 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0033\n",
            "Train loss: 0.003343916265293956\n",
            "Training epoch:  806 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0034\n",
            "Train loss: 0.0033556832931935787\n",
            "Training epoch:  807 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0033\n",
            "Train loss: 0.0033373169135302305\n",
            "Training epoch:  808 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0034\n",
            "Train loss: 0.0033559598959982395\n",
            "Training epoch:  809 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0033\n",
            "Train loss: 0.0033376342616975307\n",
            "Training epoch:  810 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0034\n",
            "Train loss: 0.0033587180078029633\n",
            "Training epoch:  811 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0033\n",
            "Train loss: 0.003347401972860098\n",
            "Training epoch:  812 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0033\n",
            "Train loss: 0.0033210348337888718\n",
            "Training epoch:  813 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0033\n",
            "Train loss: 0.003312460845336318\n",
            "Training epoch:  814 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0033\n",
            "Train loss: 0.0033087125048041344\n",
            "Training epoch:  815 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0033\n",
            "Train loss: 0.00330044561997056\n",
            "Training epoch:  816 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0033\n",
            "Train loss: 0.003292778041213751\n",
            "Training epoch:  817 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0033\n",
            "Train loss: 0.003295007860288024\n",
            "Training epoch:  818 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0033\n",
            "Train loss: 0.003272236557677388\n",
            "Training epoch:  819 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0033\n",
            "Train loss: 0.00328070274554193\n",
            "Training epoch:  820 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0033\n",
            "Train loss: 0.0033036419190466404\n",
            "Training epoch:  821 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0033\n",
            "Train loss: 0.003330440493300557\n",
            "Training epoch:  822 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0033\n",
            "Train loss: 0.003320620395243168\n",
            "Training epoch:  823 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0033\n",
            "Train loss: 0.003334691282361746\n",
            "Training epoch:  824 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0033\n",
            "Train loss: 0.003298067254945636\n",
            "Training epoch:  825 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0033\n",
            "Train loss: 0.003271097084507346\n",
            "Training epoch:  826 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0033\n",
            "Train loss: 0.0032515234779566526\n",
            "Training epoch:  827 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0033\n",
            "Train loss: 0.003260242287069559\n",
            "Training epoch:  828 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0033\n",
            "Train loss: 0.003258626675233245\n",
            "Training epoch:  829 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0033\n",
            "Train loss: 0.003277439624071121\n",
            "Training epoch:  830 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0033\n",
            "Train loss: 0.0032732486724853516\n",
            "Training epoch:  831 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0033\n",
            "Train loss: 0.0032634518574923277\n",
            "Training epoch:  832 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0032\n",
            "Train loss: 0.00323640089482069\n",
            "Training epoch:  833 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0033\n",
            "Train loss: 0.0032677561976015568\n",
            "Training epoch:  834 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0032\n",
            "Train loss: 0.003243461949750781\n",
            "Training epoch:  835 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0032\n",
            "Train loss: 0.003234174568206072\n",
            "Training epoch:  836 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0032\n",
            "Train loss: 0.0032332432456314564\n",
            "Training epoch:  837 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0032\n",
            "Train loss: 0.003226330503821373\n",
            "Training epoch:  838 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0032\n",
            "Train loss: 0.0032401776406913996\n",
            "Training epoch:  839 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0032\n",
            "Train loss: 0.0032202957663685083\n",
            "Training epoch:  840 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0032\n",
            "Train loss: 0.0032128198072314262\n",
            "Training epoch:  841 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0032\n",
            "Train loss: 0.003232723567634821\n",
            "Training epoch:  842 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0032\n",
            "Train loss: 0.0032279554288834333\n",
            "Training epoch:  843 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0032\n",
            "Train loss: 0.0032112097833305597\n",
            "Training epoch:  844 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0032\n",
            "Train loss: 0.0032054719049483538\n",
            "Training epoch:  845 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0032\n",
            "Train loss: 0.0032011326402425766\n",
            "Training epoch:  846 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0032\n",
            "Train loss: 0.0031801939476281404\n",
            "Training epoch:  847 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0032\n",
            "Train loss: 0.003169270232319832\n",
            "Training epoch:  848 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0032\n",
            "Train loss: 0.003162764711305499\n",
            "Training epoch:  849 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0031\n",
            "Train loss: 0.00312415580265224\n",
            "Training epoch:  850 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0032\n",
            "Train loss: 0.0031655782368034124\n",
            "Training epoch:  851 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0032\n",
            "Train loss: 0.0031554403249174356\n",
            "Training epoch:  852 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0031\n",
            "Train loss: 0.003149638185277581\n",
            "Training epoch:  853 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0032\n",
            "Train loss: 0.0031597099732607603\n",
            "Training epoch:  854 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0032\n",
            "Train loss: 0.0031521860510110855\n",
            "Training epoch:  855 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0032\n",
            "Train loss: 0.0031620152294635773\n",
            "Training epoch:  856 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0031\n",
            "Train loss: 0.003149526659399271\n",
            "Training epoch:  857 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0031\n",
            "Train loss: 0.0031279034446924925\n",
            "Training epoch:  858 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0031\n",
            "Train loss: 0.003137788735330105\n",
            "Training epoch:  859 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0031\n",
            "Train loss: 0.0031241036485880613\n",
            "Training epoch:  860 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0031\n",
            "Train loss: 0.0031125920359045267\n",
            "Training epoch:  861 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0031\n",
            "Train loss: 0.0031393920071423054\n",
            "Training epoch:  862 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0031\n",
            "Train loss: 0.0031490973196923733\n",
            "Training epoch:  863 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0031\n",
            "Train loss: 0.0031473368871957064\n",
            "Training epoch:  864 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0031\n",
            "Train loss: 0.0031404115725308657\n",
            "Training epoch:  865 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0031\n",
            "Train loss: 0.0031390381045639515\n",
            "Training epoch:  866 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0031\n",
            "Train loss: 0.0031185210682451725\n",
            "Training epoch:  867 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0032\n",
            "Train loss: 0.0031616997439414263\n",
            "Training epoch:  868 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0032\n",
            "Train loss: 0.003167747287079692\n",
            "Training epoch:  869 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0031\n",
            "Train loss: 0.003142354544252157\n",
            "Training epoch:  870 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0031\n",
            "Train loss: 0.0031411079689860344\n",
            "Training epoch:  871 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0031\n",
            "Train loss: 0.0031039840541779995\n",
            "Training epoch:  872 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0031\n",
            "Train loss: 0.0031052983831614256\n",
            "Training epoch:  873 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0031\n",
            "Train loss: 0.003075961023569107\n",
            "Training epoch:  874 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0031\n",
            "Train loss: 0.0030549177899956703\n",
            "Training epoch:  875 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0031\n",
            "Train loss: 0.003056245855987072\n",
            "Training epoch:  876 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0030\n",
            "Train loss: 0.0030489331111311913\n",
            "Training epoch:  877 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0030\n",
            "Train loss: 0.0030460930429399014\n",
            "Training epoch:  878 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0030\n",
            "Train loss: 0.003047556383535266\n",
            "Training epoch:  879 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0030\n",
            "Train loss: 0.003031169530004263\n",
            "Training epoch:  880 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0030\n",
            "Train loss: 0.003037656657397747\n",
            "Training epoch:  881 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0030\n",
            "Train loss: 0.003031310858204961\n",
            "Training epoch:  882 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0031\n",
            "Train loss: 0.0030527093913406134\n",
            "Training epoch:  883 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0031\n",
            "Train loss: 0.0030541163869202137\n",
            "Training epoch:  884 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0031\n",
            "Train loss: 0.0030508593190461397\n",
            "Training epoch:  885 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0030\n",
            "Train loss: 0.003028739709407091\n",
            "Training epoch:  886 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0030\n",
            "Train loss: 0.003009373787790537\n",
            "Training epoch:  887 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0030\n",
            "Train loss: 0.003007925348356366\n",
            "Training epoch:  888 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0030\n",
            "Train loss: 0.003000041702762246\n",
            "Training epoch:  889 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0030\n",
            "Train loss: 0.0030172858387231827\n",
            "Training epoch:  890 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0030\n",
            "Train loss: 0.0030243888031691313\n",
            "Training epoch:  891 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0030\n",
            "Train loss: 0.0030413048807531595\n",
            "Training epoch:  892 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0030\n",
            "Train loss: 0.003016022266820073\n",
            "Training epoch:  893 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0030\n",
            "Train loss: 0.003021843731403351\n",
            "Training epoch:  894 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0030\n",
            "Train loss: 0.0030139184091240168\n",
            "Training epoch:  895 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0030\n",
            "Train loss: 0.0030066329054534435\n",
            "Training epoch:  896 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0030\n",
            "Train loss: 0.002982689533382654\n",
            "Training epoch:  897 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0030\n",
            "Train loss: 0.0029734796844422817\n",
            "Training epoch:  898 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0029\n",
            "Train loss: 0.0029478028882294893\n",
            "Training epoch:  899 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0030\n",
            "Train loss: 0.002952652284875512\n",
            "...Saved PCA statistics and last model.\n",
            "Latent Mean values:  [ 0.21221401  0.20575443  0.17155528 -0.0598037  -0.07952599  0.23369962]\n",
            "Latent PCA values:  [3.07487874 2.75020288 2.71389295 2.55610382 2.43015809 2.20953532]\n",
            "Training epoch:  900 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0029\n",
            "Train loss: 0.002931445138528943\n",
            "Training epoch:  901 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0030\n",
            "Train loss: 0.002973275724798441\n",
            "Training epoch:  902 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0030\n",
            "Train loss: 0.0029749982059001923\n",
            "Training epoch:  903 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0030\n",
            "Train loss: 0.002971053123474121\n",
            "Training epoch:  904 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0030\n",
            "Train loss: 0.002985183382406831\n",
            "Training epoch:  905 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0030\n",
            "Train loss: 0.0029618507251143456\n",
            "Training epoch:  906 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0030\n",
            "Train loss: 0.0029698077123612165\n",
            "Training epoch:  907 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0030\n",
            "Train loss: 0.0029689399525523186\n",
            "Training epoch:  908 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0030\n",
            "Train loss: 0.002962554106488824\n",
            "Training epoch:  909 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0029\n",
            "Train loss: 0.0029478261712938547\n",
            "Training epoch:  910 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0029\n",
            "Train loss: 0.002949765417724848\n",
            "Training epoch:  911 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0029\n",
            "Train loss: 0.002939507830888033\n",
            "Training epoch:  912 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0030\n",
            "Train loss: 0.002955483505502343\n",
            "Training epoch:  913 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0029\n",
            "Train loss: 0.002943543717265129\n",
            "Training epoch:  914 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0029\n",
            "Train loss: 0.002924300031736493\n",
            "Training epoch:  915 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0029\n",
            "Train loss: 0.002886498812586069\n",
            "Training epoch:  916 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0029\n",
            "Train loss: 0.0028593456372618675\n",
            "Training epoch:  917 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0028\n",
            "Train loss: 0.002837848151102662\n",
            "Training epoch:  918 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0029\n",
            "Train loss: 0.002880148822441697\n",
            "Training epoch:  919 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0029\n",
            "Train loss: 0.002908425871282816\n",
            "Training epoch:  920 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0029\n",
            "Train loss: 0.002902030246332288\n",
            "Training epoch:  921 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0029\n",
            "Train loss: 0.0029267610516399145\n",
            "Training epoch:  922 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0029\n",
            "Train loss: 0.002935335272923112\n",
            "Training epoch:  923 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0029\n",
            "Train loss: 0.0029361152555793524\n",
            "Training epoch:  924 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0029\n",
            "Train loss: 0.002945953281596303\n",
            "Training epoch:  925 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0029\n",
            "Train loss: 0.002946682972833514\n",
            "Training epoch:  926 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0029\n",
            "Train loss: 0.002921775681897998\n",
            "Training epoch:  927 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0029\n",
            "Train loss: 0.002889581024646759\n",
            "Training epoch:  928 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0029\n",
            "Train loss: 0.002873203018680215\n",
            "Training epoch:  929 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0029\n",
            "Train loss: 0.002873688703402877\n",
            "Training epoch:  930 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0029\n",
            "Train loss: 0.002857776591554284\n",
            "Training epoch:  931 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0029\n",
            "Train loss: 0.0028723664581775665\n",
            "Training epoch:  932 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0029\n",
            "Train loss: 0.0028585258405655622\n",
            "Training epoch:  933 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0028\n",
            "Train loss: 0.002843788592144847\n",
            "Training epoch:  934 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0028\n",
            "Train loss: 0.0028384444303810596\n",
            "Training epoch:  935 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0028\n",
            "Train loss: 0.002836691215634346\n",
            "Training epoch:  936 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0028\n",
            "Train loss: 0.002840922214090824\n",
            "Training epoch:  937 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0029\n",
            "Train loss: 0.0028594702016562223\n",
            "Training epoch:  938 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0029\n",
            "Train loss: 0.0028557153418660164\n",
            "Training epoch:  939 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0028\n",
            "Train loss: 0.0028442228212952614\n",
            "Training epoch:  940 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0028\n",
            "Train loss: 0.0028452223632484674\n",
            "Training epoch:  941 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0029\n",
            "Train loss: 0.002860108856111765\n",
            "Training epoch:  942 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0029\n",
            "Train loss: 0.002866933587938547\n",
            "Training epoch:  943 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0029\n",
            "Train loss: 0.002869452815502882\n",
            "Training epoch:  944 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0028\n",
            "Train loss: 0.0028493707068264484\n",
            "Training epoch:  945 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0028\n",
            "Train loss: 0.0028366537299007177\n",
            "Training epoch:  946 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0029\n",
            "Train loss: 0.0028583514504134655\n",
            "Training epoch:  947 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0028\n",
            "Train loss: 0.0028375352267175913\n",
            "Training epoch:  948 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0028\n",
            "Train loss: 0.0028420439921319485\n",
            "Training epoch:  949 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0028\n",
            "Train loss: 0.0028105515521019697\n",
            "Training epoch:  950 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0028\n",
            "Train loss: 0.002821552800014615\n",
            "Training epoch:  951 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0028\n",
            "Train loss: 0.0027999437879770994\n",
            "Training epoch:  952 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0028\n",
            "Train loss: 0.0028274564538151026\n",
            "Training epoch:  953 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0028\n",
            "Train loss: 0.002807876095175743\n",
            "Training epoch:  954 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0028\n",
            "Train loss: 0.002813707571476698\n",
            "Training epoch:  955 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0028\n",
            "Train loss: 0.0028180668596178293\n",
            "Training epoch:  956 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0028\n",
            "Train loss: 0.002770928665995598\n",
            "Training epoch:  957 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0028\n",
            "Train loss: 0.0027542358729988337\n",
            "Training epoch:  958 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0027\n",
            "Train loss: 0.002733123954385519\n",
            "Training epoch:  959 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0027\n",
            "Train loss: 0.0027438229881227016\n",
            "Training epoch:  960 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0028\n",
            "Train loss: 0.002753734588623047\n",
            "Training epoch:  961 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0028\n",
            "Train loss: 0.0027624405920505524\n",
            "Training epoch:  962 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0028\n",
            "Train loss: 0.002768148435279727\n",
            "Training epoch:  963 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0028\n",
            "Train loss: 0.0027839133981615305\n",
            "Training epoch:  964 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0028\n",
            "Train loss: 0.0027613274287432432\n",
            "Training epoch:  965 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0027\n",
            "Train loss: 0.0027443100698292255\n",
            "Training epoch:  966 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0027\n",
            "Train loss: 0.0027259178459644318\n",
            "Training epoch:  967 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0028\n",
            "Train loss: 0.0027526773046702147\n",
            "Training epoch:  968 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0027\n",
            "Train loss: 0.0027348112780600786\n",
            "Training epoch:  969 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0028\n",
            "Train loss: 0.002764302073046565\n",
            "Training epoch:  970 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0028\n",
            "Train loss: 0.002770455088466406\n",
            "Training epoch:  971 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0028\n",
            "Train loss: 0.0027728688437491655\n",
            "Training epoch:  972 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0028\n",
            "Train loss: 0.0027664457447826862\n",
            "Training epoch:  973 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0028\n",
            "Train loss: 0.0027654042933136225\n",
            "Training epoch:  974 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0028\n",
            "Train loss: 0.0027620666660368443\n",
            "Training epoch:  975 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0028\n",
            "Train loss: 0.0027582829352468252\n",
            "Training epoch:  976 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0027\n",
            "Train loss: 0.0027454786468297243\n",
            "Training epoch:  977 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0027\n",
            "Train loss: 0.0027434895746409893\n",
            "Training epoch:  978 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0027\n",
            "Train loss: 0.002735044341534376\n",
            "Training epoch:  979 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0027\n",
            "Train loss: 0.002708424348384142\n",
            "Training epoch:  980 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0027\n",
            "Train loss: 0.002714096801355481\n",
            "Training epoch:  981 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0027\n",
            "Train loss: 0.0026966703590005636\n",
            "Training epoch:  982 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0027\n",
            "Train loss: 0.0026853091549128294\n",
            "Training epoch:  983 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0027\n",
            "Train loss: 0.0026594437658786774\n",
            "Training epoch:  984 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0026\n",
            "Train loss: 0.002646920969709754\n",
            "Training epoch:  985 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0026\n",
            "Train loss: 0.0026106464210897684\n",
            "Training epoch:  986 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0026\n",
            "Train loss: 0.002639728132635355\n",
            "Training epoch:  987 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0026\n",
            "Train loss: 0.0026366524398326874\n",
            "Training epoch:  988 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0027\n",
            "Train loss: 0.0026695651467889547\n",
            "Training epoch:  989 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0027\n",
            "Train loss: 0.0026869673747569323\n",
            "Training epoch:  990 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0027\n",
            "Train loss: 0.0026959057431668043\n",
            "Training epoch:  991 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0027\n",
            "Train loss: 0.0026967485900968313\n",
            "Training epoch:  992 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0027\n",
            "Train loss: 0.0026883932296186686\n",
            "Training epoch:  993 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0027\n",
            "Train loss: 0.002673850627616048\n",
            "Training epoch:  994 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0027\n",
            "Train loss: 0.002661798382177949\n",
            "Training epoch:  995 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0027\n",
            "Train loss: 0.002674644347280264\n",
            "Training epoch:  996 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0027\n",
            "Train loss: 0.002666506450623274\n",
            "Training epoch:  997 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0027\n",
            "Train loss: 0.002650439739227295\n",
            "Training epoch:  998 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0027\n",
            "Train loss: 0.002661856822669506\n",
            "Training epoch:  999 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0027\n",
            "Train loss: 0.0026575960218906403\n",
            "...Saved PCA statistics and last model.\n",
            "Latent Mean values:  [-0.0222603   0.08210884  0.00661959 -0.292324    0.13313283  0.06234121]\n",
            "Latent PCA values:  [2.82526958 2.72419887 2.648783   2.55136372 2.36693243 2.15596266]\n",
            "Training epoch:  1000 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0026\n",
            "Train loss: 0.0026281727477908134\n",
            "Training epoch:  1001 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0026\n",
            "Train loss: 0.0026237391866743565\n",
            "Training epoch:  1002 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0026\n",
            "Train loss: 0.0026107828598469496\n",
            "Training epoch:  1003 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0026\n",
            "Train loss: 0.002637890400364995\n",
            "Training epoch:  1004 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0026\n",
            "Train loss: 0.0026380668859928846\n",
            "Training epoch:  1005 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0026\n",
            "Train loss: 0.002631567884236574\n",
            "Training epoch:  1006 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0026\n",
            "Train loss: 0.0026101181283593178\n",
            "Training epoch:  1007 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0026\n",
            "Train loss: 0.0025958751793950796\n",
            "Training epoch:  1008 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0026\n",
            "Train loss: 0.002617219928652048\n",
            "Training epoch:  1009 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0026\n",
            "Train loss: 0.0026337443850934505\n",
            "Training epoch:  1010 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0026\n",
            "Train loss: 0.002625682856887579\n",
            "Training epoch:  1011 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0026\n",
            "Train loss: 0.002595936181023717\n",
            "Training epoch:  1012 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0026\n",
            "Train loss: 0.0025800722651183605\n",
            "Training epoch:  1013 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0026\n",
            "Train loss: 0.002574229147285223\n",
            "Training epoch:  1014 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0026\n",
            "Train loss: 0.002579801483079791\n",
            "Training epoch:  1015 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0026\n",
            "Train loss: 0.002550355391576886\n",
            "Training epoch:  1016 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0025\n",
            "Train loss: 0.0025449537206441164\n",
            "Training epoch:  1017 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0025\n",
            "Train loss: 0.0025268669705837965\n",
            "Training epoch:  1018 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0025\n",
            "Train loss: 0.002535079373046756\n",
            "Training epoch:  1019 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0025\n",
            "Train loss: 0.002532889833673835\n",
            "Training epoch:  1020 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0026\n",
            "Train loss: 0.0025648674927651882\n",
            "Training epoch:  1021 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0026\n",
            "Train loss: 0.002569206291809678\n",
            "Training epoch:  1022 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0026\n",
            "Train loss: 0.002569698030129075\n",
            "Training epoch:  1023 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0026\n",
            "Train loss: 0.0025809474755078554\n",
            "Training epoch:  1024 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0026\n",
            "Train loss: 0.0025826813653111458\n",
            "Training epoch:  1025 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0026\n",
            "Train loss: 0.002560206688940525\n",
            "Training epoch:  1026 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0026\n",
            "Train loss: 0.0025623110122978687\n",
            "Training epoch:  1027 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0026\n",
            "Train loss: 0.0025620770175009966\n",
            "Training epoch:  1028 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0026\n",
            "Train loss: 0.0025636956561356783\n",
            "Training epoch:  1029 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0026\n",
            "Train loss: 0.002570173004642129\n",
            "Training epoch:  1030 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0026\n",
            "Train loss: 0.002562030451372266\n",
            "Training epoch:  1031 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0025\n",
            "Train loss: 0.0025466480292379856\n",
            "Training epoch:  1032 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0025\n",
            "Train loss: 0.0025338453706353903\n",
            "Training epoch:  1033 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0025\n",
            "Train loss: 0.002545692725107074\n",
            "Training epoch:  1034 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0025\n",
            "Train loss: 0.00254901684820652\n",
            "Training epoch:  1035 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0026\n",
            "Train loss: 0.002553179394453764\n",
            "Training epoch:  1036 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0025\n",
            "Train loss: 0.0025338325649499893\n",
            "Training epoch:  1037 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0025\n",
            "Train loss: 0.0025429180823266506\n",
            "Training epoch:  1038 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0025\n",
            "Train loss: 0.0025264045689255\n",
            "Training epoch:  1039 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0025\n",
            "Train loss: 0.0025127839762717485\n",
            "Training epoch:  1040 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0025\n",
            "Train loss: 0.002504972741007805\n",
            "Training epoch:  1041 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0025\n",
            "Train loss: 0.002492801984772086\n",
            "Training epoch:  1042 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0025\n",
            "Train loss: 0.002504084026440978\n",
            "Training epoch:  1043 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0025\n",
            "Train loss: 0.002502958755940199\n",
            "Training epoch:  1044 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0025\n",
            "Train loss: 0.0024913533125072718\n",
            "Training epoch:  1045 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0025\n",
            "Train loss: 0.0025067010428756475\n",
            "Training epoch:  1046 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0025\n",
            "Train loss: 0.0024993375409394503\n",
            "Training epoch:  1047 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0025\n",
            "Train loss: 0.0025131679140031338\n",
            "Training epoch:  1048 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0025\n",
            "Train loss: 0.002487018471583724\n",
            "Training epoch:  1049 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0025\n",
            "Train loss: 0.0024727184791117907\n",
            "Training epoch:  1050 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0025\n",
            "Train loss: 0.0024671924766153097\n",
            "Training epoch:  1051 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0024\n",
            "Train loss: 0.0024487334303557873\n",
            "Training epoch:  1052 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0024\n",
            "Train loss: 0.0024421638809144497\n",
            "Training epoch:  1053 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0024\n",
            "Train loss: 0.002440395997837186\n",
            "Training epoch:  1054 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0025\n",
            "Train loss: 0.0024582063779234886\n",
            "Training epoch:  1055 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0025\n",
            "Train loss: 0.0024569977540522814\n",
            "Training epoch:  1056 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0024\n",
            "Train loss: 0.00244913506321609\n",
            "Training epoch:  1057 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0024\n",
            "Train loss: 0.002449801191687584\n",
            "Training epoch:  1058 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0025\n",
            "Train loss: 0.002451189560815692\n",
            "Training epoch:  1059 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0025\n",
            "Train loss: 0.002454569097608328\n",
            "Training epoch:  1060 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0025\n",
            "Train loss: 0.002464889781549573\n",
            "Training epoch:  1061 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0025\n",
            "Train loss: 0.0024636834859848022\n",
            "Training epoch:  1062 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0025\n",
            "Train loss: 0.0024704032111912966\n",
            "Training epoch:  1063 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0025\n",
            "Train loss: 0.002486260375007987\n",
            "Training epoch:  1064 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0025\n",
            "Train loss: 0.002491934923455119\n",
            "Training epoch:  1065 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0025\n",
            "Train loss: 0.002479374408721924\n",
            "Training epoch:  1066 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0025\n",
            "Train loss: 0.0024704087991267443\n",
            "Training epoch:  1067 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0025\n",
            "Train loss: 0.0024546959903091192\n",
            "Training epoch:  1068 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0024\n",
            "Train loss: 0.0024440139532089233\n",
            "Training epoch:  1069 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0024\n",
            "Train loss: 0.00243097054772079\n",
            "Training epoch:  1070 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0024\n",
            "Train loss: 0.002419745083898306\n",
            "Training epoch:  1071 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0024\n",
            "Train loss: 0.0024165280628949404\n",
            "Training epoch:  1072 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0024\n",
            "Train loss: 0.0024178363382816315\n",
            "Training epoch:  1073 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0024\n",
            "Train loss: 0.00240270490758121\n",
            "Training epoch:  1074 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0024\n",
            "Train loss: 0.002416969044134021\n",
            "Training epoch:  1075 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0024\n",
            "Train loss: 0.0024036027025431395\n",
            "Training epoch:  1076 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0024\n",
            "Train loss: 0.002408508211374283\n",
            "Training epoch:  1077 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0024\n",
            "Train loss: 0.002398095326498151\n",
            "Training epoch:  1078 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0024\n",
            "Train loss: 0.002383159240707755\n",
            "Training epoch:  1079 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0024\n",
            "Train loss: 0.002389600034803152\n",
            "Training epoch:  1080 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0024\n",
            "Train loss: 0.0023866340052336454\n",
            "Training epoch:  1081 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0024\n",
            "Train loss: 0.002393511589616537\n",
            "Training epoch:  1082 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0024\n",
            "Train loss: 0.0023738862946629524\n",
            "Training epoch:  1083 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0024\n",
            "Train loss: 0.00238302000798285\n",
            "Training epoch:  1084 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0024\n",
            "Train loss: 0.002410107757896185\n",
            "Training epoch:  1085 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0024\n",
            "Train loss: 0.0024015852250158787\n",
            "Training epoch:  1086 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0024\n",
            "Train loss: 0.0024340918753296137\n",
            "Training epoch:  1087 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0024\n",
            "Train loss: 0.002420218661427498\n",
            "Training epoch:  1088 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0024\n",
            "Train loss: 0.0023927895817905664\n",
            "Training epoch:  1089 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0024\n",
            "Train loss: 0.0023570775520056486\n",
            "Training epoch:  1090 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0023\n",
            "Train loss: 0.002338332822546363\n",
            "Training epoch:  1091 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0023\n",
            "Train loss: 0.002334252931177616\n",
            "Training epoch:  1092 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0023\n",
            "Train loss: 0.002330887597054243\n",
            "Training epoch:  1093 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0023\n",
            "Train loss: 0.002317726379260421\n",
            "Training epoch:  1094 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0023\n",
            "Train loss: 0.002312607830390334\n",
            "Training epoch:  1095 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0023\n",
            "Train loss: 0.0023087174631655216\n",
            "Training epoch:  1096 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0023\n",
            "Train loss: 0.0023048794828355312\n",
            "Training epoch:  1097 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0023\n",
            "Train loss: 0.0023167000617831945\n",
            "Training epoch:  1098 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0023\n",
            "Train loss: 0.002316553145647049\n",
            "Training epoch:  1099 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0023\n",
            "Train loss: 0.002323174150660634\n",
            "...Saved PCA statistics and last model.\n",
            "Latent Mean values:  [-0.11755659  0.0961232   0.06410011 -0.2077971  -0.07327642 -0.02572291]\n",
            "Latent PCA values:  [2.8018843  2.55794799 2.53898969 2.4047979  2.30614346 2.13229284]\n",
            "Training epoch:  1100 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0023\n",
            "Train loss: 0.002339285332709551\n",
            "Training epoch:  1101 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0023\n",
            "Train loss: 0.0023482751566916704\n",
            "Training epoch:  1102 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0023\n",
            "Train loss: 0.0023478567600250244\n",
            "Training epoch:  1103 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0024\n",
            "Train loss: 0.0023621367290616035\n",
            "Training epoch:  1104 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0023\n",
            "Train loss: 0.0023365311790257692\n",
            "Training epoch:  1105 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0023\n",
            "Train loss: 0.0023423826787620783\n",
            "Training epoch:  1106 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0023\n",
            "Train loss: 0.002306248527020216\n",
            "Training epoch:  1107 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0023\n",
            "Train loss: 0.002309604315087199\n",
            "Training epoch:  1108 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0023\n",
            "Train loss: 0.0023085628636181355\n",
            "Training epoch:  1109 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0023\n",
            "Train loss: 0.0023055204655975103\n",
            "Training epoch:  1110 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0023\n",
            "Train loss: 0.00231547886505723\n",
            "Training epoch:  1111 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0023\n",
            "Train loss: 0.0023181531578302383\n",
            "Training epoch:  1112 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0023\n",
            "Train loss: 0.0023102404084056616\n",
            "Training epoch:  1113 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0023\n",
            "Train loss: 0.0023039975203573704\n",
            "Training epoch:  1114 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0023\n",
            "Train loss: 0.0022788422647863626\n",
            "Training epoch:  1115 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0023\n",
            "Train loss: 0.0022570574656128883\n",
            "Training epoch:  1116 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0022\n",
            "Train loss: 0.002246912568807602\n",
            "Training epoch:  1117 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0022\n",
            "Train loss: 0.002240291330963373\n",
            "Training epoch:  1118 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0023\n",
            "Train loss: 0.0022606186103075743\n",
            "Training epoch:  1119 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0022\n",
            "Train loss: 0.0022485482040792704\n",
            "Training epoch:  1120 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0023\n",
            "Train loss: 0.002255123807117343\n",
            "Training epoch:  1121 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0022\n",
            "Train loss: 0.002246033400297165\n",
            "Training epoch:  1122 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0023\n",
            "Train loss: 0.0022642682306468487\n",
            "Training epoch:  1123 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0023\n",
            "Train loss: 0.0022687267046421766\n",
            "Training epoch:  1124 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0023\n",
            "Train loss: 0.0022573373280465603\n",
            "Training epoch:  1125 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0023\n",
            "Train loss: 0.002261888002976775\n",
            "Training epoch:  1126 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0023\n",
            "Train loss: 0.002260107547044754\n",
            "Training epoch:  1127 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0023\n",
            "Train loss: 0.0022834218107163906\n",
            "Training epoch:  1128 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0023\n",
            "Train loss: 0.0022867321968078613\n",
            "Training epoch:  1129 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0023\n",
            "Train loss: 0.002278112107887864\n",
            "Training epoch:  1130 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0023\n",
            "Train loss: 0.0022907464299350977\n",
            "Training epoch:  1131 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0023\n",
            "Train loss: 0.002284035552293062\n",
            "Training epoch:  1132 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0023\n",
            "Train loss: 0.002251384314149618\n",
            "Training epoch:  1133 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0022\n",
            "Train loss: 0.0022313958033919334\n",
            "Training epoch:  1134 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0022\n",
            "Train loss: 0.0022058964241296053\n",
            "Training epoch:  1135 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0022\n",
            "Train loss: 0.0022096114698797464\n",
            "Training epoch:  1136 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0022\n",
            "Train loss: 0.002198893344029784\n",
            "Training epoch:  1137 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0022\n",
            "Train loss: 0.002205380704253912\n",
            "Training epoch:  1138 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0022\n",
            "Train loss: 0.0021968460641801357\n",
            "Training epoch:  1139 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0022\n",
            "Train loss: 0.0022173470351845026\n",
            "Training epoch:  1140 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0022\n",
            "Train loss: 0.0022252004127949476\n",
            "Training epoch:  1141 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0022\n",
            "Train loss: 0.002242978196591139\n",
            "Training epoch:  1142 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0022\n",
            "Train loss: 0.0022450359538197517\n",
            "Training epoch:  1143 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0022\n",
            "Train loss: 0.0022312833461910486\n",
            "Training epoch:  1144 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0022\n",
            "Train loss: 0.0022285492159426212\n",
            "Training epoch:  1145 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0022\n",
            "Train loss: 0.0022318961564451456\n",
            "Training epoch:  1146 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0022\n",
            "Train loss: 0.002219732152298093\n",
            "Training epoch:  1147 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0022\n",
            "Train loss: 0.0021994514390826225\n",
            "Training epoch:  1148 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0022\n",
            "Train loss: 0.0022098564077168703\n",
            "Training epoch:  1149 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0022\n",
            "Train loss: 0.0021952984388917685\n",
            "Training epoch:  1150 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0022\n",
            "Train loss: 0.0021834250073879957\n",
            "Training epoch:  1151 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0022\n",
            "Train loss: 0.0021893491502851248\n",
            "Training epoch:  1152 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0022\n",
            "Train loss: 0.002187411766499281\n",
            "Training epoch:  1153 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0022\n",
            "Train loss: 0.002172856591641903\n",
            "Training epoch:  1154 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0022\n",
            "Train loss: 0.002160342875868082\n",
            "Training epoch:  1155 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0022\n",
            "Train loss: 0.002156774280592799\n",
            "Training epoch:  1156 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0022\n",
            "Train loss: 0.0021624332293868065\n",
            "Training epoch:  1157 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0022\n",
            "Train loss: 0.0021861025597900152\n",
            "Training epoch:  1158 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0022\n",
            "Train loss: 0.0021984651684761047\n",
            "Training epoch:  1159 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0022\n",
            "Train loss: 0.0022179640363901854\n",
            "Training epoch:  1160 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0022\n",
            "Train loss: 0.002190702361986041\n",
            "Training epoch:  1161 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0022\n",
            "Train loss: 0.0021947515197098255\n",
            "Training epoch:  1162 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0022\n",
            "Train loss: 0.0021800477989017963\n",
            "Training epoch:  1163 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0022\n",
            "Train loss: 0.002183909062296152\n",
            "Training epoch:  1164 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0022\n",
            "Train loss: 0.0021693112794309855\n",
            "Training epoch:  1165 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0022\n",
            "Train loss: 0.0021642507053911686\n",
            "Training epoch:  1166 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0022\n",
            "Train loss: 0.0021522005554288626\n",
            "Training epoch:  1167 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0022\n",
            "Train loss: 0.002151360036805272\n",
            "Training epoch:  1168 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0021\n",
            "Train loss: 0.002134409500285983\n",
            "Training epoch:  1169 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0021\n",
            "Train loss: 0.0021353201009333134\n",
            "Training epoch:  1170 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0021\n",
            "Train loss: 0.002124524675309658\n",
            "Training epoch:  1171 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0021\n",
            "Train loss: 0.0021203774958848953\n",
            "Training epoch:  1172 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0021\n",
            "Train loss: 0.0020977568347007036\n",
            "Training epoch:  1173 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0021\n",
            "Train loss: 0.0021182741038501263\n",
            "Training epoch:  1174 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0021\n",
            "Train loss: 0.002124715829268098\n",
            "Training epoch:  1175 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0021\n",
            "Train loss: 0.0021109769586473703\n",
            "Training epoch:  1176 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0021\n",
            "Train loss: 0.0021179995965212584\n",
            "Training epoch:  1177 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0021\n",
            "Train loss: 0.002117524156346917\n",
            "Training epoch:  1178 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0021\n",
            "Train loss: 0.0021158058661967516\n",
            "Training epoch:  1179 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0021\n",
            "Train loss: 0.0021215728484094143\n",
            "Training epoch:  1180 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0021\n",
            "Train loss: 0.0021094244439154863\n",
            "Training epoch:  1181 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0021\n",
            "Train loss: 0.0021166596561670303\n",
            "Training epoch:  1182 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0021\n",
            "Train loss: 0.0021175299771130085\n",
            "Training epoch:  1183 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0021\n",
            "Train loss: 0.0021172761917114258\n",
            "Training epoch:  1184 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0021\n",
            "Train loss: 0.0021200901828706264\n",
            "Training epoch:  1185 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0021\n",
            "Train loss: 0.0021307822316884995\n",
            "Training epoch:  1186 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0021\n",
            "Train loss: 0.0021187339443713427\n",
            "Training epoch:  1187 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0021\n",
            "Train loss: 0.002092613372951746\n",
            "Training epoch:  1188 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0021\n",
            "Train loss: 0.002077406505122781\n",
            "Training epoch:  1189 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0021\n",
            "Train loss: 0.002084899926558137\n",
            "Training epoch:  1190 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0021\n",
            "Train loss: 0.00208480516448617\n",
            "Training epoch:  1191 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0021\n",
            "Train loss: 0.002076217206194997\n",
            "Training epoch:  1192 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0021\n",
            "Train loss: 0.002078650752082467\n",
            "Training epoch:  1193 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0021\n",
            "Train loss: 0.002076758537441492\n",
            "Training epoch:  1194 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0021\n",
            "Train loss: 0.002072150819003582\n",
            "Training epoch:  1195 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0021\n",
            "Train loss: 0.002057374222204089\n",
            "Training epoch:  1196 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0021\n",
            "Train loss: 0.0020517916418612003\n",
            "Training epoch:  1197 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0021\n",
            "Train loss: 0.0020709598902612925\n",
            "Training epoch:  1198 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0021\n",
            "Train loss: 0.0020825976971536875\n",
            "Training epoch:  1199 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0021\n",
            "Train loss: 0.002089714864268899\n",
            "...Saved PCA statistics and last model.\n",
            "Latent Mean values:  [ 0.01800142 -0.24861841  0.02418995  0.23688687 -0.08609886 -0.04526822]\n",
            "Latent PCA values:  [2.61221882 2.58205332 2.51365768 2.40980326 2.28775028 2.09387773]\n",
            "Training epoch:  1200 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0021\n",
            "Train loss: 0.0020675649866461754\n",
            "Training epoch:  1201 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0021\n",
            "Train loss: 0.002089268062263727\n",
            "Training epoch:  1202 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0021\n",
            "Train loss: 0.0020701454486697912\n",
            "Training epoch:  1203 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0020\n",
            "Train loss: 0.0020435487385839224\n",
            "Training epoch:  1204 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0020\n",
            "Train loss: 0.0020256342831999063\n",
            "Training epoch:  1205 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0020\n",
            "Train loss: 0.0020346196833997965\n",
            "Training epoch:  1206 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0020\n",
            "Train loss: 0.002043059328570962\n",
            "Training epoch:  1207 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0021\n",
            "Train loss: 0.002063450403511524\n",
            "Training epoch:  1208 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0021\n",
            "Train loss: 0.0020508919842541218\n",
            "Training epoch:  1209 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0020\n",
            "Train loss: 0.002049420727416873\n",
            "Training epoch:  1210 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0020\n",
            "Train loss: 0.0020387698896229267\n",
            "Training epoch:  1211 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0020\n",
            "Train loss: 0.002047831192612648\n",
            "Training epoch:  1212 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0020\n",
            "Train loss: 0.0020456325728446245\n",
            "Training epoch:  1213 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0020\n",
            "Train loss: 0.002045200439170003\n",
            "Training epoch:  1214 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0020\n",
            "Train loss: 0.0020293202251195908\n",
            "Training epoch:  1215 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0020\n",
            "Train loss: 0.0020321407355368137\n",
            "Training epoch:  1216 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0020\n",
            "Train loss: 0.002012308221310377\n",
            "Training epoch:  1217 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0020\n",
            "Train loss: 0.001985313603654504\n",
            "Training epoch:  1218 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0020\n",
            "Train loss: 0.001967295538634062\n",
            "Training epoch:  1219 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0020\n",
            "Train loss: 0.00195499649271369\n",
            "Training epoch:  1220 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0020\n",
            "Train loss: 0.0019656606018543243\n",
            "Training epoch:  1221 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0020\n",
            "Train loss: 0.001965343253687024\n",
            "Training epoch:  1222 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0020\n",
            "Train loss: 0.0019849715754389763\n",
            "Training epoch:  1223 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0020\n",
            "Train loss: 0.0020011900924146175\n",
            "Training epoch:  1224 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0020\n",
            "Train loss: 0.0020213001407682896\n",
            "Training epoch:  1225 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0020\n",
            "Train loss: 0.0020159909036010504\n",
            "Training epoch:  1226 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0020\n",
            "Train loss: 0.002019688952714205\n",
            "Training epoch:  1227 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0020\n",
            "Train loss: 0.002012047916650772\n",
            "Training epoch:  1228 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0020\n",
            "Train loss: 0.0019991628360003233\n",
            "Training epoch:  1229 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0020\n",
            "Train loss: 0.0020074357744306326\n",
            "Training epoch:  1230 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0020\n",
            "Train loss: 0.002010904485359788\n",
            "Training epoch:  1231 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0020\n",
            "Train loss: 0.002011343836784363\n",
            "Training epoch:  1232 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0020\n",
            "Train loss: 0.001982748042792082\n",
            "Training epoch:  1233 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0020\n",
            "Train loss: 0.001983809983357787\n",
            "Training epoch:  1234 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0020\n",
            "Train loss: 0.0019715330563485622\n",
            "Training epoch:  1235 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0020\n",
            "Train loss: 0.00197590421885252\n",
            "Training epoch:  1236 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0020\n",
            "Train loss: 0.0019780034199357033\n",
            "Training epoch:  1237 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0020\n",
            "Train loss: 0.001953606028109789\n",
            "Training epoch:  1238 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0020\n",
            "Train loss: 0.001963465940207243\n",
            "Training epoch:  1239 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0020\n",
            "Train loss: 0.0019767472986131907\n",
            "Training epoch:  1240 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0020\n",
            "Train loss: 0.001974468817934394\n",
            "Training epoch:  1241 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0020\n",
            "Train loss: 0.002006818074733019\n",
            "Training epoch:  1242 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0020\n",
            "Train loss: 0.002017748774960637\n",
            "Training epoch:  1243 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0020\n",
            "Train loss: 0.0020134972874075174\n",
            "Training epoch:  1244 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0020\n",
            "Train loss: 0.001983416499570012\n",
            "Training epoch:  1245 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0020\n",
            "Train loss: 0.0019556372426450253\n",
            "Training epoch:  1246 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0019\n",
            "Train loss: 0.001946226111613214\n",
            "Training epoch:  1247 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0019\n",
            "Train loss: 0.0019371162634342909\n",
            "Training epoch:  1248 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0019\n",
            "Train loss: 0.0019363093888387084\n",
            "Training epoch:  1249 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0019\n",
            "Train loss: 0.0019291151547804475\n",
            "Training epoch:  1250 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0019\n",
            "Train loss: 0.0019360295264050364\n",
            "Training epoch:  1251 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0019\n",
            "Train loss: 0.0019317636033520103\n",
            "Training epoch:  1252 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0019\n",
            "Train loss: 0.0019450660329312086\n",
            "Training epoch:  1253 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0019\n",
            "Train loss: 0.0019253345672041178\n",
            "Training epoch:  1254 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0019\n",
            "Train loss: 0.0019360933220013976\n",
            "Training epoch:  1255 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0019\n",
            "Train loss: 0.0019295124802738428\n",
            "Training epoch:  1256 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0019\n",
            "Train loss: 0.0019396496936678886\n",
            "Training epoch:  1257 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0019\n",
            "Train loss: 0.0019383799517527223\n",
            "Training epoch:  1258 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0020\n",
            "Train loss: 0.0019536151085048914\n",
            "Training epoch:  1259 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0019\n",
            "Train loss: 0.00193732266779989\n",
            "Training epoch:  1260 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0019\n",
            "Train loss: 0.0019192838808521628\n",
            "Training epoch:  1261 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0019\n",
            "Train loss: 0.0019296426326036453\n",
            "Training epoch:  1262 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0019\n",
            "Train loss: 0.0019314432283863425\n",
            "Training epoch:  1263 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0019\n",
            "Train loss: 0.001933002145960927\n",
            "Training epoch:  1264 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0019\n",
            "Train loss: 0.0019414807902649045\n",
            "Training epoch:  1265 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0019\n",
            "Train loss: 0.0019254875369369984\n",
            "Training epoch:  1266 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0019\n",
            "Train loss: 0.001930164871737361\n",
            "Training epoch:  1267 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0019\n",
            "Train loss: 0.0019021672196686268\n",
            "Training epoch:  1268 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0019\n",
            "Train loss: 0.001894180430099368\n",
            "Training epoch:  1269 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0019\n",
            "Train loss: 0.0019046710804104805\n",
            "Training epoch:  1270 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0019\n",
            "Train loss: 0.0019070442067459226\n",
            "Training epoch:  1271 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0019\n",
            "Train loss: 0.0019039285834878683\n",
            "Training epoch:  1272 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0019\n",
            "Train loss: 0.0018815298099070787\n",
            "Training epoch:  1273 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0019\n",
            "Train loss: 0.0018808494787663221\n",
            "Training epoch:  1274 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0019\n",
            "Train loss: 0.001898701535537839\n",
            "Training epoch:  1275 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0019\n",
            "Train loss: 0.0019101141951978207\n",
            "Training epoch:  1276 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0019\n",
            "Train loss: 0.0019038764294236898\n",
            "Training epoch:  1277 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0019\n",
            "Train loss: 0.0019116457551717758\n",
            "Training epoch:  1278 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0019\n",
            "Train loss: 0.0019158811774104834\n",
            "Training epoch:  1279 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0019\n",
            "Train loss: 0.001908766687847674\n",
            "Training epoch:  1280 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0019\n",
            "Train loss: 0.0019020242616534233\n",
            "Training epoch:  1281 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0019\n",
            "Train loss: 0.0019005346111953259\n",
            "Training epoch:  1282 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0019\n",
            "Train loss: 0.001908723614178598\n",
            "Training epoch:  1283 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0019\n",
            "Train loss: 0.0019046529196202755\n",
            "Training epoch:  1284 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0019\n",
            "Train loss: 0.0018898629350587726\n",
            "Training epoch:  1285 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0019\n",
            "Train loss: 0.0018699320498853922\n",
            "Training epoch:  1286 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0019\n",
            "Train loss: 0.0018642271170392632\n",
            "Training epoch:  1287 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0019\n",
            "Train loss: 0.0018630156992003322\n",
            "Training epoch:  1288 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0019\n",
            "Train loss: 0.0018735039047896862\n",
            "Training epoch:  1289 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0019\n",
            "Train loss: 0.001865835627540946\n",
            "Training epoch:  1290 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0019\n",
            "Train loss: 0.0018674936145544052\n",
            "Training epoch:  1291 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0019\n",
            "Train loss: 0.0018522316822782159\n",
            "Training epoch:  1292 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0019\n",
            "Train loss: 0.0018816330702975392\n",
            "Training epoch:  1293 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0019\n",
            "Train loss: 0.0018509982619434595\n",
            "Training epoch:  1294 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0018\n",
            "Train loss: 0.001830784254707396\n",
            "Training epoch:  1295 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0018\n",
            "Train loss: 0.0018247837433591485\n",
            "Training epoch:  1296 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0018\n",
            "Train loss: 0.0018213052535429597\n",
            "Training epoch:  1297 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0018\n",
            "Train loss: 0.0018304054392501712\n",
            "Training epoch:  1298 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0018\n",
            "Train loss: 0.0018273843452334404\n",
            "Training epoch:  1299 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0018\n",
            "Train loss: 0.0018295585177838802\n",
            "...Saved PCA statistics and last model.\n",
            "Latent Mean values:  [ 0.06840775  0.00681893 -0.16888574 -0.05856809  0.09239213 -0.09315746]\n",
            "Latent PCA values:  [2.62124442 2.55166044 2.46816439 2.38264081 2.22704384 2.08602403]\n",
            "Training epoch:  1300 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0018\n",
            "Train loss: 0.0018070531077682972\n",
            "Training epoch:  1301 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0018\n",
            "Train loss: 0.0018207537941634655\n",
            "Training epoch:  1302 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0018\n",
            "Train loss: 0.0018378760432824492\n",
            "Training epoch:  1303 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0018\n",
            "Train loss: 0.0018438625847920775\n",
            "Training epoch:  1304 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0018\n",
            "Train loss: 0.0018457315163686872\n",
            "Training epoch:  1305 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0018\n",
            "Train loss: 0.001812159433029592\n",
            "Training epoch:  1306 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0018\n",
            "Train loss: 0.0018119668820872903\n",
            "Training epoch:  1307 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0018\n",
            "Train loss: 0.0018124262569472194\n",
            "Training epoch:  1308 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0018\n",
            "Train loss: 0.001789469737559557\n",
            "Training epoch:  1309 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0018\n",
            "Train loss: 0.0018052525119856\n",
            "Training epoch:  1310 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0018\n",
            "Train loss: 0.00179301539901644\n",
            "Training epoch:  1311 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0018\n",
            "Train loss: 0.0018085029441863298\n",
            "Training epoch:  1312 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0018\n",
            "Train loss: 0.0018196007004007697\n",
            "Training epoch:  1313 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0018\n",
            "Train loss: 0.0018219094490632415\n",
            "Training epoch:  1314 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0018\n",
            "Train loss: 0.0017938915407285094\n",
            "Training epoch:  1315 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0018\n",
            "Train loss: 0.0017803231021389365\n",
            "Training epoch:  1316 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0018\n",
            "Train loss: 0.0017676312709227204\n",
            "Training epoch:  1317 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0018\n",
            "Train loss: 0.0017708241939544678\n",
            "Training epoch:  1318 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0018\n",
            "Train loss: 0.0017638865392655134\n",
            "Training epoch:  1319 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0018\n",
            "Train loss: 0.0017692355904728174\n",
            "Training epoch:  1320 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0018\n",
            "Train loss: 0.001792664872482419\n",
            "Training epoch:  1321 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0018\n",
            "Train loss: 0.0018117319559678435\n",
            "Training epoch:  1322 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0018\n",
            "Train loss: 0.0018048807978630066\n",
            "Training epoch:  1323 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0018\n",
            "Train loss: 0.0018124858615919948\n",
            "Training epoch:  1324 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0018\n",
            "Train loss: 0.0018016883404925466\n",
            "Training epoch:  1325 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0018\n",
            "Train loss: 0.0017819333588704467\n",
            "Training epoch:  1326 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0018\n",
            "Train loss: 0.00179544638376683\n",
            "Training epoch:  1327 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0018\n",
            "Train loss: 0.0017925798892974854\n",
            "Training epoch:  1328 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0018\n",
            "Train loss: 0.0017820721259340644\n",
            "Training epoch:  1329 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0018\n",
            "Train loss: 0.0017762043280526996\n",
            "Training epoch:  1330 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0018\n",
            "Train loss: 0.0017934221541509032\n",
            "Training epoch:  1331 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0018\n",
            "Train loss: 0.0017778511391952634\n",
            "Training epoch:  1332 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0018\n",
            "Train loss: 0.0017778795445337892\n",
            "Training epoch:  1333 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0018\n",
            "Train loss: 0.00176925677806139\n",
            "Training epoch:  1334 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0018\n",
            "Train loss: 0.0017545123118907213\n",
            "Training epoch:  1335 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.0017453762702643871\n",
            "Training epoch:  1336 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.001749327639117837\n",
            "Training epoch:  1337 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.0017404374666512012\n",
            "Training epoch:  1338 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.0017312245909124613\n",
            "Training epoch:  1339 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.0017336993478238583\n",
            "Training epoch:  1340 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.0017328946851193905\n",
            "Training epoch:  1341 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.0017352591967210174\n",
            "Training epoch:  1342 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.0017287380760535598\n",
            "Training epoch:  1343 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.0017466708086431026\n",
            "Training epoch:  1344 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.0017379405908286572\n",
            "Training epoch:  1345 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0018\n",
            "Train loss: 0.0017528950702399015\n",
            "Training epoch:  1346 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.0017381812212988734\n",
            "Training epoch:  1347 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.0017299439059570432\n",
            "Training epoch:  1348 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.0017326330998912454\n",
            "Training epoch:  1349 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.0017227981006726623\n",
            "Training epoch:  1350 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0018\n",
            "Train loss: 0.0017625429900363088\n",
            "Training epoch:  1351 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.0017435478512197733\n",
            "Training epoch:  1352 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.0017467966536059976\n",
            "Training epoch:  1353 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0018\n",
            "Train loss: 0.0017537301173433661\n",
            "Training epoch:  1354 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0018\n",
            "Train loss: 0.0017588665941730142\n",
            "Training epoch:  1355 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0018\n",
            "Train loss: 0.0017581298016011715\n",
            "Training epoch:  1356 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.0017497088992968202\n",
            "Training epoch:  1357 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.0017320690676569939\n",
            "Training epoch:  1358 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.0017459356458857656\n",
            "Training epoch:  1359 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.0017213048413395882\n",
            "Training epoch:  1360 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.001735427649691701\n",
            "Training epoch:  1361 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.0017358203185722232\n",
            "Training epoch:  1362 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.0017169330967590213\n",
            "Training epoch:  1363 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.0016988030401989818\n",
            "Training epoch:  1364 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.0016747942427173257\n",
            "Training epoch:  1365 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.0016916145104914904\n",
            "Training epoch:  1366 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.0016872698906809092\n",
            "Training epoch:  1367 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.001696920022368431\n",
            "Training epoch:  1368 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.0017176206456497312\n",
            "Training epoch:  1369 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.0017307453090324998\n",
            "Training epoch:  1370 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.0017321126069873571\n",
            "Training epoch:  1371 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.0017181566217914224\n",
            "Training epoch:  1372 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.00171399035025388\n",
            "Training epoch:  1373 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.0017004888504743576\n",
            "Training epoch:  1374 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.0016893557040020823\n",
            "Training epoch:  1375 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.0016868013190105557\n",
            "Training epoch:  1376 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.0016603529220446944\n",
            "Training epoch:  1377 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.0016750622307881713\n",
            "Training epoch:  1378 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.0016807292122393847\n",
            "Training epoch:  1379 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.0016723127337172627\n",
            "Training epoch:  1380 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.0016703292494639754\n",
            "Training epoch:  1381 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.001675718347541988\n",
            "Training epoch:  1382 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.0016830051317811012\n",
            "Training epoch:  1383 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.001681094290688634\n",
            "Training epoch:  1384 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.001669996534474194\n",
            "Training epoch:  1385 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.0016612662002444267\n",
            "Training epoch:  1386 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.0016512309666723013\n",
            "Training epoch:  1387 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.001651761238463223\n",
            "Training epoch:  1388 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.0016547914128750563\n",
            "Training epoch:  1389 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.0016626446740701795\n",
            "Training epoch:  1390 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.0016549670835956931\n",
            "Training epoch:  1391 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.0016522543737664819\n",
            "Training epoch:  1392 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Train loss: 0.0016336317639797926\n",
            "Training epoch:  1393 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Train loss: 0.001626622979529202\n",
            "Training epoch:  1394 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Train loss: 0.0016310053179040551\n",
            "Training epoch:  1395 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Train loss: 0.0016263953875750303\n",
            "Training epoch:  1396 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Train loss: 0.0016237638192251325\n",
            "Training epoch:  1397 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Train loss: 0.0016291049541905522\n",
            "Training epoch:  1398 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Train loss: 0.0016125428956001997\n",
            "Training epoch:  1399 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Train loss: 0.001607511774636805\n",
            "...Saved PCA statistics and last model.\n",
            "Latent Mean values:  [ 0.11456539  0.00913768 -0.02946112  0.07613157 -0.10690714  0.00862124]\n",
            "Latent PCA values:  [2.59287281 2.52255514 2.48588303 2.37372859 2.21508333 2.1139543 ]\n",
            "Training epoch:  1400 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Train loss: 0.0016245752340182662\n",
            "Training epoch:  1401 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Train loss: 0.0016171863535419106\n",
            "Training epoch:  1402 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Train loss: 0.0016341835726052523\n",
            "Training epoch:  1403 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Train loss: 0.0016270775813609362\n",
            "Training epoch:  1404 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Train loss: 0.001623243559151888\n",
            "Training epoch:  1405 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Train loss: 0.0016432915581390262\n",
            "Training epoch:  1406 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.0016657256055623293\n",
            "Training epoch:  1407 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.001676975516602397\n",
            "Training epoch:  1408 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.001679731416516006\n",
            "Training epoch:  1409 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.00166641257237643\n",
            "Training epoch:  1410 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0017\n",
            "Train loss: 0.0016589288134127855\n",
            "Training epoch:  1411 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Train loss: 0.001646350254304707\n",
            "Training epoch:  1412 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Train loss: 0.0016198275843635201\n",
            "Training epoch:  1413 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Train loss: 0.0015991681721061468\n",
            "Training epoch:  1414 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Train loss: 0.0015941316960379481\n",
            "Training epoch:  1415 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Train loss: 0.00156304647680372\n",
            "Training epoch:  1416 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Train loss: 0.001577173126861453\n",
            "Training epoch:  1417 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Train loss: 0.0015699025243520737\n",
            "Training epoch:  1418 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Train loss: 0.001579671516083181\n",
            "Training epoch:  1419 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Train loss: 0.001582688419148326\n",
            "Training epoch:  1420 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Train loss: 0.001571670058183372\n",
            "Training epoch:  1421 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Train loss: 0.001564158359542489\n",
            "Training epoch:  1422 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Train loss: 0.0015787415904924273\n",
            "Training epoch:  1423 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Train loss: 0.0015802063280716538\n",
            "Training epoch:  1424 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Train loss: 0.0015760095557197928\n",
            "Training epoch:  1425 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Train loss: 0.0015861955471336842\n",
            "Training epoch:  1426 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Train loss: 0.0016052883584052324\n",
            "Training epoch:  1427 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Train loss: 0.0015928327338770032\n",
            "Training epoch:  1428 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Train loss: 0.0016054075676947832\n",
            "Training epoch:  1429 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Train loss: 0.001578702824190259\n",
            "Training epoch:  1430 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Train loss: 0.001570834661833942\n",
            "Training epoch:  1431 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Train loss: 0.0015655015595257282\n",
            "Training epoch:  1432 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Train loss: 0.0015687145059928298\n",
            "Training epoch:  1433 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Train loss: 0.001561099081300199\n",
            "Training epoch:  1434 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Train loss: 0.001567081781104207\n",
            "Training epoch:  1435 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Train loss: 0.0015745032578706741\n",
            "Training epoch:  1436 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Train loss: 0.0015867147594690323\n",
            "Training epoch:  1437 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Train loss: 0.0015803634887561202\n",
            "Training epoch:  1438 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Train loss: 0.0015826876042410731\n",
            "Training epoch:  1439 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Train loss: 0.0015702479286119342\n",
            "Training epoch:  1440 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Train loss: 0.001553257112391293\n",
            "Training epoch:  1441 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Train loss: 0.0015704026445746422\n",
            "Training epoch:  1442 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Train loss: 0.0015701621305197477\n",
            "Training epoch:  1443 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Train loss: 0.0015699004288762808\n",
            "Training epoch:  1444 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Train loss: 0.0015550192911177874\n",
            "Training epoch:  1445 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.0015491796657443047\n",
            "Training epoch:  1446 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Train loss: 0.0015535270795226097\n",
            "Training epoch:  1447 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0016\n",
            "Train loss: 0.0015575075522065163\n",
            "Training epoch:  1448 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.0015461948933079839\n",
            "Training epoch:  1449 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.0015178865287452936\n",
            "Training epoch:  1450 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.0015109321102499962\n",
            "Training epoch:  1451 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.0015187173848971725\n",
            "Training epoch:  1452 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.0015264397952705622\n",
            "Training epoch:  1453 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.0015459719579666853\n",
            "Training epoch:  1454 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.001547903986647725\n",
            "Training epoch:  1455 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.0015431665815412998\n",
            "Training epoch:  1456 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.001537055242806673\n",
            "Training epoch:  1457 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.001527641899883747\n",
            "Training epoch:  1458 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.0015380751574411988\n",
            "Training epoch:  1459 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.001519215409643948\n",
            "Training epoch:  1460 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.0015246964758262038\n",
            "Training epoch:  1461 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.0015308595029637218\n",
            "Training epoch:  1462 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.0015271141892299056\n",
            "Training epoch:  1463 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.0015221682842820883\n",
            "Training epoch:  1464 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.0015150364488363266\n",
            "Training epoch:  1465 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.0015212621074169874\n",
            "Training epoch:  1466 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.0015172484563663602\n",
            "Training epoch:  1467 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.0015387049643322825\n",
            "Training epoch:  1468 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.0015099130105227232\n",
            "Training epoch:  1469 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.0015029568457975984\n",
            "Training epoch:  1470 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.0015087403589859605\n",
            "Training epoch:  1471 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.0014899784000590444\n",
            "Training epoch:  1472 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.001486154505982995\n",
            "Training epoch:  1473 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.0014954047510400414\n",
            "Training epoch:  1474 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.0014972869539633393\n",
            "Training epoch:  1475 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.0015013593947514892\n",
            "Training epoch:  1476 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.0014894739724695683\n",
            "Training epoch:  1477 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.0015029343776404858\n",
            "Training epoch:  1478 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.0015021141152828932\n",
            "Training epoch:  1479 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.0015170699916779995\n",
            "Training epoch:  1480 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.001512981136329472\n",
            "Training epoch:  1481 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.0015082296449691057\n",
            "Training epoch:  1482 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.0015208004042506218\n",
            "Training epoch:  1483 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.0015135947614908218\n",
            "Training epoch:  1484 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.001511212787590921\n",
            "Training epoch:  1485 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.001496287528425455\n",
            "Training epoch:  1486 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.001490864553488791\n",
            "Training epoch:  1487 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.0014771672431379557\n",
            "Training epoch:  1488 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.0014756262535229325\n",
            "Training epoch:  1489 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.0014804709935560822\n",
            "Training epoch:  1490 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.0014913452323526144\n",
            "Training epoch:  1491 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.001481499639339745\n",
            "Training epoch:  1492 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.0014721445040777326\n",
            "Training epoch:  1493 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.0014543385477736592\n",
            "Training epoch:  1494 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.0014471536269411445\n",
            "Training epoch:  1495 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.0014498631935566664\n",
            "Training epoch:  1496 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.0014477830845862627\n",
            "Training epoch:  1497 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.0014631565427407622\n",
            "Training epoch:  1498 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.0014760884223505855\n",
            "Training epoch:  1499 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.0014839819632470608\n",
            "...Saved PCA statistics and last model.\n",
            "Latent Mean values:  [-0.29193714 -0.06998774  0.3253829  -0.11834864 -0.12685087 -0.23703426]\n",
            "Latent PCA values:  [2.56688827 2.47864227 2.43534338 2.31974801 2.23350599 2.06267932]\n",
            "Training epoch:  1500 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.001480024540796876\n",
            "Training epoch:  1501 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.0014663297915831208\n",
            "Training epoch:  1502 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.001457672449760139\n",
            "Training epoch:  1503 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.0014625485055148602\n",
            "Training epoch:  1504 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.001445721136406064\n",
            "Training epoch:  1505 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.0014571562642231584\n",
            "Training epoch:  1506 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.0014538170071318746\n",
            "Training epoch:  1507 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.0014445099513977766\n",
            "Training epoch:  1508 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.0014485043939203024\n",
            "Training epoch:  1509 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.0014400441432371736\n",
            "Training epoch:  1510 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.001425227615982294\n",
            "Training epoch:  1511 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.001412617159076035\n",
            "Training epoch:  1512 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.001407886273227632\n",
            "Training epoch:  1513 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.0014173714444041252\n",
            "Training epoch:  1514 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.0014345706440508366\n",
            "Training epoch:  1515 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.0014511438785120845\n",
            "Training epoch:  1516 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.0014390816213563085\n",
            "Training epoch:  1517 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.0014320708578452468\n",
            "Training epoch:  1518 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.001450890558771789\n",
            "Training epoch:  1519 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.00144969392567873\n",
            "Training epoch:  1520 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.001437807222828269\n",
            "Training epoch:  1521 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.0014405433321371675\n",
            "Training epoch:  1522 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.0014348826371133327\n",
            "Training epoch:  1523 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.001439065090380609\n",
            "Training epoch:  1524 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.0014491862384602427\n",
            "Training epoch:  1525 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0015\n",
            "Train loss: 0.0014566914178431034\n",
            "Training epoch:  1526 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.0014398315688595176\n",
            "Training epoch:  1527 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.0014263479970395565\n",
            "Training epoch:  1528 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.0014245938509702682\n",
            "Training epoch:  1529 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.0014004370896145701\n",
            "Training epoch:  1530 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.0013982077362015843\n",
            "Training epoch:  1531 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.0013923257356509566\n",
            "Training epoch:  1532 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.001383645343594253\n",
            "Training epoch:  1533 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.0013708524638786912\n",
            "Training epoch:  1534 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.0013597820652648807\n",
            "Training epoch:  1535 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.0013633603230118752\n",
            "Training epoch:  1536 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.00135858915746212\n",
            "Training epoch:  1537 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.001377520733512938\n",
            "Training epoch:  1538 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.0013537410413846374\n",
            "Training epoch:  1539 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.0013541097287088633\n",
            "Training epoch:  1540 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.00136646069586277\n",
            "Training epoch:  1541 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.001382476300932467\n",
            "Training epoch:  1542 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.0013912844005972147\n",
            "Training epoch:  1543 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.001422812114469707\n",
            "Training epoch:  1544 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.001410994678735733\n",
            "Training epoch:  1545 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.0014265466015785933\n",
            "Training epoch:  1546 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.001407570205628872\n",
            "Training epoch:  1547 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.0014049193123355508\n",
            "Training epoch:  1548 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.0013778600841760635\n",
            "Training epoch:  1549 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.0014045843854546547\n",
            "Training epoch:  1550 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.0014057975495234132\n",
            "Training epoch:  1551 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.0013856489676982164\n",
            "Training epoch:  1552 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.001370157115161419\n",
            "Training epoch:  1553 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0013497426407411695\n",
            "Training epoch:  1554 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0013406660873442888\n",
            "Training epoch:  1555 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0013469606637954712\n",
            "Training epoch:  1556 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0013375483686104417\n",
            "Training epoch:  1557 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0013232644414529204\n",
            "Training epoch:  1558 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0013404464116320014\n",
            "Training epoch:  1559 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0013368967920541763\n",
            "Training epoch:  1560 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.001344294287264347\n",
            "Training epoch:  1561 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0013440592447295785\n",
            "Training epoch:  1562 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.0013639364624395967\n",
            "Training epoch:  1563 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.0013656547525897622\n",
            "Training epoch:  1564 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.0013610980240628123\n",
            "Training epoch:  1565 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.0013787432108074427\n",
            "Training epoch:  1566 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.0013701755087822676\n",
            "Training epoch:  1567 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.001388896955177188\n",
            "Training epoch:  1568 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.0013986941194161773\n",
            "Training epoch:  1569 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.001383476541377604\n",
            "Training epoch:  1570 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.0013894157018512487\n",
            "Training epoch:  1571 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.0013753026723861694\n",
            "Training epoch:  1572 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.0013683839933946729\n",
            "Training epoch:  1573 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0013482868671417236\n",
            "Training epoch:  1574 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0013320009456947446\n",
            "Training epoch:  1575 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0013225447619333863\n",
            "Training epoch:  1576 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.001343567855656147\n",
            "Training epoch:  1577 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0013281269930303097\n",
            "Training epoch:  1578 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0013305653119459748\n",
            "Training epoch:  1579 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.0013548878487199545\n",
            "Training epoch:  1580 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0013428364181891084\n",
            "Training epoch:  1581 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.0013638800010085106\n",
            "Training epoch:  1582 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.0013631762703880668\n",
            "Training epoch:  1583 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.001366818672977388\n",
            "Training epoch:  1584 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.001368872239254415\n",
            "Training epoch:  1585 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0014\n",
            "Train loss: 0.0013587678549811244\n",
            "Training epoch:  1586 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0013453628635033965\n",
            "Training epoch:  1587 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0013177696382626891\n",
            "Training epoch:  1588 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0013428865931928158\n",
            "Training epoch:  1589 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0013309045461937785\n",
            "Training epoch:  1590 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.001326581696048379\n",
            "Training epoch:  1591 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0013400105526670814\n",
            "Training epoch:  1592 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0013136877678334713\n",
            "Training epoch:  1593 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0013073856243863702\n",
            "Training epoch:  1594 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0012967243092134595\n",
            "Training epoch:  1595 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0013018475146964192\n",
            "Training epoch:  1596 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0013126732083037496\n",
            "Training epoch:  1597 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0013021323829889297\n",
            "Training epoch:  1598 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0013177883811295033\n",
            "Training epoch:  1599 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0013156018685549498\n",
            "...Saved PCA statistics and last model.\n",
            "Latent Mean values:  [ 0.06630851 -0.00117691 -0.05833737 -0.06490794 -0.1821864  -0.3994806 ]\n",
            "Latent PCA values:  [2.55215532 2.4418551  2.39900107 2.28138612 2.24674421 2.05248665]\n",
            "Training epoch:  1600 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.001327325007878244\n",
            "Training epoch:  1601 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0013303349260240793\n",
            "Training epoch:  1602 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0013308783527463675\n",
            "Training epoch:  1603 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.001331596402451396\n",
            "Training epoch:  1604 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0013327952474355698\n",
            "Training epoch:  1605 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0013314075767993927\n",
            "Training epoch:  1606 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0013445143122226\n",
            "Training epoch:  1607 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0013494259910658002\n",
            "Training epoch:  1608 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0013340808218345046\n",
            "Training epoch:  1609 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0013220221735537052\n",
            "Training epoch:  1610 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0013207589508965611\n",
            "Training epoch:  1611 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0013191045727580786\n",
            "Training epoch:  1612 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.001325728720985353\n",
            "Training epoch:  1613 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0013334654504433274\n",
            "Training epoch:  1614 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0013256908860057592\n",
            "Training epoch:  1615 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0013164932606741786\n",
            "Training epoch:  1616 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0013096833135932684\n",
            "Training epoch:  1617 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.001298614195547998\n",
            "Training epoch:  1618 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0012720279628410935\n",
            "Training epoch:  1619 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0012817200040444732\n",
            "Training epoch:  1620 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0012845846358686686\n",
            "Training epoch:  1621 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0012816176749765873\n",
            "Training epoch:  1622 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.001279630116187036\n",
            "Training epoch:  1623 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0012707015266641974\n",
            "Training epoch:  1624 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0012601963244378567\n",
            "Training epoch:  1625 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0012463191524147987\n",
            "Training epoch:  1626 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0012556278379634023\n",
            "Training epoch:  1627 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0012554486747831106\n",
            "Training epoch:  1628 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0012632521102204919\n",
            "Training epoch:  1629 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.001267502666451037\n",
            "Training epoch:  1630 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0012574471766129136\n",
            "Training epoch:  1631 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.00124978250823915\n",
            "Training epoch:  1632 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0012499027652665973\n",
            "Training epoch:  1633 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0012314013438299298\n",
            "Training epoch:  1634 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0012227313127368689\n",
            "Training epoch:  1635 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0012429284397512674\n",
            "Training epoch:  1636 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0012485331390053034\n",
            "Training epoch:  1637 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0012540251482278109\n",
            "Training epoch:  1638 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.001269425847567618\n",
            "Training epoch:  1639 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.001273236470296979\n",
            "Training epoch:  1640 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0012604283401742578\n",
            "Training epoch:  1641 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0012540124589577317\n",
            "Training epoch:  1642 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0012546039652079344\n",
            "Training epoch:  1643 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0012715797638520598\n",
            "Training epoch:  1644 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0012734013143926859\n",
            "Training epoch:  1645 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0012730698799714446\n",
            "Training epoch:  1646 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0012791733024641871\n",
            "Training epoch:  1647 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.001286212238483131\n",
            "Training epoch:  1648 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0012466374319046736\n",
            "Training epoch:  1649 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0012648592237383127\n",
            "Training epoch:  1650 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0013\n",
            "Train loss: 0.0012543192133307457\n",
            "Training epoch:  1651 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0012436711695045233\n",
            "Training epoch:  1652 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0012356776278465986\n",
            "Training epoch:  1653 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.001214492367580533\n",
            "Training epoch:  1654 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0012159519828855991\n",
            "Training epoch:  1655 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0012090201489627361\n",
            "Training epoch:  1656 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0011912821792066097\n",
            "Training epoch:  1657 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0011912857880815864\n",
            "Training epoch:  1658 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0011949229519814253\n",
            "Training epoch:  1659 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0012074275873601437\n",
            "Training epoch:  1660 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0012194502633064985\n",
            "Training epoch:  1661 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0012290764134377241\n",
            "Training epoch:  1662 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0012444708263501525\n",
            "Training epoch:  1663 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0012317330110818148\n",
            "Training epoch:  1664 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0012148967944085598\n",
            "Training epoch:  1665 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0012260968796908855\n",
            "Training epoch:  1666 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.001232685288414359\n",
            "Training epoch:  1667 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0012280141236260533\n",
            "Training epoch:  1668 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.001240521320141852\n",
            "Training epoch:  1669 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0012296069180592895\n",
            "Training epoch:  1670 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0012240107171237469\n",
            "Training epoch:  1671 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0012256238842383027\n",
            "Training epoch:  1672 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.001227186992764473\n",
            "Training epoch:  1673 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.001226270105689764\n",
            "Training epoch:  1674 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0012218519113957882\n",
            "Training epoch:  1675 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.001217408454976976\n",
            "Training epoch:  1676 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0012078993022441864\n",
            "Training epoch:  1677 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0012193942675366998\n",
            "Training epoch:  1678 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0012210927670821548\n",
            "Training epoch:  1679 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0012467155465856194\n",
            "Training epoch:  1680 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0012209765845909715\n",
            "Training epoch:  1681 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.001213196781463921\n",
            "Training epoch:  1682 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0012034073006361723\n",
            "Training epoch:  1683 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0012133133132010698\n",
            "Training epoch:  1684 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0012040238361805677\n",
            "Training epoch:  1685 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0011857461649924517\n",
            "Training epoch:  1686 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0011928342282772064\n",
            "Training epoch:  1687 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0011817524209618568\n",
            "Training epoch:  1688 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0011736499145627022\n",
            "Training epoch:  1689 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0011858295183628798\n",
            "Training epoch:  1690 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0011877375654876232\n",
            "Training epoch:  1691 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0012075776467099786\n",
            "Training epoch:  1692 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.001200079801492393\n",
            "Training epoch:  1693 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0012002353323623538\n",
            "Training epoch:  1694 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.001200624043121934\n",
            "Training epoch:  1695 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.001197085133753717\n",
            "Training epoch:  1696 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.001189669594168663\n",
            "Training epoch:  1697 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0011753042927011847\n",
            "Training epoch:  1698 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0011919979006052017\n",
            "Training epoch:  1699 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.00119821319822222\n",
            "...Saved PCA statistics and last model.\n",
            "Latent Mean values:  [ 0.01674761  0.08786813 -0.15658851 -0.11987636  0.17595562 -0.3311494 ]\n",
            "Latent PCA values:  [2.50990494 2.47918318 2.46340492 2.27044777 2.18685688 2.00583931]\n",
            "Training epoch:  1700 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0011896518990397453\n",
            "Training epoch:  1701 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.001186512759886682\n",
            "Training epoch:  1702 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0011693898122757673\n",
            "Training epoch:  1703 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0011841455707326531\n",
            "Training epoch:  1704 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0011821724474430084\n",
            "Training epoch:  1705 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0011739475885406137\n",
            "Training epoch:  1706 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.001179540529847145\n",
            "Training epoch:  1707 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0011883858824148774\n",
            "Training epoch:  1708 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0011733672581613064\n",
            "Training epoch:  1709 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.00117677659727633\n",
            "Training epoch:  1710 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0011792427394539118\n",
            "Training epoch:  1711 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0011734685394912958\n",
            "Training epoch:  1712 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0011809029383584857\n",
            "Training epoch:  1713 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0011673399712890387\n",
            "Training epoch:  1714 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0011568632908165455\n",
            "Training epoch:  1715 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0011439513182267547\n",
            "Training epoch:  1716 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0011542739812284708\n",
            "Training epoch:  1717 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0011577702825888991\n",
            "Training epoch:  1718 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0011605620384216309\n",
            "Training epoch:  1719 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0011424621334299445\n",
            "Training epoch:  1720 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.001150992582552135\n",
            "Training epoch:  1721 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0011488806921988726\n",
            "Training epoch:  1722 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0011565370950847864\n",
            "Training epoch:  1723 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0011482585687190294\n",
            "Training epoch:  1724 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.00113513576798141\n",
            "Training epoch:  1725 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0011405482655391097\n",
            "Training epoch:  1726 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0011419024085626006\n",
            "Training epoch:  1727 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0011543718865141273\n",
            "Training epoch:  1728 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.00116460130084306\n",
            "Training epoch:  1729 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.001181202009320259\n",
            "Training epoch:  1730 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0011822108644992113\n",
            "Training epoch:  1731 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0011811061995103955\n",
            "Training epoch:  1732 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.001173372962512076\n",
            "Training epoch:  1733 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.001154729980044067\n",
            "Training epoch:  1734 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0011470797471702099\n",
            "Training epoch:  1735 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0011406037956476212\n",
            "Training epoch:  1736 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.001134770456701517\n",
            "Training epoch:  1737 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.00112258514855057\n",
            "Training epoch:  1738 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0011196713894605637\n",
            "Training epoch:  1739 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0011160385329276323\n",
            "Training epoch:  1740 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.001107125310227275\n",
            "Training epoch:  1741 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0011120405979454517\n",
            "Training epoch:  1742 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0011035429779440165\n",
            "Training epoch:  1743 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0010912829311564565\n",
            "Training epoch:  1744 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0010872638085857034\n",
            "Training epoch:  1745 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0011095033260062337\n",
            "Training epoch:  1746 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0011174200335517526\n",
            "Training epoch:  1747 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.001118126674555242\n",
            "Training epoch:  1748 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.001123368856497109\n",
            "Training epoch:  1749 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0011362185468897223\n",
            "Training epoch:  1750 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0011297182645648718\n",
            "Training epoch:  1751 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.001156050362624228\n",
            "Training epoch:  1752 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0012\n",
            "Train loss: 0.0011734861182048917\n",
            "Training epoch:  1753 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.001145319314673543\n",
            "Training epoch:  1754 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0011229747906327248\n",
            "Training epoch:  1755 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0011154718231409788\n",
            "Training epoch:  1756 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0010851657716557384\n",
            "Training epoch:  1757 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0010878241155296564\n",
            "Training epoch:  1758 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0011107231257483363\n",
            "Training epoch:  1759 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0011060010874643922\n",
            "Training epoch:  1760 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.001097479136660695\n",
            "Training epoch:  1761 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0011072448687627912\n",
            "Training epoch:  1762 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0010922190267592669\n",
            "Training epoch:  1763 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0010891148122027516\n",
            "Training epoch:  1764 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0010745462495833635\n",
            "Training epoch:  1765 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0010824116179719567\n",
            "Training epoch:  1766 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.001088718301616609\n",
            "Training epoch:  1767 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0010792412795126438\n",
            "Training epoch:  1768 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0011036881478503346\n",
            "Training epoch:  1769 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0011212918907403946\n",
            "Training epoch:  1770 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0011264127679169178\n",
            "Training epoch:  1771 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0011259689927101135\n",
            "Training epoch:  1772 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0011115750530734658\n",
            "Training epoch:  1773 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0011278779711574316\n",
            "Training epoch:  1774 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0011098390677943826\n",
            "Training epoch:  1775 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0011128759942948818\n",
            "Training epoch:  1776 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.001093743136152625\n",
            "Training epoch:  1777 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.001088202465325594\n",
            "Training epoch:  1778 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0010796783026307821\n",
            "Training epoch:  1779 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0010956961195915937\n",
            "Training epoch:  1780 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0010987346759065986\n",
            "Training epoch:  1781 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0011137574911117554\n",
            "Training epoch:  1782 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0010963883250951767\n",
            "Training epoch:  1783 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.001118905027396977\n",
            "Training epoch:  1784 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0011099089169874787\n",
            "Training epoch:  1785 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0011084344005212188\n",
            "Training epoch:  1786 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0011154210660606623\n",
            "Training epoch:  1787 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0011302130296826363\n",
            "Training epoch:  1788 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0011315400479361415\n",
            "Training epoch:  1789 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0011220453307032585\n",
            "Training epoch:  1790 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0010965942637994885\n",
            "Training epoch:  1791 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0010804686462506652\n",
            "Training epoch:  1792 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0010846726363524795\n",
            "Training epoch:  1793 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.001080088783055544\n",
            "Training epoch:  1794 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.001061778049916029\n",
            "Training epoch:  1795 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0010551726445555687\n",
            "Training epoch:  1796 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0010543749667704105\n",
            "Training epoch:  1797 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.0010415575234219432\n",
            "Training epoch:  1798 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.001047462341375649\n",
            "Training epoch:  1799 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.001055186497978866\n",
            "...Saved PCA statistics and last model.\n",
            "Latent Mean values:  [-0.07874136  0.20373514  0.16409482 -0.19761027  0.05081916 -0.05280524]\n",
            "Latent PCA values:  [2.47209029 2.4186449  2.33340519 2.27564342 2.15500665 2.04736538]\n",
            "Training epoch:  1800 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0010805841302499175\n",
            "Training epoch:  1801 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0010773760732263327\n",
            "Training epoch:  1802 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0011121064890176058\n",
            "Training epoch:  1803 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0011190548539161682\n",
            "Training epoch:  1804 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0010889960685744882\n",
            "Training epoch:  1805 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.001095832441933453\n",
            "Training epoch:  1806 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0011008561123162508\n",
            "Training epoch:  1807 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0011066780425608158\n",
            "Training epoch:  1808 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.001096490304917097\n",
            "Training epoch:  1809 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.001096431165933609\n",
            "Training epoch:  1810 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0010823007905855775\n",
            "Training epoch:  1811 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0010796089190989733\n",
            "Training epoch:  1812 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0010601832764223218\n",
            "Training epoch:  1813 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0010595388012006879\n",
            "Training epoch:  1814 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.001059212489053607\n",
            "Training epoch:  1815 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0010528634302318096\n",
            "Training epoch:  1816 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0010571321472525597\n",
            "Training epoch:  1817 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.0010447469539940357\n",
            "Training epoch:  1818 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.0010417368030175567\n",
            "Training epoch:  1819 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.0010410461109131575\n",
            "Training epoch:  1820 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.001052284613251686\n",
            "Training epoch:  1821 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.0010380748426541686\n",
            "Training epoch:  1822 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0010571139864623547\n",
            "Training epoch:  1823 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.0010443752398714423\n",
            "Training epoch:  1824 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.0010418974561616778\n",
            "Training epoch:  1825 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.001062204479239881\n",
            "Training epoch:  1826 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0010568471625447273\n",
            "Training epoch:  1827 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.0010476192692294717\n",
            "Training epoch:  1828 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.001026793965138495\n",
            "Training epoch:  1829 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.0010476004099473357\n",
            "Training epoch:  1830 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.001063674339093268\n",
            "Training epoch:  1831 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.001053983229212463\n",
            "Training epoch:  1832 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0010547091951593757\n",
            "Training epoch:  1833 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0010551480809226632\n",
            "Training epoch:  1834 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0011\n",
            "Train loss: 0.0010622665286064148\n",
            "Training epoch:  1835 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.0010459874756634235\n",
            "Training epoch:  1836 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.0010369133669883013\n",
            "Training epoch:  1837 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.0010215389775112271\n",
            "Training epoch:  1838 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.0010100509971380234\n",
            "Training epoch:  1839 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.001014649635180831\n",
            "Training epoch:  1840 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.0010135781485587358\n",
            "Training epoch:  1841 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.001025873003527522\n",
            "Training epoch:  1842 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.0010212610941380262\n",
            "Training epoch:  1843 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.0010336776031181216\n",
            "Training epoch:  1844 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.0010380175663158298\n",
            "Training epoch:  1845 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.0010149802546948195\n",
            "Training epoch:  1846 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.0010100509971380234\n",
            "Training epoch:  1847 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.0010020402260124683\n",
            "Training epoch:  1848 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.0010019359178841114\n",
            "Training epoch:  1849 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.0010086138499900699\n",
            "Training epoch:  1850 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.0010177898220717907\n",
            "Training epoch:  1851 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.0010185937862843275\n",
            "Training epoch:  1852 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.0010232494678348303\n",
            "Training epoch:  1853 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.0010322191519662738\n",
            "Training epoch:  1854 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.0010330583900213242\n",
            "Training epoch:  1855 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.0010210168547928333\n",
            "Training epoch:  1856 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.0010130824521183968\n",
            "Training epoch:  1857 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.0010117734782397747\n",
            "Training epoch:  1858 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.0010017795721068978\n",
            "Training epoch:  1859 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.0010043829679489136\n",
            "Training epoch:  1860 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.0010110489092767239\n",
            "Training epoch:  1861 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.0010063074296340346\n",
            "Training epoch:  1862 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.0010168520966544747\n",
            "Training epoch:  1863 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.001015442656353116\n",
            "Training epoch:  1864 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.001013525645248592\n",
            "Training epoch:  1865 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.0010082586668431759\n",
            "Training epoch:  1866 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.001006405451335013\n",
            "Training epoch:  1867 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.9815e-04\n",
            "Train loss: 0.0009981498587876558\n",
            "Training epoch:  1868 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.9818e-04\n",
            "Train loss: 0.000998180010356009\n",
            "Training epoch:  1869 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.9221e-04\n",
            "Train loss: 0.0009922090684995055\n",
            "Training epoch:  1870 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.7884e-04\n",
            "Train loss: 0.000978842843323946\n",
            "Training epoch:  1871 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.7426e-04\n",
            "Train loss: 0.0009742619004100561\n",
            "Training epoch:  1872 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.8078e-04\n",
            "Train loss: 0.000980775454081595\n",
            "Training epoch:  1873 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.9397e-04\n",
            "Train loss: 0.0009939667070284486\n",
            "Training epoch:  1874 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.9641e-04\n",
            "Train loss: 0.000996414222754538\n",
            "Training epoch:  1875 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.8314e-04\n",
            "Train loss: 0.0009831370553001761\n",
            "Training epoch:  1876 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.8328e-04\n",
            "Train loss: 0.0009832845535129309\n",
            "Training epoch:  1877 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 9.9140e-04\n",
            "Train loss: 0.0009914017282426357\n",
            "Training epoch:  1878 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.0010026885429397225\n",
            "Training epoch:  1879 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.8467e-04\n",
            "Train loss: 0.0009846650063991547\n",
            "Training epoch:  1880 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.7331e-04\n",
            "Train loss: 0.000973312824498862\n",
            "Training epoch:  1881 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.9351e-04\n",
            "Train loss: 0.000993511755950749\n",
            "Training epoch:  1882 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.8994e-04\n",
            "Train loss: 0.0009899430442601442\n",
            "Training epoch:  1883 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.0010038024047389627\n",
            "Training epoch:  1884 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.0010089683346450329\n",
            "Training epoch:  1885 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.8521e-04\n",
            "Train loss: 0.0009852099465206265\n",
            "Training epoch:  1886 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.0010022909846156836\n",
            "Training epoch:  1887 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.0010192834306508303\n",
            "Training epoch:  1888 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.0010113627649843693\n",
            "Training epoch:  1889 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.8898e-04\n",
            "Train loss: 0.0009889800567179918\n",
            "Training epoch:  1890 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.8854e-04\n",
            "Train loss: 0.0009885367471724749\n",
            "Training epoch:  1891 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.7041e-04\n",
            "Train loss: 0.0009704125695861876\n",
            "Training epoch:  1892 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 9.6689e-04\n",
            "Train loss: 0.000966890249401331\n",
            "Training epoch:  1893 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.6997e-04\n",
            "Train loss: 0.0009699725778773427\n",
            "Training epoch:  1894 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.8098e-04\n",
            "Train loss: 0.0009809836046770215\n",
            "Training epoch:  1895 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.7707e-04\n",
            "Train loss: 0.0009770742617547512\n",
            "Training epoch:  1896 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.9010e-04\n",
            "Train loss: 0.0009900993900373578\n",
            "Training epoch:  1897 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.8068e-04\n",
            "Train loss: 0.0009806806920096278\n",
            "Training epoch:  1898 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 9.7941e-04\n",
            "Train loss: 0.0009794067591428757\n",
            "Training epoch:  1899 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.9742e-04\n",
            "Train loss: 0.0009974235435947776\n",
            "...Saved PCA statistics and last model.\n",
            "Latent Mean values:  [ 0.03562284  0.14099663 -0.01026207 -0.03053221 -0.09190951 -0.21550593]\n",
            "Latent PCA values:  [2.5060386  2.37420958 2.2955489  2.19763612 2.16638543 2.06734828]\n",
            "Training epoch:  1900 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.8308e-04\n",
            "Train loss: 0.000983084668405354\n",
            "Training epoch:  1901 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.9179e-04\n",
            "Train loss: 0.0009917911374941468\n",
            "Training epoch:  1902 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 9.9229e-04\n",
            "Train loss: 0.000992292771115899\n",
            "Training epoch:  1903 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.9272e-04\n",
            "Train loss: 0.0009927168721333146\n",
            "Training epoch:  1904 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.9708e-04\n",
            "Train loss: 0.000997075461782515\n",
            "Training epoch:  1905 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.0010031687561422586\n",
            "Training epoch:  1906 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.9325e-04\n",
            "Train loss: 0.000993250752799213\n",
            "Training epoch:  1907 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.0010117656784132123\n",
            "Training epoch:  1908 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.9738e-04\n",
            "Train loss: 0.000997377559542656\n",
            "Training epoch:  1909 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 0.0010\n",
            "Train loss: 0.0010082926601171494\n",
            "Training epoch:  1910 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.9018e-04\n",
            "Train loss: 0.000990184722468257\n",
            "Training epoch:  1911 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.8722e-04\n",
            "Train loss: 0.000987216946668923\n",
            "Training epoch:  1912 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.7993e-04\n",
            "Train loss: 0.000979926553554833\n",
            "Training epoch:  1913 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.7541e-04\n",
            "Train loss: 0.0009754134225659072\n",
            "Training epoch:  1914 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.6731e-04\n",
            "Train loss: 0.0009673101594671607\n",
            "Training epoch:  1915 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.6776e-04\n",
            "Train loss: 0.0009677616762928665\n",
            "Training epoch:  1916 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.6349e-04\n",
            "Train loss: 0.0009634901653043926\n",
            "Training epoch:  1917 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.6819e-04\n",
            "Train loss: 0.0009681949741207063\n",
            "Training epoch:  1918 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.4288e-04\n",
            "Train loss: 0.0009428802295587957\n",
            "Training epoch:  1919 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.4839e-04\n",
            "Train loss: 0.0009483934845775366\n",
            "Training epoch:  1920 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.5691e-04\n",
            "Train loss: 0.0009569140966050327\n",
            "Training epoch:  1921 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.5448e-04\n",
            "Train loss: 0.0009544799686409533\n",
            "Training epoch:  1922 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.5691e-04\n",
            "Train loss: 0.0009569135145284235\n",
            "Training epoch:  1923 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.5907e-04\n",
            "Train loss: 0.0009590709814801812\n",
            "Training epoch:  1924 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.6054e-04\n",
            "Train loss: 0.0009605393279343843\n",
            "Training epoch:  1925 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.5831e-04\n",
            "Train loss: 0.0009583145147189498\n",
            "Training epoch:  1926 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.4761e-04\n",
            "Train loss: 0.0009476093691773713\n",
            "Training epoch:  1927 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.6618e-04\n",
            "Train loss: 0.0009661799995228648\n",
            "Training epoch:  1928 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.6635e-04\n",
            "Train loss: 0.0009663546225056052\n",
            "Training epoch:  1929 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.6982e-04\n",
            "Train loss: 0.0009698187350295484\n",
            "Training epoch:  1930 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.6807e-04\n",
            "Train loss: 0.0009680734947323799\n",
            "Training epoch:  1931 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.4932e-04\n",
            "Train loss: 0.000949322187807411\n",
            "Training epoch:  1932 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.6333e-04\n",
            "Train loss: 0.0009633348090574145\n",
            "Training epoch:  1933 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.7526e-04\n",
            "Train loss: 0.0009752627811394632\n",
            "Training epoch:  1934 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.6734e-04\n",
            "Train loss: 0.0009673413005657494\n",
            "Training epoch:  1935 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.5120e-04\n",
            "Train loss: 0.0009512003161944449\n",
            "Training epoch:  1936 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.5035e-04\n",
            "Train loss: 0.0009503540932200849\n",
            "Training epoch:  1937 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.4058e-04\n",
            "Train loss: 0.00094057951355353\n",
            "Training epoch:  1938 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.4658e-04\n",
            "Train loss: 0.0009465849725529552\n",
            "Training epoch:  1939 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.4154e-04\n",
            "Train loss: 0.0009415405220352113\n",
            "Training epoch:  1940 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.4060e-04\n",
            "Train loss: 0.0009406027966178954\n",
            "Training epoch:  1941 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.5084e-04\n",
            "Train loss: 0.0009508401853963733\n",
            "Training epoch:  1942 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.5247e-04\n",
            "Train loss: 0.0009524747729301453\n",
            "Training epoch:  1943 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 9.5608e-04\n",
            "Train loss: 0.0009560822509229183\n",
            "Training epoch:  1944 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.3715e-04\n",
            "Train loss: 0.000937147531658411\n",
            "Training epoch:  1945 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.3267e-04\n",
            "Train loss: 0.0009326676954515278\n",
            "Training epoch:  1946 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.2417e-04\n",
            "Train loss: 0.0009241733350791037\n",
            "Training epoch:  1947 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.1573e-04\n",
            "Train loss: 0.0009157324093393981\n",
            "Training epoch:  1948 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.3004e-04\n",
            "Train loss: 0.000930041482206434\n",
            "Training epoch:  1949 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 9.0417e-04\n",
            "Train loss: 0.0009041735902428627\n",
            "Training epoch:  1950 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.1357e-04\n",
            "Train loss: 0.0009135703439824283\n",
            "Training epoch:  1951 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.1541e-04\n",
            "Train loss: 0.0009154140716418624\n",
            "Training epoch:  1952 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 8.9624e-04\n",
            "Train loss: 0.0008962435531429946\n",
            "Training epoch:  1953 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.0807e-04\n",
            "Train loss: 0.0009080719319172204\n",
            "Training epoch:  1954 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 8.9901e-04\n",
            "Train loss: 0.0008990094065666199\n",
            "Training epoch:  1955 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.2202e-04\n",
            "Train loss: 0.0009220176143571734\n",
            "Training epoch:  1956 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.1754e-04\n",
            "Train loss: 0.0009175444138236344\n",
            "Training epoch:  1957 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.0503e-04\n",
            "Train loss: 0.0009050326189026237\n",
            "Training epoch:  1958 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.1447e-04\n",
            "Train loss: 0.0009144747746177018\n",
            "Training epoch:  1959 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.2742e-04\n",
            "Train loss: 0.0009274237090721726\n",
            "Training epoch:  1960 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.2581e-04\n",
            "Train loss: 0.0009258130448870361\n",
            "Training epoch:  1961 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.3217e-04\n",
            "Train loss: 0.0009321719408035278\n",
            "Training epoch:  1962 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.2900e-04\n",
            "Train loss: 0.0009289977024309337\n",
            "Training epoch:  1963 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.2943e-04\n",
            "Train loss: 0.0009294276242144406\n",
            "Training epoch:  1964 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.0956e-04\n",
            "Train loss: 0.0009095581481233239\n",
            "Training epoch:  1965 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.1042e-04\n",
            "Train loss: 0.0009104165364988148\n",
            "Training epoch:  1966 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.1850e-04\n",
            "Train loss: 0.0009185018134303391\n",
            "Training epoch:  1967 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 9.1571e-04\n",
            "Train loss: 0.0009157140157185495\n",
            "Training epoch:  1968 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.1610e-04\n",
            "Train loss: 0.0009161049383692443\n",
            "Training epoch:  1969 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.0922e-04\n",
            "Train loss: 0.0009092245600186288\n",
            "Training epoch:  1970 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.1473e-04\n",
            "Train loss: 0.0009147305390797555\n",
            "Training epoch:  1971 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.0842e-04\n",
            "Train loss: 0.0009084166958928108\n",
            "Training epoch:  1972 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 8.9521e-04\n",
            "Train loss: 0.0008952061762101948\n",
            "Training epoch:  1973 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 8.7453e-04\n",
            "Train loss: 0.0008745308150537312\n",
            "Training epoch:  1974 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 8.6560e-04\n",
            "Train loss: 0.0008656008867546916\n",
            "Training epoch:  1975 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 8.7929e-04\n",
            "Train loss: 0.0008792889420874417\n",
            "Training epoch:  1976 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 8.8473e-04\n",
            "Train loss: 0.0008847293793223798\n",
            "Training epoch:  1977 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 8.8802e-04\n",
            "Train loss: 0.0008880200912244618\n",
            "Training epoch:  1978 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.0015e-04\n",
            "Train loss: 0.0009001498692668974\n",
            "Training epoch:  1979 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 8.8439e-04\n",
            "Train loss: 0.0008843930554576218\n",
            "Training epoch:  1980 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 8.9065e-04\n",
            "Train loss: 0.0008906505536288023\n",
            "Training epoch:  1981 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 8.9318e-04\n",
            "Train loss: 0.0008931783959269524\n",
            "Training epoch:  1982 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.0550e-04\n",
            "Train loss: 0.0009055027621798217\n",
            "Training epoch:  1983 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.2197e-04\n",
            "Train loss: 0.0009219685452990234\n",
            "Training epoch:  1984 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.1290e-04\n",
            "Train loss: 0.0009129007230512798\n",
            "Training epoch:  1985 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.3080e-04\n",
            "Train loss: 0.0009308032458648086\n",
            "Training epoch:  1986 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.2805e-04\n",
            "Train loss: 0.0009280455997213721\n",
            "Training epoch:  1987 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.3323e-04\n",
            "Train loss: 0.000933226605411619\n",
            "Training epoch:  1988 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.1602e-04\n",
            "Train loss: 0.0009160203626379371\n",
            "Training epoch:  1989 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.1740e-04\n",
            "Train loss: 0.0009173985454253852\n",
            "Training epoch:  1990 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.1974e-04\n",
            "Train loss: 0.0009197431500069797\n",
            "Training epoch:  1991 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.1454e-04\n",
            "Train loss: 0.0009145444491878152\n",
            "Training epoch:  1992 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 8.9630e-04\n",
            "Train loss: 0.0008963012369349599\n",
            "Training epoch:  1993 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.0865e-04\n",
            "Train loss: 0.000908654707018286\n",
            "Training epoch:  1994 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.1185e-04\n",
            "Train loss: 0.000911852577701211\n",
            "Training epoch:  1995 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 0s 2ms/step - loss: 9.1136e-04\n",
            "Train loss: 0.0009113587439060211\n",
            "Training epoch:  1996 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.0226e-04\n",
            "Train loss: 0.0009022647282108665\n",
            "Training epoch:  1997 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.0534e-04\n",
            "Train loss: 0.0009053425746969879\n",
            "Training epoch:  1998 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 8.9751e-04\n",
            "Train loss: 0.0008975118398666382\n",
            "Training epoch:  1999 of 2000\n",
            "Epoch 1/1\n",
            "296/296 [==============================] - 1s 2ms/step - loss: 9.0864e-04\n",
            "Train loss: 0.0009086351492442191\n",
            "...Saved PCA statistics and last model.\n",
            "Latent Mean values:  [ 0.10191115 -0.04321165 -0.06824845 -0.02294343 -0.09998311 -0.05885071]\n",
            "Latent PCA values:  [2.50695517 2.39650568 2.30125292 2.21009193 2.17378149 2.0220913 ]\n",
            "...Done.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAFCVJREFUeJzt3X+wXGd93/H3JxaGAAbHSGGCJEdO\nItIYagJza+g4TVzsTGRDLKbQjBVCoBg0beKGFNrGlGBaM20gZMiPwZgohjpQ145DDVWCiJMQZ2hD\n7VguYCwbgWyMLQUqmR/Gxk2MJ9/+sUfpcq2rPffevXd3n/t+zezcPed5dvd77jn72XOfc/bcVBWS\npLZ8x6QLkCSNn+EuSQ0y3CWpQYa7JDXIcJekBhnuktQgw11rUpJK8gOTrkNaKYa7Ji7JPUn+b5KH\nhm7vmnRdC0nyyiS3JvlGkoNJfjXJuqH2U5J8KMk3k3wxyU/Pe/xPd/O/meTDSU7p+1ipL8Nd0+In\nq+rJQ7eLJ13QcTwR+EVgPfB84BzgXw+1Xw48AjwdeDlwRZJnAXQ/fxt4Rdf+MPDuPo+VFsNw11RL\n8qokf5HkXUkeSPLZJOcMtT8jye4kX01yIMlrh9pOSPLvktyV5MFub3vz0NOfm+TzSb6e5PIk6VNT\nVV1RVf+jqh6pqkPA1cBZ3Ws+CXgp8Oaqeqiq/iewm0GYwyCw/6CqPl5VDwFvBv5JkpN6PFbqzXDX\nLHg+cBeDPeW3ANcPDWVcCxwEngG8DPhPSV7Ytb0e2AGcDzwFeDWDPeWjXgz8A+AM4KeAnwBIcmoX\n+Kf2rO9HgX3d/WcCj1bV54baPw0c3ft+VjcNQFXdxWBP/Zk9Hiv1ZrhrWny4C9Sjt9cOtR0GfqOq\nvlVVvwfsB17U7YWfBfxSVf11VX0KuBL42e5xrwF+uar218Cnq+orQ8/7tqr6elXdC9wI/DBAVd1b\nVSd3848ryauBOeDXullPBr4xr9sDwElD7Q8s0D7qsVJv60Z3kVbFS6rqTxdoO1TffoW7LzLYU38G\n8NWqenBe21x3fzODPf6FfHno/sMMwrW3JC8BfgU4t6ru72Y/xOCvhGFPAR7s0f63Ix4r9eaeu2bB\nxnnj4acCf9XdTkly0ry2Q939+4DvX4mCkmwDfofBgeDPDDV9DliXZOvQvOfw/4dt9nXTR5/n+4DH\nd48b9VipN8Nds+C7gV9I8rgk/xT4IWBPVd0HfAL4lSRPSHIGcBHwX7rHXQm8NcnWDJyR5GnLLaYb\n078aeGlV/eVwW1V9E7geuCzJk5KcBWwHPtB1uRr4yST/qDuAehlwfVU92OOxUm+Gu6bFH8w7z/1D\nQ203A1uB+4H/CLxsaOx8B7CFwV78h4C3DA3vvBO4DvhjBmPZ7wW+c1Qh3QHVh45zQPXNwFOBPUP1\nfnSo/ee61zkMXAP8i6raB9D9/OcMQv4wg/H0n+vzWGkx4j/r0DRL8irgNVX1I5OuRZol7rlLUoMM\nd0lqkMMyktQg99wlqUET+xLT+vXra8uWLZN6eUmaSbfeeuv9VbVhVL+JhfuWLVvYu3fvpF5ekmZS\nki/26TdyWCbJ+5IcTnL7Au0vT3Jbks8k+USS5xyrnyRp9fQZc78K2Hac9i8AP1ZVfx94K7BrDHVJ\nkpZh5LBMVX08yZbjtH9iaPImYNPyy5IkLce4z5a5CPjoQo1JdibZm2TvkSNHxvzSkqSjxhbuSf4x\ng3D/pYX6VNWuqpqrqrkNG0Ye7JUkLdFYzpbprsZ3JXDevH+GIEmagGXvuXdXzrseeMW8fw8mSZqQ\nkXvuSa4BzgbWJznI4H9YPg6gqt4DXAo8DXh39/8UHq2quWM/myRpNfQ5W2bHiPbXMPhflZKkKeG1\nZcZgyyUfYcslH5l0GZL0dwx3SWqQ4S5JDTLcJalBhrskNchwXyEeZJU0SRO7nnurDPTJOPp7v+dt\nL5pwJUsz6/Vr+rjnLkkNMtwlqUGGu5ZkLR5TWIvLrNlluGvVGI5q3fA2Punt3XCXjmPSb1BpqQz3\nNWI5IWXArR5/1xoXw13SRKzFD7LVXOY1Ge5rcaOSlsL3yuxak+E+DXzTaFotZdtsfXuexeUz3HVM\nvsGn3yz+vmeh5lmosQ/DXZIaZLj30MonuVaW28nsaXmdGe6L1PLGoOkyTdvaNNWifpq5KuTwhueV\n9SStdc2E+7isxKVXx/Gcx3qO5X6greRlZqf9ErbTXp/6c10eW/Ph7orXsGkYWlgr2+RaWc5p5Zi7\nenPctR2uy/YZ7hO21t9ka/maN7Nev6ab4d6wWQ2PcdU9q8svjcPIcE/yviSHk9y+QHuS/FaSA0lu\nS/K88Ze5fL7RNavcdrUUfQ6oXgW8C3j/Au3nAVu72/OBK7qfkrQmTOOH78g996r6OPDV43TZDry/\nBm4CTk7yPeMqUBrmXuziHe935u+zXeM4FXIjcN/Q9MFu3pfG8NwjuWHOHk+Rk1beqp7nnmQnsBPg\n1FNPXc2XVk9r8cPSD5vFmdXf16zWvVTjCPdDwOah6U3dvMeoql3ALoC5ubkaw2uvSWsxgCUtzjjC\nfTdwcZJrGRxIfaCqVmVIplWrvYex1vZo+vADdG1p8VIcI8M9yTXA2cD6JAeBtwCPA6iq9wB7gPOB\nA8DDwD9bqWK1fAb52jXJdb+cD8tp3Wanta6jRoZ7Ve0Y0V7Az4+tImkZpv0Ntxa0GOSzqPkLh0kr\n6XhX67znbS96TNAtN7QMP/Xl5QdWkecUS1otMxnuhuRj+TtZu9biul/LF5zry2GZBq32hrsar7cW\n3ozjtBaHb9biMh9Pk+G+lJV8rPAwUKSVZyivjCbDXdLqcAdoehnuUqfvHqSBNv38a8BwnyqGhrQw\n3x+LY7iree7FaZxm5UNmJk+F1OxYK6edaTqt5e1vTYf7Wl7x08Z1IY3Xmg735TCMpLbN+nvccJek\nBhnuktQgw12SGuSpkJKA2TnFT/245y5JDTLcJalBhrskNcgx9ynnOKgWw+1FR7nnrqk1618i0dK5\n7pfPcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkN6hXuSbYl2Z/kQJJLjtF+apIbk3wyyW1Jzh9/\nqZKkvkaGe5ITgMuB84DTgR1JTp/X7ZeB66rqucCFwLvHXagkqb8+e+5nAgeq6u6qegS4Ftg+r08B\nT+nuPxX4q/GVKElarD7hvhG4b2j6YDdv2L8HfibJQWAP8C+P9URJdibZm2TvkSNHllCuJKmPcR1Q\n3QFcVVWbgPOBDyR5zHNX1a6qmququQ0bNozppSVJ8/UJ90PA5qHpTd28YRcB1wFU1f8CngCsH0eB\nkqTF6xPutwBbk5yW5EQGB0x3z+tzL3AOQJIfYhDujrtI0oSMDPeqehS4GLgBuJPBWTH7klyW5IKu\n2xuA1yb5NHAN8KqqqpUqWpJ0fL2u515VexgcKB2ed+nQ/TuAs8ZbmiRpqfyGqiQ1yHCXpAYZ7pLU\nIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y\n3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAb1Cvck25Ls\nT3IgySUL9PmpJHck2Zfkv463TEnSYqwb1SHJCcDlwI8DB4FbkuyuqjuG+mwF3gicVVVfS/LdK1Ww\nJGm0PnvuZwIHquruqnoEuBbYPq/Pa4HLq+prAFV1eLxlSpIWo0+4bwTuG5o+2M0b9kzgmUn+IslN\nSbYd64mS7EyyN8neI0eOLK1iSdJI4zqgug7YCpwN7AB+J8nJ8ztV1a6qmququQ0bNozppSVJ8/UJ\n90PA5qHpTd28YQeB3VX1rar6AvA5BmEvSZqAPuF+C7A1yWlJTgQuBHbP6/NhBnvtJFnPYJjm7jHW\nKUlahJHhXlWPAhcDNwB3AtdV1b4klyW5oOt2A/CVJHcANwL/pqq+slJFS5KOb+SpkABVtQfYM2/e\npUP3C3h9d5MkTZjfUJWkBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLU\nIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y\n3CWpQYa7JDXIcJekBhnuktQgw12SGtQr3JNsS7I/yYEklxyn30uTVJK58ZUoSVqskeGe5ATgcuA8\n4HRgR5LTj9HvJOB1wM3jLlKStDh99tzPBA5U1d1V9QhwLbD9GP3eCrwd+Osx1idJWoI+4b4RuG9o\n+mA37+8keR6wuao+crwnSrIzyd4ke48cObLoYiVJ/Sz7gGqS7wDeCbxhVN+q2lVVc1U1t2HDhuW+\ntCRpAX3C/RCweWh6UzfvqJOAZwN/nuQe4AXAbg+qStLk9An3W4CtSU5LciJwIbD7aGNVPVBV66tq\nS1VtAW4CLqiqvStSsSRppJHhXlWPAhcDNwB3AtdV1b4klyW5YKULlCQt3ro+napqD7Bn3rxLF+h7\n9vLLkiQth99QlaQGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12S\nGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalB\nhrskNchwl6QGGe6S1KBe4Z5kW5L9SQ4kueQY7a9PckeS25J8LMn3jr9USVJfI8M9yQnA5cB5wOnA\njiSnz+v2SWCuqs4APgj86rgLlST112fP/UzgQFXdXVWPANcC24c7VNWNVfVwN3kTsGm8ZUqSFqNP\nuG8E7huaPtjNW8hFwEeP1ZBkZ5K9SfYeOXKkf5WSpEUZ6wHVJD8DzAHvOFZ7Ve2qqrmqmtuwYcM4\nX1qSNGRdjz6HgM1D05u6ed8mybnAm4Afq6q/GU95kqSl6LPnfguwNclpSU4ELgR2D3dI8lzgt4EL\nqurw+MuUJC3GyHCvqkeBi4EbgDuB66pqX5LLklzQdXsH8GTg95N8KsnuBZ5OkrQK+gzLUFV7gD3z\n5l06dP/cMdclSVoGv6EqSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwl\nqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIa\nZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDWoV7gn2ZZkf5IDSS45Rvvjk/xe135zki3jLlSS1N/IcE9y\nAnA5cB5wOrAjyenzul0EfK2qfgD4deDt4y5UktRfnz33M4EDVXV3VT0CXAtsn9dnO/C73f0PAuck\nyfjKlCQtRqrq+B2SlwHbquo13fQrgOdX1cVDfW7v+hzspu/q+tw/77l2Aju7yR8E9i+j9vXA/SN7\nzQaXZTq5LNOnleWApS/L91bVhlGd1i3hiZesqnYBu8bxXEn2VtXcOJ5r0lyW6eSyTJ9WlgNWfln6\nDMscAjYPTW/q5h2zT5J1wFOBr4yjQEnS4vUJ91uArUlOS3IicCGwe16f3cAru/svA/6sRo33SJJW\nzMhhmap6NMnFwA3ACcD7qmpfksuAvVW1G3gv8IEkB4CvMvgAWGljGd6ZEi7LdHJZpk8rywErvCwj\nD6hKkmaP31CVpAYZ7pLUoJkM91GXQ5hmSTYnuTHJHUn2JXldN/+UJH+S5PPdz++adK19JDkhySeT\n/GE3fVp3CYoD3SUpTpx0jX0kOTnJB5N8NsmdSf7hDK+Tf9VtW7cnuSbJE2ZlvSR5X5LD3Xdnjs47\n5nrIwG91y3RbkudNrvLHWmBZ3tFtY7cl+VCSk4fa3tgty/4kP7Hc15+5cO95OYRp9ijwhqo6HXgB\n8PNd/ZcAH6uqrcDHuulZ8DrgzqHptwO/3l2K4msMLk0xC34T+KOq+nvAcxgs08ytkyQbgV8A5qrq\n2QxOgriQ2VkvVwHb5s1baD2cB2ztbjuBK1apxr6u4rHL8ifAs6vqDOBzwBsBugy4EHhW95h3d1m3\nZDMX7vS7HMLUqqovVdX/7u4/yCBENvLtl3D4XeAlk6mwvySbgBcBV3bTAV7I4BIUMDvL8VTgRxmc\n9UVVPVJVX2cG10lnHfCd3XdOngh8iRlZL1X1cQZn3A1baD1sB95fAzcBJyf5ntWpdLRjLUtV/XFV\nPdpN3sTge0MwWJZrq+pvquoLwAEGWbdksxjuG4H7hqYPdvNmTnf1zOcCNwNPr6ovdU1fBp4+obIW\n4zeAfwv8bTf9NODrQxvvrKyb04AjwH/uhpiuTPIkZnCdVNUh4NeAexmE+gPArczmejlqofUw61nw\nauCj3f2xL8sshnsTkjwZ+G/AL1bVN4bbui+ATfU5qkleDByuqlsnXcsYrAOeB1xRVc8Fvsm8IZhZ\nWCcA3Xj0dgYfWM8AnsRjhwZm1qysh1GSvInBEO3VK/UasxjufS6HMNWSPI5BsF9dVdd3s//P0T8p\nu5+HJ1VfT2cBFyS5h8HQ2AsZjFuf3A0HwOysm4PAwaq6uZv+IIOwn7V1AnAu8IWqOlJV3wKuZ7Cu\nZnG9HLXQepjJLEjyKuDFwMuHvsk/9mWZxXDvczmEqdWNS78XuLOq3jnUNHwJh1cC/321a1uMqnpj\nVW2qqi0M1sGfVdXLgRsZXIICZmA5AKrqy8B9SX6wm3UOcAcztk469wIvSPLEbls7uiwzt16GLLQe\ndgM/25018wLggaHhm6mUZBuDocwLqurhoabdwIUZ/OOj0xgcJP7LZb1YVc3cDTifwZHmu4A3Tbqe\nRdb+Iwz+rLwN+FR3O5/BePXHgM8DfwqcMulaF7FMZwN/2N3/vm6jPAD8PvD4SdfXcxl+GNjbrZcP\nA981q+sE+A/AZ4HbgQ8Aj5+V9QJcw+BYwbcY/EV10ULrAQiDM+fuAj7D4AyhiS/DiGU5wGBs/eh7\n/z1D/d/ULct+4Lzlvr6XH5CkBs3isIwkaQTDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXo/wGa\ncQviGf/3dwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcwbeKpUCcj-",
        "colab_type": "code",
        "outputId": "df0b184b-208e-4424-a5d6-a76b77b74ee5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(18,10))\n",
        "x = range(len(training_loss))\n",
        "plt.ylim([0.0, 0.009])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Binary Cross Entropy Loss')\n",
        "plt.plot(x, training_loss, label='Train')\n",
        "\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fadd2df3a90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABC8AAAJRCAYAAABlSZ22AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 1296x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCpbs0UEegR9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv results/history results/history-keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bdyeeh2-8b6A",
        "colab_type": "text"
      },
      "source": [
        "## Train pytorch model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N35dufmG8P9p",
        "colab": {}
      },
      "source": [
        "!cp \"/content/drive/My Drive/music_datasets/bach_interim/.\" data/interim -r"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "7ac69da7-b409-4365-b548-13a9d396873b",
        "id": "moBXC5GA8P9v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "source": [
        "from src.train_pytorch import train as train_pytorch\n",
        "\n",
        "train_pytorch('data/interim/samples.npy', 'data/interim/lengths.npy', 2000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-50685de59a3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_pytorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrain_pytorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_pytorch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/interim/samples.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data/interim/lengths.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_double_autoencoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_qty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: train() got an unexpected keyword argument 'use_double_autoencoder'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBGqngG4HTMD",
        "colab_type": "text"
      },
      "source": [
        "## Test model and move results to permanent storage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hvuxlLi0tI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from src.composer import play\n",
        "\n",
        "play(framework='keras', sub_dir_name='e1/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UdN7iHDNgi1",
        "colab_type": "code",
        "outputId": "f8eb861b-6408-4243-a38d-899f643998f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 23375
        }
      },
      "source": [
        "!zip -r results.zip results/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: results/ (stored 0%)\n",
            "  adding: results/feedforward_net_track.csv (deflated 83%)\n",
            "  adding: results/RNN_harmonization_overfit.mid (deflated 59%)\n",
            "  adding: results/retrained_feedforward_snapshot.mid (deflated 64%)\n",
            "  adding: results/RNN_single_overfit.mid (deflated 21%)\n",
            "  adding: results/feedforward_net_track_intervals.mp3 (deflated 5%)\n",
            "  adding: results/loaded_feedforward_generating.csv (deflated 79%)\n",
            "  adding: results/feedforward_net_track_intervals.mid (deflated 65%)\n",
            "  adding: results/RNN_single_early.mid (deflated 15%)\n",
            "  adding: results/feedforward_net_track_intervals.csv (deflated 80%)\n",
            "  adding: results/retrained_feedforward_untrained.csv (deflated 80%)\n",
            "  adding: results/retrained_feedforward_untrained.pth (deflated 11%)\n",
            "  adding: results/.gitkeep (stored 0%)\n",
            "  adding: results/model.png (deflated 17%)\n",
            "  adding: results/feedforward_net_track_intervals_snapshot.csv (deflated 80%)\n",
            "  adding: results/RNN_harmonization_random.mid (deflated 55%)\n",
            "  adding: results/03_predict_2_from_20.mid (deflated 78%)\n",
            "  adding: results/config.txt (deflated 31%)\n",
            "  adding: results/retrained_feedforward_trained.pth (deflated 9%)\n",
            "  adding: results/feedforward_net_track_intervals_snapshot.mid (deflated 66%)\n",
            "  adding: results/loaded_feedforward_generating.mid (deflated 62%)\n",
            "  adding: results/history/ (stored 0%)\n",
            "  adding: results/history/losses.png (deflated 15%)\n",
            "  adding: results/history/e1300/ (stored 0%)\n",
            "  adding: results/history/e1300/random_vectors5.mid (deflated 3%)\n",
            "  adding: results/history/e1300/test/ (stored 0%)\n",
            "  adding: results/history/e1300/test/s0.png (deflated 23%)\n",
            "  adding: results/history/e1300/test/s13.png (deflated 24%)\n",
            "  adding: results/history/e1300/test/s5.png (deflated 23%)\n",
            "  adding: results/history/e1300/test/s12.png (deflated 23%)\n",
            "  adding: results/history/e1300/test/s6.png (deflated 24%)\n",
            "  adding: results/history/e1300/test/s9.png (deflated 23%)\n",
            "  adding: results/history/e1300/test/s2.png (deflated 25%)\n",
            "  adding: results/history/e1300/test/s3.png (deflated 23%)\n",
            "  adding: results/history/e1300/test/s4.png (deflated 24%)\n",
            "  adding: results/history/e1300/test/s1.png (deflated 23%)\n",
            "  adding: results/history/e1300/test/s10.png (deflated 22%)\n",
            "  adding: results/history/e1300/test/s15.png (deflated 24%)\n",
            "  adding: results/history/e1300/test/s7.png (deflated 23%)\n",
            "  adding: results/history/e1300/test/s14.png (deflated 23%)\n",
            "  adding: results/history/e1300/test/s11.png (deflated 25%)\n",
            "  adding: results/history/e1300/test/s8.png (deflated 23%)\n",
            "  adding: results/history/e1300/latent_pca_values.npy (deflated 5%)\n",
            "  adding: results/history/e1300/latent_stds.png (deflated 15%)\n",
            "  adding: results/history/e1300/latent_pca_values.png (deflated 17%)\n",
            "  adding: results/history/e1300/latent_means.npy (deflated 6%)\n",
            "  adding: results/history/e1300/latent_pca_vectors.npy (deflated 4%)\n",
            "  adding: results/history/e1300/test.mid (deflated 4%)\n",
            "  adding: results/history/e1300/random_vectors3.mid (deflated 3%)\n",
            "  adding: results/history/e1300/random_vectors9.mid (deflated 3%)\n",
            "  adding: results/history/e1300/random_vectors4.mid (deflated 3%)\n",
            "  adding: results/history/e1300/random_vectors6.mid (deflated 3%)\n",
            "  adding: results/history/e1300/random_vectors7.mid (deflated 3%)\n",
            "  adding: results/history/e1300/random_vectors8.mid (deflated 3%)\n",
            "  adding: results/history/e1300/latent_stds.npy (deflated 13%)\n",
            "  adding: results/history/e1300/random_vectors1.mid (deflated 3%)\n",
            "  adding: results/history/e1300/random_vectors2.mid (deflated 3%)\n",
            "  adding: results/history/e1300/random_vectors0.mid (deflated 3%)\n",
            "  adding: results/history/e1300/latent_means.png (deflated 13%)\n",
            "  adding: results/history/e140/ (stored 0%)\n",
            "  adding: results/history/e140/random_vectors5.mid (deflated 3%)\n",
            "  adding: results/history/e140/test/ (stored 0%)\n",
            "  adding: results/history/e140/test/s0.png (deflated 18%)\n",
            "  adding: results/history/e140/test/s13.png (deflated 19%)\n",
            "  adding: results/history/e140/test/s5.png (deflated 18%)\n",
            "  adding: results/history/e140/test/s12.png (deflated 21%)\n",
            "  adding: results/history/e140/test/s6.png (deflated 21%)\n",
            "  adding: results/history/e140/test/s9.png (deflated 22%)\n",
            "  adding: results/history/e140/test/s2.png (deflated 19%)\n",
            "  adding: results/history/e140/test/s3.png (deflated 18%)\n",
            "  adding: results/history/e140/test/s4.png (deflated 18%)\n",
            "  adding: results/history/e140/test/s1.png (deflated 20%)\n",
            "  adding: results/history/e140/test/s10.png (deflated 21%)\n",
            "  adding: results/history/e140/test/s15.png (deflated 21%)\n",
            "  adding: results/history/e140/test/s7.png (deflated 19%)\n",
            "  adding: results/history/e140/test/s14.png (deflated 22%)\n",
            "  adding: results/history/e140/test/s11.png (deflated 17%)\n",
            "  adding: results/history/e140/test/s8.png (deflated 17%)\n",
            "  adding: results/history/e140/latent_pca_values.npy (deflated 5%)\n",
            "  adding: results/history/e140/latent_stds.png (deflated 15%)\n",
            "  adding: results/history/e140/latent_pca_values.png (deflated 19%)\n",
            "  adding: results/history/e140/latent_means.npy (deflated 6%)\n",
            "  adding: results/history/e140/latent_pca_vectors.npy (deflated 4%)\n",
            "  adding: results/history/e140/test.mid (deflated 3%)\n",
            "  adding: results/history/e140/random_vectors3.mid (deflated 3%)\n",
            "  adding: results/history/e140/random_vectors9.mid (deflated 3%)\n",
            "  adding: results/history/e140/random_vectors4.mid (deflated 3%)\n",
            "  adding: results/history/e140/random_vectors6.mid (deflated 3%)\n",
            "  adding: results/history/e140/random_vectors7.mid (deflated 3%)\n",
            "  adding: results/history/e140/random_vectors8.mid (deflated 3%)\n",
            "  adding: results/history/e140/latent_stds.npy (deflated 14%)\n",
            "  adding: results/history/e140/random_vectors1.mid (deflated 3%)\n",
            "  adding: results/history/e140/random_vectors2.mid (deflated 3%)\n",
            "  adding: results/history/e140/random_vectors0.mid (deflated 3%)\n",
            "  adding: results/history/e140/latent_means.png (deflated 13%)\n",
            "  adding: results/history/e900/ (stored 0%)\n",
            "  adding: results/history/e900/random_vectors5.mid (deflated 3%)\n",
            "  adding: results/history/e900/test/ (stored 0%)\n",
            "  adding: results/history/e900/test/s0.png (deflated 22%)\n",
            "  adding: results/history/e900/test/s13.png (deflated 22%)\n",
            "  adding: results/history/e900/test/s5.png (deflated 21%)\n",
            "  adding: results/history/e900/test/s12.png (deflated 22%)\n",
            "  adding: results/history/e900/test/s6.png (deflated 21%)\n",
            "  adding: results/history/e900/test/s9.png (deflated 21%)\n",
            "  adding: results/history/e900/test/s2.png (deflated 23%)\n",
            "  adding: results/history/e900/test/s3.png (deflated 22%)\n",
            "  adding: results/history/e900/test/s4.png (deflated 22%)\n",
            "  adding: results/history/e900/test/s1.png (deflated 22%)\n",
            "  adding: results/history/e900/test/s10.png (deflated 22%)\n",
            "  adding: results/history/e900/test/s15.png (deflated 22%)\n",
            "  adding: results/history/e900/test/s7.png (deflated 21%)\n",
            "  adding: results/history/e900/test/s14.png (deflated 21%)\n",
            "  adding: results/history/e900/test/s11.png (deflated 27%)\n",
            "  adding: results/history/e900/test/s8.png (deflated 21%)\n",
            "  adding: results/history/e900/latent_pca_values.npy (deflated 5%)\n",
            "  adding: results/history/e900/latent_stds.png (deflated 15%)\n",
            "  adding: results/history/e900/latent_pca_values.png (deflated 17%)\n",
            "  adding: results/history/e900/latent_means.npy (deflated 6%)\n",
            "  adding: results/history/e900/latent_pca_vectors.npy (deflated 4%)\n",
            "  adding: results/history/e900/test.mid (deflated 8%)\n",
            "  adding: results/history/e900/random_vectors3.mid (deflated 3%)\n",
            "  adding: results/history/e900/random_vectors9.mid (deflated 3%)\n",
            "  adding: results/history/e900/random_vectors4.mid (deflated 3%)\n",
            "  adding: results/history/e900/random_vectors6.mid (deflated 3%)\n",
            "  adding: results/history/e900/random_vectors7.mid (deflated 3%)\n",
            "  adding: results/history/e900/random_vectors8.mid (deflated 3%)\n",
            "  adding: results/history/e900/latent_stds.npy (deflated 13%)\n",
            "  adding: results/history/e900/random_vectors1.mid (deflated 3%)\n",
            "  adding: results/history/e900/random_vectors2.mid (deflated 3%)\n",
            "  adding: results/history/e900/random_vectors0.mid (deflated 3%)\n",
            "  adding: results/history/e900/latent_means.png (deflated 13%)\n",
            "  adding: results/history/e1700/ (stored 0%)\n",
            "  adding: results/history/e1700/random_vectors5.mid (deflated 3%)\n",
            "  adding: results/history/e1700/test/ (stored 0%)\n",
            "  adding: results/history/e1700/test/s0.png (deflated 22%)\n",
            "  adding: results/history/e1700/test/s13.png (deflated 23%)\n",
            "  adding: results/history/e1700/test/s5.png (deflated 22%)\n",
            "  adding: results/history/e1700/test/s12.png (deflated 23%)\n",
            "  adding: results/history/e1700/test/s6.png (deflated 21%)\n",
            "  adding: results/history/e1700/test/s9.png (deflated 28%)\n",
            "  adding: results/history/e1700/test/s2.png (deflated 22%)\n",
            "  adding: results/history/e1700/test/s3.png (deflated 22%)\n",
            "  adding: results/history/e1700/test/s4.png (deflated 22%)\n",
            "  adding: results/history/e1700/test/s1.png (deflated 24%)\n",
            "  adding: results/history/e1700/test/s10.png (deflated 22%)\n",
            "  adding: results/history/e1700/test/s15.png (deflated 21%)\n",
            "  adding: results/history/e1700/test/s7.png (deflated 24%)\n",
            "  adding: results/history/e1700/test/s14.png (deflated 22%)\n",
            "  adding: results/history/e1700/test/s11.png (deflated 23%)\n",
            "  adding: results/history/e1700/test/s8.png (deflated 22%)\n",
            "  adding: results/history/e1700/latent_pca_values.npy (deflated 4%)\n",
            "  adding: results/history/e1700/latent_stds.png (deflated 15%)\n",
            "  adding: results/history/e1700/latent_pca_values.png (deflated 17%)\n",
            "  adding: results/history/e1700/latent_means.npy (deflated 6%)\n",
            "  adding: results/history/e1700/latent_pca_vectors.npy (deflated 4%)\n",
            "  adding: results/history/e1700/test.mid (deflated 71%)\n",
            "  adding: results/history/e1700/random_vectors3.mid (deflated 3%)\n",
            "  adding: results/history/e1700/random_vectors9.mid (deflated 3%)\n",
            "  adding: results/history/e1700/random_vectors4.mid (deflated 3%)\n",
            "  adding: results/history/e1700/random_vectors6.mid (deflated 3%)\n",
            "  adding: results/history/e1700/random_vectors7.mid (deflated 3%)\n",
            "  adding: results/history/e1700/random_vectors8.mid (deflated 3%)\n",
            "  adding: results/history/e1700/latent_stds.npy (deflated 13%)\n",
            "  adding: results/history/e1700/random_vectors1.mid (deflated 3%)\n",
            "  adding: results/history/e1700/random_vectors2.mid (deflated 3%)\n",
            "  adding: results/history/e1700/random_vectors0.mid (deflated 3%)\n",
            "  adding: results/history/e1700/latent_means.png (deflated 13%)\n",
            "  adding: results/history/e1500/ (stored 0%)\n",
            "  adding: results/history/e1500/random_vectors5.mid (deflated 3%)\n",
            "  adding: results/history/e1500/test/ (stored 0%)\n",
            "  adding: results/history/e1500/test/s0.png (deflated 28%)\n",
            "  adding: results/history/e1500/test/s13.png (deflated 21%)\n",
            "  adding: results/history/e1500/test/s5.png (deflated 21%)\n",
            "  adding: results/history/e1500/test/s12.png (deflated 22%)\n",
            "  adding: results/history/e1500/test/s6.png (deflated 23%)\n",
            "  adding: results/history/e1500/test/s9.png (deflated 22%)\n",
            "  adding: results/history/e1500/test/s2.png (deflated 22%)\n",
            "  adding: results/history/e1500/test/s3.png (deflated 22%)\n",
            "  adding: results/history/e1500/test/s4.png (deflated 22%)\n",
            "  adding: results/history/e1500/test/s1.png (deflated 21%)\n",
            "  adding: results/history/e1500/test/s10.png (deflated 22%)\n",
            "  adding: results/history/e1500/test/s15.png (deflated 21%)\n",
            "  adding: results/history/e1500/test/s7.png (deflated 22%)\n",
            "  adding: results/history/e1500/test/s14.png (deflated 21%)\n",
            "  adding: results/history/e1500/test/s11.png (deflated 21%)\n",
            "  adding: results/history/e1500/test/s8.png (deflated 22%)\n",
            "  adding: results/history/e1500/latent_pca_values.npy (deflated 4%)\n",
            "  adding: results/history/e1500/latent_stds.png (deflated 15%)\n",
            "  adding: results/history/e1500/latent_pca_values.png (deflated 16%)\n",
            "  adding: results/history/e1500/latent_means.npy (deflated 6%)\n",
            "  adding: results/history/e1500/latent_pca_vectors.npy (deflated 4%)\n",
            "  adding: results/history/e1500/test.mid (deflated 65%)\n",
            "  adding: results/history/e1500/random_vectors3.mid (deflated 3%)\n",
            "  adding: results/history/e1500/random_vectors9.mid (deflated 3%)\n",
            "  adding: results/history/e1500/random_vectors4.mid (deflated 3%)\n",
            "  adding: results/history/e1500/random_vectors6.mid (deflated 3%)\n",
            "  adding: results/history/e1500/random_vectors7.mid (deflated 3%)\n",
            "  adding: results/history/e1500/random_vectors8.mid (deflated 3%)\n",
            "  adding: results/history/e1500/latent_stds.npy (deflated 13%)\n",
            "  adding: results/history/e1500/random_vectors1.mid (deflated 3%)\n",
            "  adding: results/history/e1500/random_vectors2.mid (deflated 3%)\n",
            "  adding: results/history/e1500/random_vectors0.mid (deflated 3%)\n",
            "  adding: results/history/e1500/latent_means.png (deflated 13%)\n",
            "  adding: results/history/e80/ (stored 0%)\n",
            "  adding: results/history/e80/random_vectors5.mid (deflated 3%)\n",
            "  adding: results/history/e80/test/ (stored 0%)\n",
            "  adding: results/history/e80/test/s0.png (deflated 24%)\n",
            "  adding: results/history/e80/test/s13.png (deflated 23%)\n",
            "  adding: results/history/e80/test/s5.png (deflated 23%)\n",
            "  adding: results/history/e80/test/s12.png (deflated 24%)\n",
            "  adding: results/history/e80/test/s6.png (deflated 22%)\n",
            "  adding: results/history/e80/test/s9.png (deflated 23%)\n",
            "  adding: results/history/e80/test/s2.png (deflated 22%)\n",
            "  adding: results/history/e80/test/s3.png (deflated 25%)\n",
            "  adding: results/history/e80/test/s4.png (deflated 23%)\n",
            "  adding: results/history/e80/test/s1.png (deflated 23%)\n",
            "  adding: results/history/e80/test/s10.png (deflated 23%)\n",
            "  adding: results/history/e80/test/s15.png (deflated 24%)\n",
            "  adding: results/history/e80/test/s7.png (deflated 23%)\n",
            "  adding: results/history/e80/test/s14.png (deflated 24%)\n",
            "  adding: results/history/e80/test/s11.png (deflated 23%)\n",
            "  adding: results/history/e80/test/s8.png (deflated 22%)\n",
            "  adding: results/history/e80/latent_pca_values.npy (deflated 5%)\n",
            "  adding: results/history/e80/latent_stds.png (deflated 17%)\n",
            "  adding: results/history/e80/latent_pca_values.png (deflated 19%)\n",
            "  adding: results/history/e80/latent_means.npy (deflated 7%)\n",
            "  adding: results/history/e80/latent_pca_vectors.npy (deflated 4%)\n",
            "  adding: results/history/e80/test.mid (deflated 3%)\n",
            "  adding: results/history/e80/random_vectors3.mid (deflated 3%)\n",
            "  adding: results/history/e80/random_vectors9.mid (deflated 3%)\n",
            "  adding: results/history/e80/random_vectors4.mid (deflated 3%)\n",
            "  adding: results/history/e80/random_vectors6.mid (deflated 3%)\n",
            "  adding: results/history/e80/random_vectors7.mid (deflated 3%)\n",
            "  adding: results/history/e80/random_vectors8.mid (deflated 3%)\n",
            "  adding: results/history/e80/latent_stds.npy (deflated 15%)\n",
            "  adding: results/history/e80/random_vectors1.mid (deflated 3%)\n",
            "  adding: results/history/e80/random_vectors2.mid (deflated 3%)\n",
            "  adding: results/history/e80/random_vectors0.mid (deflated 3%)\n",
            "  adding: results/history/e80/latent_means.png (deflated 13%)\n",
            "  adding: results/history/e180/ (stored 0%)\n",
            "  adding: results/history/e180/random_vectors5.mid (deflated 3%)\n",
            "  adding: results/history/e180/test/ (stored 0%)\n",
            "  adding: results/history/e180/test/s0.png (deflated 22%)\n",
            "  adding: results/history/e180/test/s13.png (deflated 21%)\n",
            "  adding: results/history/e180/test/s5.png (deflated 19%)\n",
            "  adding: results/history/e180/test/s12.png (deflated 21%)\n",
            "  adding: results/history/e180/test/s6.png (deflated 21%)\n",
            "  adding: results/history/e180/test/s9.png (deflated 21%)\n",
            "  adding: results/history/e180/test/s2.png (deflated 20%)\n",
            "  adding: results/history/e180/test/s3.png (deflated 20%)\n",
            "  adding: results/history/e180/test/s4.png (deflated 21%)\n",
            "  adding: results/history/e180/test/s1.png (deflated 18%)\n",
            "  adding: results/history/e180/test/s10.png (deflated 20%)\n",
            "  adding: results/history/e180/test/s15.png (deflated 20%)\n",
            "  adding: results/history/e180/test/s7.png (deflated 20%)\n",
            "  adding: results/history/e180/test/s14.png (deflated 21%)\n",
            "  adding: results/history/e180/test/s11.png (deflated 20%)\n",
            "  adding: results/history/e180/test/s8.png (deflated 21%)\n",
            "  adding: results/history/e180/latent_pca_values.npy (deflated 5%)\n",
            "  adding: results/history/e180/latent_stds.png (deflated 16%)\n",
            "  adding: results/history/e180/latent_pca_values.png (deflated 18%)\n",
            "  adding: results/history/e180/latent_means.npy (deflated 6%)\n",
            "  adding: results/history/e180/latent_pca_vectors.npy (deflated 4%)\n",
            "  adding: results/history/e180/test.mid (deflated 3%)\n",
            "  adding: results/history/e180/random_vectors3.mid (deflated 3%)\n",
            "  adding: results/history/e180/random_vectors9.mid (deflated 3%)\n",
            "  adding: results/history/e180/random_vectors4.mid (deflated 3%)\n",
            "  adding: results/history/e180/random_vectors6.mid (deflated 3%)\n",
            "  adding: results/history/e180/random_vectors7.mid (deflated 3%)\n",
            "  adding: results/history/e180/random_vectors8.mid (deflated 3%)\n",
            "  adding: results/history/e180/latent_stds.npy (deflated 14%)\n",
            "  adding: results/history/e180/random_vectors1.mid (deflated 3%)\n",
            "  adding: results/history/e180/random_vectors2.mid (deflated 3%)\n",
            "  adding: results/history/e180/random_vectors0.mid (deflated 3%)\n",
            "  adding: results/history/e180/latent_means.png (deflated 14%)\n",
            "  adding: results/history/e30/ (stored 0%)\n",
            "  adding: results/history/e30/random_vectors5.mid (deflated 3%)\n",
            "  adding: results/history/e30/test/ (stored 0%)\n",
            "  adding: results/history/e30/test/s0.png (deflated 8%)\n",
            "  adding: results/history/e30/test/s13.png (deflated 8%)\n",
            "  adding: results/history/e30/test/s5.png (deflated 8%)\n",
            "  adding: results/history/e30/test/s12.png (deflated 8%)\n",
            "  adding: results/history/e30/test/s6.png (deflated 8%)\n",
            "  adding: results/history/e30/test/s9.png (deflated 9%)\n",
            "  adding: results/history/e30/test/s2.png (deflated 8%)\n",
            "  adding: results/history/e30/test/s3.png (deflated 8%)\n",
            "  adding: results/history/e30/test/s4.png (deflated 9%)\n",
            "  adding: results/history/e30/test/s1.png (deflated 9%)\n",
            "  adding: results/history/e30/test/s10.png (deflated 9%)\n",
            "  adding: results/history/e30/test/s15.png (deflated 9%)\n",
            "  adding: results/history/e30/test/s7.png (deflated 9%)\n",
            "  adding: results/history/e30/test/s14.png (deflated 8%)\n",
            "  adding: results/history/e30/test/s11.png (deflated 9%)\n",
            "  adding: results/history/e30/test/s8.png (deflated 9%)\n",
            "  adding: results/history/e30/latent_pca_values.npy (deflated 5%)\n",
            "  adding: results/history/e30/latent_stds.png (deflated 15%)\n",
            "  adding: results/history/e30/latent_pca_values.png (deflated 20%)\n",
            "  adding: results/history/e30/latent_means.npy (deflated 7%)\n",
            "  adding: results/history/e30/latent_pca_vectors.npy (deflated 4%)\n",
            "  adding: results/history/e30/test.mid (deflated 3%)\n",
            "  adding: results/history/e30/random_vectors3.mid (deflated 3%)\n",
            "  adding: results/history/e30/random_vectors9.mid (deflated 3%)\n",
            "  adding: results/history/e30/random_vectors4.mid (deflated 3%)\n",
            "  adding: results/history/e30/random_vectors6.mid (deflated 3%)\n",
            "  adding: results/history/e30/random_vectors7.mid (deflated 3%)\n",
            "  adding: results/history/e30/random_vectors8.mid (deflated 3%)\n",
            "  adding: results/history/e30/latent_stds.npy (deflated 14%)\n",
            "  adding: results/history/e30/random_vectors1.mid (deflated 3%)\n",
            "  adding: results/history/e30/random_vectors2.mid (deflated 3%)\n",
            "  adding: results/history/e30/random_vectors0.mid (deflated 3%)\n",
            "  adding: results/history/e30/latent_means.png (deflated 13%)\n",
            "  adding: results/history/e90/ (stored 0%)\n",
            "  adding: results/history/e90/random_vectors5.mid (deflated 3%)\n",
            "  adding: results/history/e90/test/ (stored 0%)\n",
            "  adding: results/history/e90/test/s0.png (deflated 27%)\n",
            "  adding: results/history/e90/test/s13.png (deflated 27%)\n",
            "  adding: results/history/e90/test/s5.png (deflated 27%)\n",
            "  adding: results/history/e90/test/s12.png (deflated 27%)\n",
            "  adding: results/history/e90/test/s6.png (deflated 27%)\n",
            "  adding: results/history/e90/test/s9.png (deflated 26%)\n",
            "  adding: results/history/e90/test/s2.png (deflated 27%)\n",
            "  adding: results/history/e90/test/s3.png (deflated 25%)\n",
            "  adding: results/history/e90/test/s4.png (deflated 25%)\n",
            "  adding: results/history/e90/test/s1.png (deflated 25%)\n",
            "  adding: results/history/e90/test/s10.png (deflated 26%)\n",
            "  adding: results/history/e90/test/s15.png (deflated 26%)\n",
            "  adding: results/history/e90/test/s7.png (deflated 25%)\n",
            "  adding: results/history/e90/test/s14.png (deflated 26%)\n",
            "  adding: results/history/e90/test/s11.png (deflated 25%)\n",
            "  adding: results/history/e90/test/s8.png (deflated 28%)\n",
            "  adding: results/history/e90/latent_pca_values.npy (deflated 5%)\n",
            "  adding: results/history/e90/latent_stds.png (deflated 16%)\n",
            "  adding: results/history/e90/latent_pca_values.png (deflated 19%)\n",
            "  adding: results/history/e90/latent_means.npy (deflated 7%)\n",
            "  adding: results/history/e90/latent_pca_vectors.npy (deflated 4%)\n",
            "  adding: results/history/e90/test.mid (deflated 3%)\n",
            "  adding: results/history/e90/random_vectors3.mid (deflated 3%)\n",
            "  adding: results/history/e90/random_vectors9.mid (deflated 3%)\n",
            "  adding: results/history/e90/random_vectors4.mid (deflated 3%)\n",
            "  adding: results/history/e90/random_vectors6.mid (deflated 3%)\n",
            "  adding: results/history/e90/random_vectors7.mid (deflated 3%)\n",
            "  adding: results/history/e90/random_vectors8.mid (deflated 3%)\n",
            "  adding: results/history/e90/latent_stds.npy (deflated 14%)\n",
            "  adding: results/history/e90/random_vectors1.mid (deflated 3%)\n",
            "  adding: results/history/e90/random_vectors2.mid (deflated 3%)\n",
            "  adding: results/history/e90/random_vectors0.mid (deflated 3%)\n",
            "  adding: results/history/e90/latent_means.png (deflated 13%)\n",
            "  adding: results/history/e120/ (stored 0%)\n",
            "  adding: results/history/e120/random_vectors5.mid (deflated 3%)\n",
            "  adding: results/history/e120/test/ (stored 0%)\n",
            "  adding: results/history/e120/test/s0.png (deflated 20%)\n",
            "  adding: results/history/e120/test/s13.png (deflated 19%)\n",
            "  adding: results/history/e120/test/s5.png (deflated 19%)\n",
            "  adding: results/history/e120/test/s12.png (deflated 19%)\n",
            "  adding: results/history/e120/test/s6.png (deflated 20%)\n",
            "  adding: results/history/e120/test/s9.png (deflated 19%)\n",
            "  adding: results/history/e120/test/s2.png (deflated 21%)\n",
            "  adding: results/history/e120/test/s3.png (deflated 19%)\n",
            "  adding: results/history/e120/test/s4.png (deflated 20%)\n",
            "  adding: results/history/e120/test/s1.png (deflated 19%)\n",
            "  adding: results/history/e120/test/s10.png (deflated 21%)\n",
            "  adding: results/history/e120/test/s15.png (deflated 20%)\n",
            "  adding: results/history/e120/test/s7.png (deflated 20%)\n",
            "  adding: results/history/e120/test/s14.png (deflated 20%)\n",
            "  adding: results/history/e120/test/s11.png (deflated 20%)\n",
            "  adding: results/history/e120/test/s8.png (deflated 19%)\n",
            "  adding: results/history/e120/latent_pca_values.npy (deflated 5%)\n",
            "  adding: results/history/e120/latent_stds.png (deflated 18%)\n",
            "  adding: results/history/e120/latent_pca_values.png (deflated 19%)\n",
            "  adding: results/history/e120/latent_means.npy (deflated 6%)\n",
            "  adding: results/history/e120/latent_pca_vectors.npy (deflated 4%)\n",
            "  adding: results/history/e120/test.mid (deflated 3%)\n",
            "  adding: results/history/e120/random_vectors3.mid (deflated 3%)\n",
            "  adding: results/history/e120/random_vectors9.mid (deflated 3%)\n",
            "  adding: results/history/e120/random_vectors4.mid (deflated 3%)\n",
            "  adding: results/history/e120/random_vectors6.mid (deflated 3%)\n",
            "  adding: results/history/e120/random_vectors7.mid (deflated 3%)\n",
            "  adding: results/history/e120/random_vectors8.mid (deflated 3%)\n",
            "  adding: results/history/e120/latent_stds.npy (deflated 15%)\n",
            "  adding: results/history/e120/random_vectors1.mid (deflated 3%)\n",
            "  adding: results/history/e120/random_vectors2.mid (deflated 3%)\n",
            "  adding: results/history/e120/random_vectors0.mid (deflated 3%)\n",
            "  adding: results/history/e120/latent_means.png (deflated 14%)\n",
            "  adding: results/history/e20/ (stored 0%)\n",
            "  adding: results/history/e20/random_vectors5.mid (deflated 3%)\n",
            "  adding: results/history/e20/test/ (stored 0%)\n",
            "  adding: results/history/e20/test/s0.png (deflated 8%)\n",
            "  adding: results/history/e20/test/s13.png (deflated 8%)\n",
            "  adding: results/history/e20/test/s5.png (deflated 9%)\n",
            "  adding: results/history/e20/test/s12.png (deflated 9%)\n",
            "  adding: results/history/e20/test/s6.png (deflated 9%)\n",
            "  adding: results/history/e20/test/s9.png (deflated 8%)\n",
            "  adding: results/history/e20/test/s2.png (deflated 8%)\n",
            "  adding: results/history/e20/test/s3.png (deflated 9%)\n",
            "  adding: results/history/e20/test/s4.png (deflated 9%)\n",
            "  adding: results/history/e20/test/s1.png (deflated 8%)\n",
            "  adding: results/history/e20/test/s10.png (deflated 9%)\n",
            "  adding: results/history/e20/test/s15.png (deflated 8%)\n",
            "  adding: results/history/e20/test/s7.png (deflated 9%)\n",
            "  adding: results/history/e20/test/s14.png (deflated 8%)\n",
            "  adding: results/history/e20/test/s11.png (deflated 8%)\n",
            "  adding: results/history/e20/test/s8.png (deflated 9%)\n",
            "  adding: results/history/e20/latent_pca_values.npy (deflated 5%)\n",
            "  adding: results/history/e20/latent_stds.png (deflated 15%)\n",
            "  adding: results/history/e20/latent_pca_values.png (deflated 18%)\n",
            "  adding: results/history/e20/latent_means.npy (deflated 5%)\n",
            "  adding: results/history/e20/latent_pca_vectors.npy (deflated 4%)\n",
            "  adding: results/history/e20/test.mid (deflated 3%)\n",
            "  adding: results/history/e20/random_vectors3.mid (deflated 3%)\n",
            "  adding: results/history/e20/random_vectors9.mid (deflated 3%)\n",
            "  adding: results/history/e20/random_vectors4.mid (deflated 3%)\n",
            "  adding: results/history/e20/random_vectors6.mid (deflated 3%)\n",
            "  adding: results/history/e20/random_vectors7.mid (deflated 3%)\n",
            "  adding: results/history/e20/random_vectors8.mid (deflated 3%)\n",
            "  adding: results/history/e20/latent_stds.npy (deflated 14%)\n",
            "  adding: results/history/e20/random_vectors1.mid (deflated 3%)\n",
            "  adding: results/history/e20/random_vectors2.mid (deflated 3%)\n",
            "  adding: results/history/e20/random_vectors0.mid (deflated 3%)\n",
            "  adding: results/history/e20/latent_means.png (deflated 14%)\n",
            "  adding: results/history/e50/ (stored 0%)\n",
            "  adding: results/history/e50/random_vectors5.mid (deflated 3%)\n",
            "  adding: results/history/e50/test/ (stored 0%)\n",
            "  adding: results/history/e50/test/s0.png (deflated 22%)\n",
            "  adding: results/history/e50/test/s13.png (deflated 24%)\n",
            "  adding: results/history/e50/test/s5.png (deflated 22%)\n",
            "  adding: results/history/e50/test/s12.png (deflated 23%)\n",
            "  adding: results/history/e50/test/s6.png (deflated 22%)\n",
            "  adding: results/history/e50/test/s9.png (deflated 24%)\n",
            "  adding: results/history/e50/test/s2.png (deflated 21%)\n",
            "  adding: results/history/e50/test/s3.png (deflated 22%)\n",
            "  adding: results/history/e50/test/s4.png (deflated 22%)\n",
            "  adding: results/history/e50/test/s1.png (deflated 22%)\n",
            "  adding: results/history/e50/test/s10.png (deflated 23%)\n",
            "  adding: results/history/e50/test/s15.png (deflated 24%)\n",
            "  adding: results/history/e50/test/s7.png (deflated 22%)\n",
            "  adding: results/history/e50/test/s14.png (deflated 22%)\n",
            "  adding: results/history/e50/test/s11.png (deflated 22%)\n",
            "  adding: results/history/e50/test/s8.png (deflated 22%)\n",
            "  adding: results/history/e50/latent_pca_values.npy (deflated 5%)\n",
            "  adding: results/history/e50/latent_stds.png (deflated 15%)\n",
            "  adding: results/history/e50/latent_pca_values.png (deflated 19%)\n",
            "  adding: results/history/e50/latent_means.npy (deflated 6%)\n",
            "  adding: results/history/e50/latent_pca_vectors.npy (deflated 4%)\n",
            "  adding: results/history/e50/test.mid (deflated 3%)\n",
            "  adding: results/history/e50/random_vectors3.mid (deflated 3%)\n",
            "  adding: results/history/e50/random_vectors9.mid (deflated 3%)\n",
            "  adding: results/history/e50/random_vectors4.mid (deflated 3%)\n",
            "  adding: results/history/e50/random_vectors6.mid (deflated 3%)\n",
            "  adding: results/history/e50/random_vectors7.mid (deflated 3%)\n",
            "  adding: results/history/e50/random_vectors8.mid (deflated 3%)\n",
            "  adding: results/history/e50/latent_stds.npy (deflated 13%)\n",
            "  adding: results/history/e50/random_vectors1.mid (deflated 3%)\n",
            "  adding: results/history/e50/random_vectors2.mid (deflated 3%)\n",
            "  adding: results/history/e50/random_vectors0.mid (deflated 3%)\n",
            "  adding: results/history/e50/latent_means.png (deflated 13%)\n",
            "  adding: results/history/e300/ (stored 0%)\n",
            "  adding: results/history/e300/random_vectors5.mid (deflated 3%)\n",
            "  adding: results/history/e300/test/ (stored 0%)\n",
            "  adding: results/history/e300/test/s0.png (deflated 20%)\n",
            "  adding: results/history/e300/test/s13.png (deflated 21%)\n",
            "  adding: results/history/e300/test/s5.png (deflated 21%)\n",
            "  adding: results/history/e300/test/s12.png (deflated 20%)\n",
            "  adding: results/history/e300/test/s6.png (deflated 21%)\n",
            "  adding: results/history/e300/test/s9.png (deflated 20%)\n",
            "  adding: results/history/e300/test/s2.png (deflated 21%)\n",
            "  adding: results/history/e300/test/s3.png (deflated 20%)\n",
            "  adding: results/history/e300/test/s4.png (deflated 20%)\n",
            "  adding: results/history/e300/test/s1.png (deflated 20%)\n",
            "  adding: results/history/e300/test/s10.png (deflated 21%)\n",
            "  adding: results/history/e300/test/s15.png (deflated 19%)\n",
            "  adding: results/history/e300/test/s7.png (deflated 19%)\n",
            "  adding: results/history/e300/test/s14.png (deflated 19%)\n",
            "  adding: results/history/e300/test/s11.png (deflated 20%)\n",
            "  adding: results/history/e300/test/s8.png (deflated 20%)\n",
            "  adding: results/history/e300/latent_pca_values.npy (deflated 5%)\n",
            "  adding: results/history/e300/latent_stds.png (deflated 16%)\n",
            "  adding: results/history/e300/latent_pca_values.png (deflated 19%)\n",
            "  adding: results/history/e300/latent_means.npy (deflated 5%)\n",
            "  adding: results/history/e300/latent_pca_vectors.npy (deflated 4%)\n",
            "  adding: results/history/e300/test.mid (deflated 3%)\n",
            "  adding: results/history/e300/random_vectors3.mid (deflated 3%)\n",
            "  adding: results/history/e300/random_vectors9.mid (deflated 3%)\n",
            "  adding: results/history/e300/random_vectors4.mid (deflated 3%)\n",
            "  adding: results/history/e300/random_vectors6.mid (deflated 3%)\n",
            "  adding: results/history/e300/random_vectors7.mid (deflated 3%)\n",
            "  adding: results/history/e300/random_vectors8.mid (deflated 3%)\n",
            "  adding: results/history/e300/latent_stds.npy (deflated 13%)\n",
            "  adding: results/history/e300/random_vectors1.mid (deflated 3%)\n",
            "  adding: results/history/e300/random_vectors2.mid (deflated 3%)\n",
            "  adding: results/history/e300/random_vectors0.mid (deflated 3%)\n",
            "  adding: results/history/e300/latent_means.png (deflated 13%)\n",
            "  adding: results/history/e1900/ (stored 0%)\n",
            "  adding: results/history/e1900/random_vectors5.mid (deflated 3%)\n",
            "  adding: results/history/e1900/test/ (stored 0%)\n",
            "  adding: results/history/e1900/test/s0.png (deflated 23%)\n",
            "  adding: results/history/e1900/test/s13.png (deflated 23%)\n",
            "  adding: results/history/e1900/test/s5.png (deflated 23%)\n",
            "  adding: results/history/e1900/test/s12.png (deflated 24%)\n",
            "  adding: results/history/e1900/test/s6.png (deflated 25%)\n",
            "  adding: results/history/e1900/test/s9.png (deflated 25%)\n",
            "  adding: results/history/e1900/test/s2.png (deflated 24%)\n",
            "  adding: results/history/e1900/test/s3.png (deflated 22%)\n",
            "  adding: results/history/e1900/test/s4.png (deflated 24%)\n",
            "  adding: results/history/e1900/test/s1.png (deflated 23%)\n",
            "  adding: results/history/e1900/test/s10.png (deflated 23%)\n",
            "  adding: results/history/e1900/test/s15.png (deflated 24%)\n",
            "  adding: results/history/e1900/test/s7.png (deflated 24%)\n",
            "  adding: results/history/e1900/test/s14.png (deflated 23%)\n",
            "  adding: results/history/e1900/test/s11.png (deflated 25%)\n",
            "  adding: results/history/e1900/test/s8.png (deflated 24%)\n",
            "  adding: results/history/e1900/latent_pca_values.npy (deflated 4%)\n",
            "  adding: results/history/e1900/latent_stds.png (deflated 15%)\n",
            "  adding: results/history/e1900/latent_pca_values.png (deflated 15%)\n",
            "  adding: results/history/e1900/latent_means.npy (deflated 5%)\n",
            "  adding: results/history/e1900/latent_pca_vectors.npy (deflated 4%)\n",
            "  adding: results/history/e1900/test.mid (deflated 42%)\n",
            "  adding: results/history/e1900/random_vectors3.mid (deflated 3%)\n",
            "  adding: results/history/e1900/random_vectors9.mid (deflated 3%)\n",
            "  adding: results/history/e1900/random_vectors4.mid (deflated 3%)\n",
            "  adding: results/history/e1900/random_vectors6.mid (deflated 3%)\n",
            "  adding: results/history/e1900/random_vectors7.mid (deflated 3%)\n",
            "  adding: results/history/e1900/random_vectors8.mid (deflated 3%)\n",
            "  adding: results/history/e1900/latent_stds.npy (deflated 13%)\n",
            "  adding: results/history/e1900/random_vectors1.mid (deflated 3%)\n",
            "  adding: results/history/e1900/random_vectors2.mid (deflated 3%)\n",
            "  adding: results/history/e1900/random_vectors0.mid (deflated 3%)\n",
            "  adding: results/history/e1900/latent_means.png (deflated 13%)\n",
            "  adding: results/history/model.h5 (deflated 32%)\n",
            "  adding: results/history/e40/ (stored 0%)\n",
            "  adding: results/history/e40/random_vectors5.mid (deflated 3%)\n",
            "  adding: results/history/e40/test/ (stored 0%)\n",
            "  adding: results/history/e40/test/s0.png (deflated 18%)\n",
            "  adding: results/history/e40/test/s13.png (deflated 19%)\n",
            "  adding: results/history/e40/test/s5.png (deflated 21%)\n",
            "  adding: results/history/e40/test/s12.png (deflated 21%)\n",
            "  adding: results/history/e40/test/s6.png (deflated 20%)\n",
            "  adding: results/history/e40/test/s9.png (deflated 21%)\n",
            "  adding: results/history/e40/test/s2.png (deflated 19%)\n",
            "  adding: results/history/e40/test/s3.png (deflated 21%)\n",
            "  adding: results/history/e40/test/s4.png (deflated 20%)\n",
            "  adding: results/history/e40/test/s1.png (deflated 20%)\n",
            "  adding: results/history/e40/test/s10.png (deflated 19%)\n",
            "  adding: results/history/e40/test/s15.png (deflated 21%)\n",
            "  adding: results/history/e40/test/s7.png (deflated 17%)\n",
            "  adding: results/history/e40/test/s14.png (deflated 21%)\n",
            "  adding: results/history/e40/test/s11.png (deflated 18%)\n",
            "  adding: results/history/e40/test/s8.png (deflated 22%)\n",
            "  adding: results/history/e40/latent_pca_values.npy (deflated 5%)\n",
            "  adding: results/history/e40/latent_stds.png (deflated 17%)\n",
            "  adding: results/history/e40/latent_pca_values.png (deflated 19%)\n",
            "  adding: results/history/e40/latent_means.npy (deflated 5%)\n",
            "  adding: results/history/e40/latent_pca_vectors.npy (deflated 4%)\n",
            "  adding: results/history/e40/test.mid (deflated 3%)\n",
            "  adding: results/history/e40/random_vectors3.mid (deflated 3%)\n",
            "  adding: results/history/e40/random_vectors9.mid (deflated 3%)\n",
            "  adding: results/history/e40/random_vectors4.mid (deflated 3%)\n",
            "  adding: results/history/e40/random_vectors6.mid (deflated 3%)\n",
            "  adding: results/history/e40/random_vectors7.mid (deflated 3%)\n",
            "  adding: results/history/e40/random_vectors8.mid (deflated 3%)\n",
            "  adding: results/history/e40/latent_stds.npy (deflated 15%)\n",
            "  adding: results/history/e40/random_vectors1.mid (deflated 3%)\n",
            "  adding: results/history/e40/random_vectors2.mid (deflated 3%)\n",
            "  adding: results/history/e40/random_vectors0.mid (deflated 3%)\n",
            "  adding: results/history/e40/latent_means.png (deflated 13%)\n",
            "  adding: results/history/e100/ (stored 0%)\n",
            "  adding: results/history/e100/random_vectors5.mid (deflated 3%)\n",
            "  adding: results/history/e100/test/ (stored 0%)\n",
            "  adding: results/history/e100/test/s0.png (deflated 18%)\n",
            "  adding: results/history/e100/test/s13.png (deflated 19%)\n",
            "  adding: results/history/e100/test/s5.png (deflated 18%)\n",
            "  adding: results/history/e100/test/s12.png (deflated 17%)\n",
            "  adding: results/history/e100/test/s6.png (deflated 19%)\n",
            "  adding: results/history/e100/test/s9.png (deflated 18%)\n",
            "  adding: results/history/e100/test/s2.png (deflated 18%)\n",
            "  adding: results/history/e100/test/s3.png (deflated 17%)\n",
            "  adding: results/history/e100/test/s4.png (deflated 17%)\n",
            "  adding: results/history/e100/test/s1.png (deflated 17%)\n",
            "  adding: results/history/e100/test/s10.png (deflated 17%)\n",
            "  adding: results/history/e100/test/s15.png (deflated 17%)\n",
            "  adding: results/history/e100/test/s7.png (deflated 17%)\n",
            "  adding: results/history/e100/test/s14.png (deflated 18%)\n",
            "  adding: results/history/e100/test/s11.png (deflated 16%)\n",
            "  adding: results/history/e100/test/s8.png (deflated 18%)\n",
            "  adding: results/history/e100/latent_pca_values.npy (deflated 5%)\n",
            "  adding: results/history/e100/latent_stds.png (deflated 14%)\n",
            "  adding: results/history/e100/latent_pca_values.png (deflated 18%)\n",
            "  adding: results/history/e100/latent_means.npy (deflated 7%)\n",
            "  adding: results/history/e100/latent_pca_vectors.npy (deflated 4%)\n",
            "  adding: results/history/e100/test.mid (deflated 3%)\n",
            "  adding: results/history/e100/random_vectors3.mid (deflated 3%)\n",
            "  adding: results/history/e100/random_vectors9.mid (deflated 3%)\n",
            "  adding: results/history/e100/random_vectors4.mid (deflated 3%)\n",
            "  adding: results/history/e100/random_vectors6.mid (deflated 3%)\n",
            "  adding: results/history/e100/random_vectors7.mid (deflated 3%)\n",
            "  adding: results/history/e100/random_vectors8.mid (deflated 3%)\n",
            "  adding: results/history/e100/latent_stds.npy (deflated 13%)\n",
            "  adding: results/history/e100/random_vectors1.mid (deflated 3%)\n",
            "  adding: results/history/e100/random_vectors2.mid (deflated 3%)\n",
            "  adding: results/history/e100/random_vectors0.mid (deflated 3%)\n",
            "  adding: results/history/e100/latent_means.png (deflated 13%)\n",
            "  adding: results/history/e250/ (stored 0%)\n",
            "  adding: results/history/e250/random_vectors5.mid (deflated 3%)\n",
            "  adding: results/history/e250/test/ (stored 0%)\n",
            "  adding: results/history/e250/test/s0.png (deflated 21%)\n",
            "  adding: results/history/e250/test/s13.png (deflated 21%)\n",
            "  adding: results/history/e250/test/s5.png (deflated 21%)\n",
            "  adding: results/history/e250/test/s12.png (deflated 22%)\n",
            "  adding: results/history/e250/test/s6.png (deflated 22%)\n",
            "  adding: results/history/e250/test/s9.png (deflated 21%)\n",
            "  adding: results/history/e250/test/s2.png (deflated 21%)\n",
            "  adding: results/history/e250/test/s3.png (deflated 20%)\n",
            "  adding: results/history/e250/test/s4.png (deflated 21%)\n",
            "  adding: results/history/e250/test/s1.png (deflated 21%)\n",
            "  adding: results/history/e250/test/s10.png (deflated 21%)\n",
            "  adding: results/history/e250/test/s15.png (deflated 21%)\n",
            "  adding: results/history/e250/test/s7.png (deflated 21%)\n",
            "  adding: results/history/e250/test/s14.png (deflated 21%)\n",
            "  adding: results/history/e250/test/s11.png (deflated 21%)\n",
            "  adding: results/history/e250/test/s8.png (deflated 22%)\n",
            "  adding: results/history/e250/latent_pca_values.npy (deflated 5%)\n",
            "  adding: results/history/e250/latent_stds.png (deflated 14%)\n",
            "  adding: results/history/e250/latent_pca_values.png (deflated 19%)\n",
            "  adding: results/history/e250/latent_means.npy (deflated 6%)\n",
            "  adding: results/history/e250/latent_pca_vectors.npy (deflated 4%)\n",
            "  adding: results/history/e250/test.mid (deflated 3%)\n",
            "  adding: results/history/e250/random_vectors3.mid (deflated 3%)\n",
            "  adding: results/history/e250/random_vectors9.mid (deflated 3%)\n",
            "  adding: results/history/e250/random_vectors4.mid (deflated 3%)\n",
            "  adding: results/history/e250/random_vectors6.mid (deflated 3%)\n",
            "  adding: results/history/e250/random_vectors7.mid (deflated 3%)\n",
            "  adding: results/history/e250/random_vectors8.mid (deflated 3%)\n",
            "  adding: results/history/e250/latent_stds.npy (deflated 14%)\n",
            "  adding: results/history/e250/random_vectors1.mid (deflated 3%)\n",
            "  adding: results/history/e250/random_vectors2.mid (deflated 3%)\n",
            "  adding: results/history/e250/random_vectors0.mid (deflated 3%)\n",
            "  adding: results/history/e250/latent_means.png (deflated 13%)\n",
            "  adding: results/history/e600/ (stored 0%)\n",
            "  adding: results/history/e600/random_vectors5.mid (deflated 3%)\n",
            "  adding: results/history/e600/test/ (stored 0%)\n",
            "  adding: results/history/e600/test/s0.png (deflated 23%)\n",
            "  adding: results/history/e600/test/s13.png (deflated 23%)\n",
            "  adding: results/history/e600/test/s5.png (deflated 23%)\n",
            "  adding: results/history/e600/test/s12.png (deflated 23%)\n",
            "  adding: results/history/e600/test/s6.png (deflated 22%)\n",
            "  adding: results/history/e600/test/s9.png (deflated 23%)\n",
            "  adding: results/history/e600/test/s2.png (deflated 21%)\n",
            "  adding: results/history/e600/test/s3.png (deflated 22%)\n",
            "  adding: results/history/e600/test/s4.png (deflated 22%)\n",
            "  adding: results/history/e600/test/s1.png (deflated 23%)\n",
            "  adding: results/history/e600/test/s10.png (deflated 21%)\n",
            "  adding: results/history/e600/test/s15.png (deflated 21%)\n",
            "  adding: results/history/e600/test/s7.png (deflated 24%)\n",
            "  adding: results/history/e600/test/s14.png (deflated 22%)\n",
            "  adding: results/history/e600/test/s11.png (deflated 21%)\n",
            "  adding: results/history/e600/test/s8.png (deflated 22%)\n",
            "  adding: results/history/e600/latent_pca_values.npy (deflated 5%)\n",
            "  adding: results/history/e600/latent_stds.png (deflated 17%)\n",
            "  adding: results/history/e600/latent_pca_values.png (deflated 16%)\n",
            "  adding: results/history/e600/latent_means.npy (deflated 6%)\n",
            "  adding: results/history/e600/latent_pca_vectors.npy (deflated 4%)\n",
            "  adding: results/history/e600/test.mid (deflated 3%)\n",
            "  adding: results/history/e600/random_vectors3.mid (deflated 3%)\n",
            "  adding: results/history/e600/random_vectors9.mid (deflated 3%)\n",
            "  adding: results/history/e600/random_vectors4.mid (deflated 3%)\n",
            "  adding: results/history/e600/random_vectors6.mid (deflated 3%)\n",
            "  adding: results/history/e600/random_vectors7.mid (deflated 3%)\n",
            "  adding: results/history/e600/random_vectors8.mid (deflated 3%)\n",
            "  adding: results/history/e600/latent_stds.npy (deflated 14%)\n",
            "  adding: results/history/e600/random_vectors1.mid (deflated 3%)\n",
            "  adding: results/history/e600/random_vectors2.mid (deflated 3%)\n",
            "  adding: results/history/e600/random_vectors0.mid (deflated 3%)\n",
            "  adding: results/history/e600/latent_means.png (deflated 14%)\n",
            "  adding: results/history/e2000/ (stored 0%)\n",
            "  adding: results/history/e2000/random_vectors5.mid (deflated 3%)\n",
            "  adding: results/history/e2000/test/ (stored 0%)\n",
            "  adding: results/history/e2000/test/s0.png (deflated 24%)\n",
            "  adding: results/history/e2000/test/s13.png (deflated 22%)\n",
            "  adding: results/history/e2000/test/s5.png (deflated 24%)\n",
            "  adding: results/history/e2000/test/s12.png (deflated 23%)\n",
            "  adding: results/history/e2000/test/s6.png (deflated 24%)\n",
            "  adding: results/history/e2000/test/s9.png (deflated 24%)\n",
            "  adding: results/history/e2000/test/s2.png (deflated 23%)\n",
            "  adding: results/history/e2000/test/s3.png (deflated 23%)\n",
            "  adding: results/history/e2000/test/s4.png (deflated 23%)\n",
            "  adding: results/history/e2000/test/s1.png (deflated 23%)\n",
            "  adding: results/history/e2000/test/s10.png (deflated 24%)\n",
            "  adding: results/history/e2000/test/s15.png (deflated 24%)\n",
            "  adding: results/history/e2000/test/s7.png (deflated 24%)\n",
            "  adding: results/history/e2000/test/s14.png (deflated 24%)\n",
            "  adding: results/history/e2000/test/s11.png (deflated 23%)\n",
            "  adding: results/history/e2000/test/s8.png (deflated 23%)\n",
            "  adding: results/history/e2000/latent_pca_values.npy (deflated 4%)\n",
            "  adding: results/history/e2000/latent_stds.png (deflated 15%)\n",
            "  adding: results/history/e2000/latent_pca_values.png (deflated 14%)\n",
            "  adding: results/history/e2000/latent_means.npy (deflated 6%)\n",
            "  adding: results/history/e2000/latent_pca_vectors.npy (deflated 4%)\n",
            "  adding: results/history/e2000/test.mid (deflated 30%)\n",
            "  adding: results/history/e2000/random_vectors3.mid (deflated 3%)\n",
            "  adding: results/history/e2000/random_vectors9.mid (deflated 3%)\n",
            "  adding: results/history/e2000/random_vectors4.mid (deflated 3%)\n",
            "  adding: results/history/e2000/random_vectors6.mid (deflated 3%)\n",
            "  adding: results/history/e2000/random_vectors7.mid (deflated 3%)\n",
            "  adding: results/history/e2000/random_vectors8.mid (deflated 3%)\n",
            "  adding: results/history/e2000/latent_stds.npy (deflated 13%)\n",
            "  adding: results/history/e2000/random_vectors1.mid (deflated 3%)\n",
            "  adding: results/history/e2000/random_vectors2.mid (deflated 3%)\n",
            "  adding: results/history/e2000/random_vectors0.mid (deflated 3%)\n",
            "  adding: results/history/e2000/latent_means.png (deflated 12%)\n",
            "  adding: results/history/e1000/ (stored 0%)\n",
            "  adding: results/history/e1000/random_vectors5.mid (deflated 3%)\n",
            "  adding: results/history/e1000/test/ (stored 0%)\n",
            "  adding: results/history/e1000/test/s0.png (deflated 21%)\n",
            "  adding: results/history/e1000/test/s13.png (deflated 21%)\n",
            "  adding: results/history/e1000/test/s5.png (deflated 21%)\n",
            "  adding: results/history/e1000/test/s12.png (deflated 21%)\n",
            "  adding: results/history/e1000/test/s6.png (deflated 21%)\n",
            "  adding: results/history/e1000/test/s9.png (deflated 25%)\n",
            "  adding: results/history/e1000/test/s2.png (deflated 19%)\n",
            "  adding: results/history/e1000/test/s3.png (deflated 21%)\n",
            "  adding: results/history/e1000/test/s4.png (deflated 21%)\n",
            "  adding: results/history/e1000/test/s1.png (deflated 20%)\n",
            "  adding: results/history/e1000/test/s10.png (deflated 20%)\n",
            "  adding: results/history/e1000/test/s15.png (deflated 20%)\n",
            "  adding: results/history/e1000/test/s7.png (deflated 25%)\n",
            "  adding: results/history/e1000/test/s14.png (deflated 25%)\n",
            "  adding: results/history/e1000/test/s11.png (deflated 21%)\n",
            "  adding: results/history/e1000/test/s8.png (deflated 20%)\n",
            "  adding: results/history/e1000/latent_pca_values.npy (deflated 4%)\n",
            "  adding: results/history/e1000/latent_stds.png (deflated 16%)\n",
            "  adding: results/history/e1000/latent_pca_values.png (deflated 16%)\n",
            "  adding: results/history/e1000/latent_means.npy (deflated 6%)\n",
            "  adding: results/history/e1000/latent_pca_vectors.npy (deflated 4%)\n",
            "  adding: results/history/e1000/test.mid (deflated 35%)\n",
            "  adding: results/history/e1000/random_vectors3.mid (deflated 3%)\n",
            "  adding: results/history/e1000/random_vectors9.mid (deflated 3%)\n",
            "  adding: results/history/e1000/random_vectors4.mid (deflated 3%)\n",
            "  adding: results/history/e1000/random_vectors6.mid (deflated 3%)\n",
            "  adding: results/history/e1000/random_vectors7.mid (deflated 3%)\n",
            "  adding: results/history/e1000/random_vectors8.mid (deflated 3%)\n",
            "  adding: results/history/e1000/latent_stds.npy (deflated 14%)\n",
            "  adding: results/history/e1000/random_vectors1.mid (deflated 3%)\n",
            "  adding: results/history/e1000/random_vectors2.mid (deflated 3%)\n",
            "  adding: results/history/e1000/random_vectors0.mid (deflated 3%)\n",
            "  adding: results/history/e1000/latent_means.png (deflated 12%)\n",
            "  adding: results/history/e450/ (stored 0%)\n",
            "  adding: results/history/e450/random_vectors5.mid (deflated 3%)\n",
            "  adding: results/history/e450/test/ (stored 0%)\n",
            "  adding: results/history/e450/test/s0.png (deflated 22%)\n",
            "  adding: results/history/e450/test/s13.png (deflated 25%)\n",
            "  adding: results/history/e450/test/s5.png (deflated 24%)\n",
            "  adding: results/history/e450/test/s12.png (deflated 24%)\n",
            "  adding: results/history/e450/test/s6.png (deflated 23%)\n",
            "  adding: results/history/e450/test/s9.png (deflated 23%)\n",
            "  adding: results/history/e450/test/s2.png (deflated 23%)\n",
            "  adding: results/history/e450/test/s3.png (deflated 24%)\n",
            "  adding: results/history/e450/test/s4.png (deflated 23%)\n",
            "  adding: results/history/e450/test/s1.png (deflated 22%)\n",
            "  adding: results/history/e450/test/s10.png (deflated 25%)\n",
            "  adding: results/history/e450/test/s15.png (deflated 23%)\n",
            "  adding: results/history/e450/test/s7.png (deflated 23%)\n",
            "  adding: results/history/e450/test/s14.png (deflated 23%)\n",
            "  adding: results/history/e450/test/s11.png (deflated 24%)\n",
            "  adding: results/history/e450/test/s8.png (deflated 25%)\n",
            "  adding: results/history/e450/latent_pca_values.npy (deflated 4%)\n",
            "  adding: results/history/e450/latent_stds.png (deflated 18%)\n",
            "  adding: results/history/e450/latent_pca_values.png (deflated 19%)\n",
            "  adding: results/history/e450/latent_means.npy (deflated 5%)\n",
            "  adding: results/history/e450/latent_pca_vectors.npy (deflated 4%)\n",
            "  adding: results/history/e450/test.mid (deflated 3%)\n",
            "  adding: results/history/e450/random_vectors3.mid (deflated 3%)\n",
            "  adding: results/history/e450/random_vectors9.mid (deflated 3%)\n",
            "  adding: results/history/e450/random_vectors4.mid (deflated 3%)\n",
            "  adding: results/history/e450/random_vectors6.mid (deflated 3%)\n",
            "  adding: results/history/e450/random_vectors7.mid (deflated 3%)\n",
            "  adding: results/history/e450/random_vectors8.mid (deflated 3%)\n",
            "  adding: results/history/e450/latent_stds.npy (deflated 14%)\n",
            "  adding: results/history/e450/random_vectors1.mid (deflated 3%)\n",
            "  adding: results/history/e450/random_vectors2.mid (deflated 3%)\n",
            "  adding: results/history/e450/random_vectors0.mid (deflated 3%)\n",
            "  adding: results/history/e450/latent_means.png (deflated 14%)\n",
            "  adding: results/history/e160/ (stored 0%)\n",
            "  adding: results/history/e160/random_vectors5.mid (deflated 3%)\n",
            "  adding: results/history/e160/test/ (stored 0%)\n",
            "  adding: results/history/e160/test/s0.png (deflated 22%)\n",
            "  adding: results/history/e160/test/s13.png (deflated 21%)\n",
            "  adding: results/history/e160/test/s5.png (deflated 20%)\n",
            "  adding: results/history/e160/test/s12.png (deflated 21%)\n",
            "  adding: results/history/e160/test/s6.png (deflated 21%)\n",
            "  adding: results/history/e160/test/s9.png (deflated 22%)\n",
            "  adding: results/history/e160/test/s2.png (deflated 21%)\n",
            "  adding: results/history/e160/test/s3.png (deflated 21%)\n",
            "  adding: results/history/e160/test/s4.png (deflated 21%)\n",
            "  adding: results/history/e160/test/s1.png (deflated 18%)\n",
            "  adding: results/history/e160/test/s10.png (deflated 22%)\n",
            "  adding: results/history/e160/test/s15.png (deflated 23%)\n",
            "  adding: results/history/e160/test/s7.png (deflated 22%)\n",
            "  adding: results/history/e160/test/s14.png (deflated 21%)\n",
            "  adding: results/history/e160/test/s11.png (deflated 21%)\n",
            "  adding: results/history/e160/test/s8.png (deflated 20%)\n",
            "  adding: results/history/e160/latent_pca_values.npy (deflated 5%)\n",
            "  adding: results/history/e160/latent_stds.png (deflated 15%)\n",
            "  adding: results/history/e160/latent_pca_values.png (deflated 19%)\n",
            "  adding: results/history/e160/latent_means.npy (deflated 5%)\n",
            "  adding: results/history/e160/latent_pca_vectors.npy (deflated 4%)\n",
            "  adding: results/history/e160/test.mid (deflated 3%)\n",
            "  adding: results/history/e160/random_vectors3.mid (deflated 3%)\n",
            "  adding: results/history/e160/random_vectors9.mid (deflated 3%)\n",
            "  adding: results/history/e160/random_vectors4.mid (deflated 3%)\n",
            "  adding: results/history/e160/random_vectors6.mid (deflated 3%)\n",
            "  adding: results/history/e160/random_vectors7.mid (deflated 3%)\n",
            "  adding: results/history/e160/random_vectors8.mid (deflated 3%)\n",
            "  adding: results/history/e160/latent_stds.npy (deflated 14%)\n",
            "  adding: results/history/e160/random_vectors1.mid (deflated 3%)\n",
            "  adding: results/history/e160/random_vectors2.mid (deflated 3%)\n",
            "  adding: results/history/e160/random_vectors0.mid (deflated 3%)\n",
            "  adding: results/history/e160/latent_means.png (deflated 13%)\n",
            "  adding: results/history/e1800/ (stored 0%)\n",
            "  adding: results/history/e1800/random_vectors5.mid (deflated 3%)\n",
            "  adding: results/history/e1800/test/ (stored 0%)\n",
            "  adding: results/history/e1800/test/s0.png (deflated 23%)\n",
            "  adding: results/history/e1800/test/s13.png (deflated 23%)\n",
            "  adding: results/history/e1800/test/s5.png (deflated 23%)\n",
            "  adding: results/history/e1800/test/s12.png (deflated 22%)\n",
            "  adding: results/history/e1800/test/s6.png (deflated 22%)\n",
            "  adding: results/history/e1800/test/s9.png (deflated 22%)\n",
            "  adding: results/history/e1800/test/s2.png (deflated 24%)\n",
            "  adding: results/history/e1800/test/s3.png (deflated 24%)\n",
            "  adding: results/history/e1800/test/s4.png (deflated 23%)\n",
            "  adding: results/history/e1800/test/s1.png (deflated 22%)\n",
            "  adding: results/history/e1800/test/s10.png (deflated 23%)\n",
            "  adding: results/history/e1800/test/s15.png (deflated 22%)\n",
            "  adding: results/history/e1800/test/s7.png (deflated 23%)\n",
            "  adding: results/history/e1800/test/s14.png (deflated 23%)\n",
            "  adding: results/history/e1800/test/s11.png (deflated 22%)\n",
            "  adding: results/history/e1800/test/s8.png (deflated 24%)\n",
            "  adding: results/history/e1800/latent_pca_values.npy (deflated 4%)\n",
            "  adding: results/history/e1800/latent_stds.png (deflated 15%)\n",
            "  adding: results/history/e1800/latent_pca_values.png (deflated 16%)\n",
            "  adding: results/history/e1800/latent_means.npy (deflated 6%)\n",
            "  adding: results/history/e1800/latent_pca_vectors.npy (deflated 4%)\n",
            "  adding: results/history/e1800/test.mid (deflated 65%)\n",
            "  adding: results/history/e1800/random_vectors3.mid (deflated 3%)\n",
            "  adding: results/history/e1800/random_vectors9.mid (deflated 3%)\n",
            "  adding: results/history/e1800/random_vectors4.mid (deflated 3%)\n",
            "  adding: results/history/e1800/random_vectors6.mid (deflated 3%)\n",
            "  adding: results/history/e1800/random_vectors7.mid (deflated 3%)\n",
            "  adding: results/history/e1800/random_vectors8.mid (deflated 3%)\n",
            "  adding: results/history/e1800/latent_stds.npy (deflated 13%)\n",
            "  adding: results/history/e1800/random_vectors1.mid (deflated 3%)\n",
            "  adding: results/history/e1800/random_vectors2.mid (deflated 3%)\n",
            "  adding: results/history/e1800/random_vectors0.mid (deflated 3%)\n",
            "  adding: results/history/e1800/latent_means.png (deflated 12%)\n",
            "  adding: results/history/e60/ (stored 0%)\n",
            "  adding: results/history/e60/random_vectors5.mid (deflated 3%)\n",
            "  adding: results/history/e60/test/ (stored 0%)\n",
            "  adding: results/history/e60/test/s0.png (deflated 19%)\n",
            "  adding: results/history/e60/test/s13.png (deflated 21%)\n",
            "  adding: results/history/e60/test/s5.png (deflated 20%)\n",
            "  adding: results/history/e60/test/s12.png (deflated 21%)\n",
            "  adding: results/history/e60/test/s6.png (deflated 21%)\n",
            "  adding: results/history/e60/test/s9.png (deflated 22%)\n",
            "  adding: results/history/e60/test/s2.png (deflated 21%)\n",
            "  adding: results/history/e60/test/s3.png (deflated 20%)\n",
            "  adding: results/history/e60/test/s4.png (deflated 22%)\n",
            "  adding: results/history/e60/test/s1.png (deflated 21%)\n",
            "  adding: results/history/e60/test/s10.png (deflated 21%)\n",
            "  adding: results/history/e60/test/s15.png (deflated 20%)\n",
            "  adding: results/history/e60/test/s7.png (deflated 21%)\n",
            "  adding: results/history/e60/test/s14.png (deflated 22%)\n",
            "  adding: results/history/e60/test/s11.png (deflated 21%)\n",
            "  adding: results/history/e60/test/s8.png (deflated 23%)\n",
            "  adding: results/history/e60/latent_pca_values.npy (deflated 5%)\n",
            "  adding: results/history/e60/latent_stds.png (deflated 17%)\n",
            "  adding: results/history/e60/latent_pca_values.png (deflated 19%)\n",
            "  adding: results/history/e60/latent_means.npy (deflated 5%)\n",
            "  adding: results/history/e60/latent_pca_vectors.npy (deflated 4%)\n",
            "  adding: results/history/e60/test.mid (deflated 3%)\n",
            "  adding: results/history/e60/random_vectors3.mid (deflated 3%)\n",
            "  adding: results/history/e60/random_vectors9.mid (deflated 3%)\n",
            "  adding: results/history/e60/random_vectors4.mid (deflated 3%)\n",
            "  adding: results/history/e60/random_vectors6.mid (deflated 3%)\n",
            "  adding: results/history/e60/random_vectors7.mid (deflated 3%)\n",
            "  adding: results/history/e60/random_vectors8.mid (deflated 3%)\n",
            "  adding: results/history/e60/latent_stds.npy (deflated 16%)\n",
            "  adding: results/history/e60/random_vectors1.mid (deflated 3%)\n",
            "  adding: results/history/e60/random_vectors2.mid (deflated 3%)\n",
            "  adding: results/history/e60/random_vectors0.mid (deflated 3%)\n",
            "  adding: results/history/e60/latent_means.png (deflated 16%)\n",
            "  adding: results/history/e1200/ (stored 0%)\n",
            "  adding: results/history/e1200/random_vectors5.mid (deflated 3%)\n",
            "  adding: results/history/e1200/test/ (stored 0%)\n",
            "  adding: results/history/e1200/test/s0.png (deflated 22%)\n",
            "  adding: results/history/e1200/test/s13.png (deflated 22%)\n",
            "  adding: results/history/e1200/test/s5.png (deflated 23%)\n",
            "  adding: results/history/e1200/test/s12.png (deflated 24%)\n",
            "  adding: results/history/e1200/test/s6.png (deflated 24%)\n",
            "  adding: results/history/e1200/test/s9.png (deflated 23%)\n",
            "  adding: results/history/e1200/test/s2.png (deflated 22%)\n",
            "  adding: results/history/e1200/test/s3.png (deflated 24%)\n",
            "  adding: results/history/e1200/test/s4.png (deflated 22%)\n",
            "  adding: results/history/e1200/test/s1.png (deflated 22%)\n",
            "  adding: results/history/e1200/test/s10.png (deflated 23%)\n",
            "  adding: results/history/e1200/test/s15.png (deflated 23%)\n",
            "  adding: results/history/e1200/test/s7.png (deflated 23%)\n",
            "  adding: results/history/e1200/test/s14.png (deflated 22%)\n",
            "  adding: results/history/e1200/test/s11.png (deflated 22%)\n",
            "  adding: results/history/e1200/test/s8.png (deflated 24%)\n",
            "  adding: results/history/e1200/latent_pca_values.npy (deflated 4%)\n",
            "  adding: results/history/e1200/latent_stds.png (deflated 15%)\n",
            "  adding: results/history/e1200/latent_pca_values.png (deflated 16%)\n",
            "  adding: results/history/e1200/latent_means.npy (deflated 5%)\n",
            "  adding: results/history/e1200/latent_pca_vectors.npy (deflated 4%)\n",
            "  adding: results/history/e1200/test.mid (deflated 63%)\n",
            "  adding: results/history/e1200/random_vectors3.mid (deflated 3%)\n",
            "  adding: results/history/e1200/random_vectors9.mid (deflated 3%)\n",
            "  adding: results/history/e1200/random_vectors4.mid (deflated 3%)\n",
            "  adding: results/history/e1200/random_vectors6.mid (deflated 3%)\n",
            "  adding: results/history/e1200/random_vectors7.mid (deflated 3%)\n",
            "  adding: results/history/e1200/random_vectors8.mid (deflated 3%)\n",
            "  adding: results/history/e1200/latent_stds.npy (deflated 14%)\n",
            "  adding: results/history/e1200/random_vectors1.mid (deflated 3%)\n",
            "  adding: results/history/e1200/random_vectors2.mid (deflated 3%)\n",
            "  adding: results/history/e1200/random_vectors0.mid (deflated 3%)\n",
            "  adding: results/history/e1200/latent_means.png (deflated 12%)\n",
            "  adding: results/history/e1/ (stored 0%)\n",
            "  adding: results/history/e1/random_vectors5.mid (deflated 3%)\n",
            "  adding: results/history/e1/test/ (stored 0%)\n",
            "  adding: results/history/e1/test/s0.png (stored 0%)\n",
            "  adding: results/history/e1/test/s13.png (stored 0%)\n",
            "  adding: results/history/e1/test/s5.png (stored 0%)\n",
            "  adding: results/history/e1/test/s12.png (stored 0%)\n",
            "  adding: results/history/e1/test/s6.png (stored 0%)\n",
            "  adding: results/history/e1/test/s9.png (stored 0%)\n",
            "  adding: results/history/e1/test/s2.png (stored 0%)\n",
            "  adding: results/history/e1/test/s3.png (stored 0%)\n",
            "  adding: results/history/e1/test/s4.png (stored 0%)\n",
            "  adding: results/history/e1/test/s1.png (stored 0%)\n",
            "  adding: results/history/e1/test/s10.png (stored 0%)\n",
            "  adding: results/history/e1/test/s15.png (stored 0%)\n",
            "  adding: results/history/e1/test/s7.png (stored 0%)\n",
            "  adding: results/history/e1/test/s14.png (stored 0%)\n",
            "  adding: results/history/e1/test/s11.png (stored 0%)\n",
            "  adding: results/history/e1/test/s8.png (stored 0%)\n",
            "  adding: results/history/e1/latent_pca_values.npy (deflated 4%)\n",
            "  adding: results/history/e1/latent_stds.png (deflated 13%)\n",
            "  adding: results/history/e1/latent_pca_values.png (deflated 14%)\n",
            "  adding: results/history/e1/latent_means.npy (deflated 5%)\n",
            "  adding: results/history/e1/latent_pca_vectors.npy (deflated 4%)\n",
            "  adding: results/history/e1/test.mid (deflated 3%)\n",
            "  adding: results/history/e1/random_vectors3.mid (deflated 3%)\n",
            "  adding: results/history/e1/random_vectors9.mid (deflated 3%)\n",
            "  adding: results/history/e1/random_vectors4.mid (deflated 3%)\n",
            "  adding: results/history/e1/random_vectors6.mid (deflated 3%)\n",
            "  adding: results/history/e1/random_vectors7.mid (deflated 3%)\n",
            "  adding: results/history/e1/random_vectors8.mid (deflated 3%)\n",
            "  adding: results/history/e1/latent_stds.npy (deflated 12%)\n",
            "  adding: results/history/e1/random_vectors1.mid (deflated 3%)\n",
            "  adding: results/history/e1/random_vectors2.mid (deflated 3%)\n",
            "  adding: results/history/e1/random_vectors0.mid (deflated 3%)\n",
            "  adding: results/history/e1/latent_means.png (deflated 12%)\n",
            "  adding: results/history/e1400/ (stored 0%)\n",
            "  adding: results/history/e1400/random_vectors5.mid (deflated 3%)\n",
            "  adding: results/history/e1400/test/ (stored 0%)\n",
            "  adding: results/history/e1400/test/s0.png (deflated 23%)\n",
            "  adding: results/history/e1400/test/s13.png (deflated 22%)\n",
            "  adding: results/history/e1400/test/s5.png (deflated 21%)\n",
            "  adding: results/history/e1400/test/s12.png (deflated 22%)\n",
            "  adding: results/history/e1400/test/s6.png (deflated 24%)\n",
            "  adding: results/history/e1400/test/s9.png (deflated 21%)\n",
            "  adding: results/history/e1400/test/s2.png (deflated 22%)\n",
            "  adding: results/history/e1400/test/s3.png (deflated 21%)\n",
            "  adding: results/history/e1400/test/s4.png (deflated 23%)\n",
            "  adding: results/history/e1400/test/s1.png (deflated 23%)\n",
            "  adding: results/history/e1400/test/s10.png (deflated 22%)\n",
            "  adding: results/history/e1400/test/s15.png (deflated 22%)\n",
            "  adding: results/history/e1400/test/s7.png (deflated 22%)\n",
            "  adding: results/history/e1400/test/s14.png (deflated 22%)\n",
            "  adding: results/history/e1400/test/s11.png (deflated 22%)\n",
            "  adding: results/history/e1400/test/s8.png (deflated 22%)\n",
            "  adding: results/history/e1400/latent_pca_values.npy (deflated 4%)\n",
            "  adding: results/history/e1400/latent_stds.png (deflated 15%)\n",
            "  adding: results/history/e1400/latent_pca_values.png (deflated 16%)\n",
            "  adding: results/history/e1400/latent_means.npy (deflated 6%)\n",
            "  adding: results/history/e1400/latent_pca_vectors.npy (deflated 4%)\n",
            "  adding: results/history/e1400/test.mid (deflated 69%)\n",
            "  adding: results/history/e1400/random_vectors3.mid (deflated 3%)\n",
            "  adding: results/history/e1400/random_vectors9.mid (deflated 3%)\n",
            "  adding: results/history/e1400/random_vectors4.mid (deflated 3%)\n",
            "  adding: results/history/e1400/random_vectors6.mid (deflated 3%)\n",
            "  adding: results/history/e1400/random_vectors7.mid (deflated 3%)\n",
            "  adding: results/history/e1400/random_vectors8.mid (deflated 3%)\n",
            "  adding: results/history/e1400/latent_stds.npy (deflated 14%)\n",
            "  adding: results/history/e1400/random_vectors1.mid (deflated 3%)\n",
            "  adding: results/history/e1400/random_vectors2.mid (deflated 3%)\n",
            "  adding: results/history/e1400/random_vectors0.mid (deflated 3%)\n",
            "  adding: results/history/e1400/latent_means.png (deflated 13%)\n",
            "  adding: results/history/e500/ (stored 0%)\n",
            "  adding: results/history/e500/random_vectors5.mid (deflated 3%)\n",
            "  adding: results/history/e500/test/ (stored 0%)\n",
            "  adding: results/history/e500/test/s0.png (deflated 27%)\n",
            "  adding: results/history/e500/test/s13.png (deflated 27%)\n",
            "  adding: results/history/e500/test/s5.png (deflated 25%)\n",
            "  adding: results/history/e500/test/s12.png (deflated 26%)\n",
            "  adding: results/history/e500/test/s6.png (deflated 26%)\n",
            "  adding: results/history/e500/test/s9.png (deflated 26%)\n",
            "  adding: results/history/e500/test/s2.png (deflated 26%)\n",
            "  adding: results/history/e500/test/s3.png (deflated 26%)\n",
            "  adding: results/history/e500/test/s4.png (deflated 27%)\n",
            "  adding: results/history/e500/test/s1.png (deflated 25%)\n",
            "  adding: results/history/e500/test/s10.png (deflated 26%)\n",
            "  adding: results/history/e500/test/s15.png (deflated 26%)\n",
            "  adding: results/history/e500/test/s7.png (deflated 26%)\n",
            "  adding: results/history/e500/test/s14.png (deflated 28%)\n",
            "  adding: results/history/e500/test/s11.png (deflated 27%)\n",
            "  adding: results/history/e500/test/s8.png (deflated 26%)\n",
            "  adding: results/history/e500/latent_pca_values.npy (deflated 5%)\n",
            "  adding: results/history/e500/latent_stds.png (deflated 16%)\n",
            "  adding: results/history/e500/latent_pca_values.png (deflated 17%)\n",
            "  adding: results/history/e500/latent_means.npy (deflated 5%)\n",
            "  adding: results/history/e500/latent_pca_vectors.npy (deflated 4%)\n",
            "  adding: results/history/e500/test.mid (deflated 3%)\n",
            "  adding: results/history/e500/random_vectors3.mid (deflated 3%)\n",
            "  adding: results/history/e500/random_vectors9.mid (deflated 3%)\n",
            "  adding: results/history/e500/random_vectors4.mid (deflated 3%)\n",
            "  adding: results/history/e500/random_vectors6.mid (deflated 3%)\n",
            "  adding: results/history/e500/random_vectors7.mid (deflated 3%)\n",
            "  adding: results/history/e500/random_vectors8.mid (deflated 3%)\n",
            "  adding: results/history/e500/latent_stds.npy (deflated 15%)\n",
            "  adding: results/history/e500/random_vectors1.mid (deflated 3%)\n",
            "  adding: results/history/e500/random_vectors2.mid (deflated 3%)\n",
            "  adding: results/history/e500/random_vectors0.mid (deflated 3%)\n",
            "  adding: results/history/e500/latent_means.png (deflated 14%)\n",
            "  adding: results/history/e10/ (stored 0%)\n",
            "  adding: results/history/e10/random_vectors5.mid (deflated 3%)\n",
            "  adding: results/history/e10/test/ (stored 0%)\n",
            "  adding: results/history/e10/test/s0.png (stored 0%)\n",
            "  adding: results/history/e10/test/s13.png (stored 0%)\n",
            "  adding: results/history/e10/test/s5.png (stored 0%)\n",
            "  adding: results/history/e10/test/s12.png (stored 0%)\n",
            "  adding: results/history/e10/test/s6.png (stored 0%)\n",
            "  adding: results/history/e10/test/s9.png (stored 0%)\n",
            "  adding: results/history/e10/test/s2.png (stored 0%)\n",
            "  adding: results/history/e10/test/s3.png (stored 0%)\n",
            "  adding: results/history/e10/test/s4.png (stored 0%)\n",
            "  adding: results/history/e10/test/s1.png (stored 0%)\n",
            "  adding: results/history/e10/test/s10.png (stored 0%)\n",
            "  adding: results/history/e10/test/s15.png (stored 0%)\n",
            "  adding: results/history/e10/test/s7.png (stored 0%)\n",
            "  adding: results/history/e10/test/s14.png (stored 0%)\n",
            "  adding: results/history/e10/test/s11.png (stored 0%)\n",
            "  adding: results/history/e10/test/s8.png (stored 0%)\n",
            "  adding: results/history/e10/latent_pca_values.npy (deflated 5%)\n",
            "  adding: results/history/e10/latent_stds.png (deflated 15%)\n",
            "  adding: results/history/e10/latent_pca_values.png (deflated 16%)\n",
            "  adding: results/history/e10/latent_means.npy (deflated 6%)\n",
            "  adding: results/history/e10/latent_pca_vectors.npy (deflated 4%)\n",
            "  adding: results/history/e10/test.mid (deflated 3%)\n",
            "  adding: results/history/e10/random_vectors3.mid (deflated 3%)\n",
            "  adding: results/history/e10/random_vectors9.mid (deflated 3%)\n",
            "  adding: results/history/e10/random_vectors4.mid (deflated 3%)\n",
            "  adding: results/history/e10/random_vectors6.mid (deflated 3%)\n",
            "  adding: results/history/e10/random_vectors7.mid (deflated 3%)\n",
            "  adding: results/history/e10/random_vectors8.mid (deflated 3%)\n",
            "  adding: results/history/e10/latent_stds.npy (deflated 13%)\n",
            "  adding: results/history/e10/random_vectors1.mid (deflated 3%)\n",
            "  adding: results/history/e10/random_vectors2.mid (deflated 3%)\n",
            "  adding: results/history/e10/random_vectors0.mid (deflated 3%)\n",
            "  adding: results/history/e10/latent_means.png (deflated 13%)\n",
            "  adding: results/history/e70/ (stored 0%)\n",
            "  adding: results/history/e70/random_vectors5.mid (deflated 3%)\n",
            "  adding: results/history/e70/test/ (stored 0%)\n",
            "  adding: results/history/e70/test/s0.png (deflated 24%)\n",
            "  adding: results/history/e70/test/s13.png (deflated 22%)\n",
            "  adding: results/history/e70/test/s5.png (deflated 24%)\n",
            "  adding: results/history/e70/test/s12.png (deflated 22%)\n",
            "  adding: results/history/e70/test/s6.png (deflated 25%)\n",
            "  adding: results/history/e70/test/s9.png (deflated 25%)\n",
            "  adding: results/history/e70/test/s2.png (deflated 22%)\n",
            "  adding: results/history/e70/test/s3.png (deflated 25%)\n",
            "  adding: results/history/e70/test/s4.png (deflated 24%)\n",
            "  adding: results/history/e70/test/s1.png (deflated 23%)\n",
            "  adding: results/history/e70/test/s10.png (deflated 23%)\n",
            "  adding: results/history/e70/test/s15.png (deflated 23%)\n",
            "  adding: results/history/e70/test/s7.png (deflated 24%)\n",
            "  adding: results/history/e70/test/s14.png (deflated 23%)\n",
            "  adding: results/history/e70/test/s11.png (deflated 23%)\n",
            "  adding: results/history/e70/test/s8.png (deflated 23%)\n",
            "  adding: results/history/e70/latent_pca_values.npy (deflated 5%)\n",
            "  adding: results/history/e70/latent_stds.png (deflated 17%)\n",
            "  adding: results/history/e70/latent_pca_values.png (deflated 20%)\n",
            "  adding: results/history/e70/latent_means.npy (deflated 5%)\n",
            "  adding: results/history/e70/latent_pca_vectors.npy (deflated 4%)\n",
            "  adding: results/history/e70/test.mid (deflated 3%)\n",
            "  adding: results/history/e70/random_vectors3.mid (deflated 3%)\n",
            "  adding: results/history/e70/random_vectors9.mid (deflated 3%)\n",
            "  adding: results/history/e70/random_vectors4.mid (deflated 3%)\n",
            "  adding: results/history/e70/random_vectors6.mid (deflated 3%)\n",
            "  adding: results/history/e70/random_vectors7.mid (deflated 3%)\n",
            "  adding: results/history/e70/random_vectors8.mid (deflated 3%)\n",
            "  adding: results/history/e70/latent_stds.npy (deflated 15%)\n",
            "  adding: results/history/e70/random_vectors1.mid (deflated 3%)\n",
            "  adding: results/history/e70/random_vectors2.mid (deflated 3%)\n",
            "  adding: results/history/e70/random_vectors0.mid (deflated 3%)\n",
            "  adding: results/history/e70/latent_means.png (deflated 16%)\n",
            "  adding: results/history/e200/ (stored 0%)\n",
            "  adding: results/history/e200/random_vectors5.mid (deflated 3%)\n",
            "  adding: results/history/e200/test/ (stored 0%)\n",
            "  adding: results/history/e200/test/s0.png (deflated 23%)\n",
            "  adding: results/history/e200/test/s13.png (deflated 22%)\n",
            "  adding: results/history/e200/test/s5.png (deflated 22%)\n",
            "  adding: results/history/e200/test/s12.png (deflated 23%)\n",
            "  adding: results/history/e200/test/s6.png (deflated 22%)\n",
            "  adding: results/history/e200/test/s9.png (deflated 22%)\n",
            "  adding: results/history/e200/test/s2.png (deflated 21%)\n",
            "  adding: results/history/e200/test/s3.png (deflated 18%)\n",
            "  adding: results/history/e200/test/s4.png (deflated 22%)\n",
            "  adding: results/history/e200/test/s1.png (deflated 19%)\n",
            "  adding: results/history/e200/test/s10.png (deflated 23%)\n",
            "  adding: results/history/e200/test/s15.png (deflated 21%)\n",
            "  adding: results/history/e200/test/s7.png (deflated 23%)\n",
            "  adding: results/history/e200/test/s14.png (deflated 23%)\n",
            "  adding: results/history/e200/test/s11.png (deflated 22%)\n",
            "  adding: results/history/e200/test/s8.png (deflated 21%)\n",
            "  adding: results/history/e200/latent_pca_values.npy (deflated 5%)\n",
            "  adding: results/history/e200/latent_stds.png (deflated 15%)\n",
            "  adding: results/history/e200/latent_pca_values.png (deflated 18%)\n",
            "  adding: results/history/e200/latent_means.npy (deflated 5%)\n",
            "  adding: results/history/e200/latent_pca_vectors.npy (deflated 4%)\n",
            "  adding: results/history/e200/test.mid (deflated 3%)\n",
            "  adding: results/history/e200/random_vectors3.mid (deflated 3%)\n",
            "  adding: results/history/e200/random_vectors9.mid (deflated 3%)\n",
            "  adding: results/history/e200/random_vectors4.mid (deflated 3%)\n",
            "  adding: results/history/e200/random_vectors6.mid (deflated 3%)\n",
            "  adding: results/history/e200/random_vectors7.mid (deflated 3%)\n",
            "  adding: results/history/e200/random_vectors8.mid (deflated 3%)\n",
            "  adding: results/history/e200/latent_stds.npy (deflated 14%)\n",
            "  adding: results/history/e200/random_vectors1.mid (deflated 3%)\n",
            "  adding: results/history/e200/random_vectors2.mid (deflated 3%)\n",
            "  adding: results/history/e200/random_vectors0.mid (deflated 3%)\n",
            "  adding: results/history/e200/latent_means.png (deflated 13%)\n",
            "  adding: results/history/e400/ (stored 0%)\n",
            "  adding: results/history/e400/random_vectors5.mid (deflated 3%)\n",
            "  adding: results/history/e400/test/ (stored 0%)\n",
            "  adding: results/history/e400/test/s0.png (deflated 22%)\n",
            "  adding: results/history/e400/test/s13.png (deflated 23%)\n",
            "  adding: results/history/e400/test/s5.png (deflated 24%)\n",
            "  adding: results/history/e400/test/s12.png (deflated 22%)\n",
            "  adding: results/history/e400/test/s6.png (deflated 22%)\n",
            "  adding: results/history/e400/test/s9.png (deflated 23%)\n",
            "  adding: results/history/e400/test/s2.png (deflated 20%)\n",
            "  adding: results/history/e400/test/s3.png (deflated 21%)\n",
            "  adding: results/history/e400/test/s4.png (deflated 23%)\n",
            "  adding: results/history/e400/test/s1.png (deflated 21%)\n",
            "  adding: results/history/e400/test/s10.png (deflated 22%)\n",
            "  adding: results/history/e400/test/s15.png (deflated 22%)\n",
            "  adding: results/history/e400/test/s7.png (deflated 23%)\n",
            "  adding: results/history/e400/test/s14.png (deflated 21%)\n",
            "  adding: results/history/e400/test/s11.png (deflated 23%)\n",
            "  adding: results/history/e400/test/s8.png (deflated 23%)\n",
            "  adding: results/history/e400/latent_pca_values.npy (deflated 5%)\n",
            "  adding: results/history/e400/latent_stds.png (deflated 15%)\n",
            "  adding: results/history/e400/latent_pca_values.png (deflated 19%)\n",
            "  adding: results/history/e400/latent_means.npy (deflated 6%)\n",
            "  adding: results/history/e400/latent_pca_vectors.npy (deflated 4%)\n",
            "  adding: results/history/e400/test.mid (deflated 3%)\n",
            "  adding: results/history/e400/random_vectors3.mid (deflated 3%)\n",
            "  adding: results/history/e400/random_vectors9.mid (deflated 3%)\n",
            "  adding: results/history/e400/random_vectors4.mid (deflated 3%)\n",
            "  adding: results/history/e400/random_vectors6.mid (deflated 3%)\n",
            "  adding: results/history/e400/random_vectors7.mid (deflated 3%)\n",
            "  adding: results/history/e400/random_vectors8.mid (deflated 3%)\n",
            "  adding: results/history/e400/latent_stds.npy (deflated 14%)\n",
            "  adding: results/history/e400/random_vectors1.mid (deflated 3%)\n",
            "  adding: results/history/e400/random_vectors2.mid (deflated 3%)\n",
            "  adding: results/history/e400/random_vectors0.mid (deflated 3%)\n",
            "  adding: results/history/e400/latent_means.png (deflated 14%)\n",
            "  adding: results/history/e700/ (stored 0%)\n",
            "  adding: results/history/e700/random_vectors5.mid (deflated 3%)\n",
            "  adding: results/history/e700/test/ (stored 0%)\n",
            "  adding: results/history/e700/test/s0.png (deflated 26%)\n",
            "  adding: results/history/e700/test/s13.png (deflated 21%)\n",
            "  adding: results/history/e700/test/s5.png (deflated 27%)\n",
            "  adding: results/history/e700/test/s12.png (deflated 26%)\n",
            "  adding: results/history/e700/test/s6.png (deflated 21%)\n",
            "  adding: results/history/e700/test/s9.png (deflated 22%)\n",
            "  adding: results/history/e700/test/s2.png (deflated 21%)\n",
            "  adding: results/history/e700/test/s3.png (deflated 22%)\n",
            "  adding: results/history/e700/test/s4.png (deflated 26%)\n",
            "  adding: results/history/e700/test/s1.png (deflated 21%)\n",
            "  adding: results/history/e700/test/s10.png (deflated 22%)\n",
            "  adding: results/history/e700/test/s15.png (deflated 22%)\n",
            "  adding: results/history/e700/test/s7.png (deflated 21%)\n",
            "  adding: results/history/e700/test/s14.png (deflated 21%)\n",
            "  adding: results/history/e700/test/s11.png (deflated 20%)\n",
            "  adding: results/history/e700/test/s8.png (deflated 23%)\n",
            "  adding: results/history/e700/latent_pca_values.npy (deflated 5%)\n",
            "  adding: results/history/e700/latent_stds.png (deflated 17%)\n",
            "  adding: results/history/e700/latent_pca_values.png (deflated 17%)\n",
            "  adding: results/history/e700/latent_means.npy (deflated 5%)\n",
            "  adding: results/history/e700/latent_pca_vectors.npy (deflated 4%)\n",
            "  adding: results/history/e700/test.mid (deflated 3%)\n",
            "  adding: results/history/e700/random_vectors3.mid (deflated 3%)\n",
            "  adding: results/history/e700/random_vectors9.mid (deflated 3%)\n",
            "  adding: results/history/e700/random_vectors4.mid (deflated 3%)\n",
            "  adding: results/history/e700/random_vectors6.mid (deflated 3%)\n",
            "  adding: results/history/e700/random_vectors7.mid (deflated 3%)\n",
            "  adding: results/history/e700/random_vectors8.mid (deflated 3%)\n",
            "  adding: results/history/e700/latent_stds.npy (deflated 14%)\n",
            "  adding: results/history/e700/random_vectors1.mid (deflated 3%)\n",
            "  adding: results/history/e700/random_vectors2.mid (deflated 3%)\n",
            "  adding: results/history/e700/random_vectors0.mid (deflated 3%)\n",
            "  adding: results/history/e700/latent_means.png (deflated 13%)\n",
            "  adding: results/history/e350/ (stored 0%)\n",
            "  adding: results/history/e350/random_vectors5.mid (deflated 3%)\n",
            "  adding: results/history/e350/test/ (stored 0%)\n",
            "  adding: results/history/e350/test/s0.png (deflated 21%)\n",
            "  adding: results/history/e350/test/s13.png (deflated 22%)\n",
            "  adding: results/history/e350/test/s5.png (deflated 22%)\n",
            "  adding: results/history/e350/test/s12.png (deflated 22%)\n",
            "  adding: results/history/e350/test/s6.png (deflated 21%)\n",
            "  adding: results/history/e350/test/s9.png (deflated 22%)\n",
            "  adding: results/history/e350/test/s2.png (deflated 21%)\n",
            "  adding: results/history/e350/test/s3.png (deflated 21%)\n",
            "  adding: results/history/e350/test/s4.png (deflated 21%)\n",
            "  adding: results/history/e350/test/s1.png (deflated 23%)\n",
            "  adding: results/history/e350/test/s10.png (deflated 22%)\n",
            "  adding: results/history/e350/test/s15.png (deflated 20%)\n",
            "  adding: results/history/e350/test/s7.png (deflated 20%)\n",
            "  adding: results/history/e350/test/s14.png (deflated 20%)\n",
            "  adding: results/history/e350/test/s11.png (deflated 20%)\n",
            "  adding: results/history/e350/test/s8.png (deflated 22%)\n",
            "  adding: results/history/e350/latent_pca_values.npy (deflated 5%)\n",
            "  adding: results/history/e350/latent_stds.png (deflated 16%)\n",
            "  adding: results/history/e350/latent_pca_values.png (deflated 19%)\n",
            "  adding: results/history/e350/latent_means.npy (deflated 5%)\n",
            "  adding: results/history/e350/latent_pca_vectors.npy (deflated 4%)\n",
            "  adding: results/history/e350/test.mid (deflated 3%)\n",
            "  adding: results/history/e350/random_vectors3.mid (deflated 3%)\n",
            "  adding: results/history/e350/random_vectors9.mid (deflated 3%)\n",
            "  adding: results/history/e350/random_vectors4.mid (deflated 3%)\n",
            "  adding: results/history/e350/random_vectors6.mid (deflated 3%)\n",
            "  adding: results/history/e350/random_vectors7.mid (deflated 3%)\n",
            "  adding: results/history/e350/random_vectors8.mid (deflated 3%)\n",
            "  adding: results/history/e350/latent_stds.npy (deflated 14%)\n",
            "  adding: results/history/e350/random_vectors1.mid (deflated 3%)\n",
            "  adding: results/history/e350/random_vectors2.mid (deflated 3%)\n",
            "  adding: results/history/e350/random_vectors0.mid (deflated 3%)\n",
            "  adding: results/history/e350/latent_means.png (deflated 13%)\n",
            "  adding: results/history/e800/ (stored 0%)\n",
            "  adding: results/history/e800/random_vectors5.mid (deflated 3%)\n",
            "  adding: results/history/e800/test/ (stored 0%)\n",
            "  adding: results/history/e800/test/s0.png (deflated 23%)\n",
            "  adding: results/history/e800/test/s13.png (deflated 21%)\n",
            "  adding: results/history/e800/test/s5.png (deflated 22%)\n",
            "  adding: results/history/e800/test/s12.png (deflated 21%)\n",
            "  adding: results/history/e800/test/s6.png (deflated 22%)\n",
            "  adding: results/history/e800/test/s9.png (deflated 21%)\n",
            "  adding: results/history/e800/test/s2.png (deflated 21%)\n",
            "  adding: results/history/e800/test/s3.png (deflated 20%)\n",
            "  adding: results/history/e800/test/s4.png (deflated 22%)\n",
            "  adding: results/history/e800/test/s1.png (deflated 21%)\n",
            "  adding: results/history/e800/test/s10.png (deflated 21%)\n",
            "  adding: results/history/e800/test/s15.png (deflated 21%)\n",
            "  adding: results/history/e800/test/s7.png (deflated 21%)\n",
            "  adding: results/history/e800/test/s14.png (deflated 21%)\n",
            "  adding: results/history/e800/test/s11.png (deflated 21%)\n",
            "  adding: results/history/e800/test/s8.png (deflated 22%)\n",
            "  adding: results/history/e800/latent_pca_values.npy (deflated 5%)\n",
            "  adding: results/history/e800/latent_stds.png (deflated 16%)\n",
            "  adding: results/history/e800/latent_pca_values.png (deflated 16%)\n",
            "  adding: results/history/e800/latent_means.npy (deflated 6%)\n",
            "  adding: results/history/e800/latent_pca_vectors.npy (deflated 4%)\n",
            "  adding: results/history/e800/test.mid (deflated 31%)\n",
            "  adding: results/history/e800/random_vectors3.mid (deflated 3%)\n",
            "  adding: results/history/e800/random_vectors9.mid (deflated 3%)\n",
            "  adding: results/history/e800/random_vectors4.mid (deflated 3%)\n",
            "  adding: results/history/e800/random_vectors6.mid (deflated 3%)\n",
            "  adding: results/history/e800/random_vectors7.mid (deflated 3%)\n",
            "  adding: results/history/e800/random_vectors8.mid (deflated 3%)\n",
            "  adding: results/history/e800/latent_stds.npy (deflated 14%)\n",
            "  adding: results/history/e800/random_vectors1.mid (deflated 3%)\n",
            "  adding: results/history/e800/random_vectors2.mid (deflated 3%)\n",
            "  adding: results/history/e800/random_vectors0.mid (deflated 3%)\n",
            "  adding: results/history/e800/latent_means.png (deflated 13%)\n",
            "  adding: results/history/e1100/ (stored 0%)\n",
            "  adding: results/history/e1100/random_vectors5.mid (deflated 3%)\n",
            "  adding: results/history/e1100/test/ (stored 0%)\n",
            "  adding: results/history/e1100/test/s0.png (deflated 23%)\n",
            "  adding: results/history/e1100/test/s13.png (deflated 23%)\n",
            "  adding: results/history/e1100/test/s5.png (deflated 21%)\n",
            "  adding: results/history/e1100/test/s12.png (deflated 22%)\n",
            "  adding: results/history/e1100/test/s6.png (deflated 23%)\n",
            "  adding: results/history/e1100/test/s9.png (deflated 21%)\n",
            "  adding: results/history/e1100/test/s2.png (deflated 22%)\n",
            "  adding: results/history/e1100/test/s3.png (deflated 22%)\n",
            "  adding: results/history/e1100/test/s4.png (deflated 23%)\n",
            "  adding: results/history/e1100/test/s1.png (deflated 22%)\n",
            "  adding: results/history/e1100/test/s10.png (deflated 24%)\n",
            "  adding: results/history/e1100/test/s15.png (deflated 22%)\n",
            "  adding: results/history/e1100/test/s7.png (deflated 22%)\n",
            "  adding: results/history/e1100/test/s14.png (deflated 22%)\n",
            "  adding: results/history/e1100/test/s11.png (deflated 22%)\n",
            "  adding: results/history/e1100/test/s8.png (deflated 23%)\n",
            "  adding: results/history/e1100/latent_pca_values.npy (deflated 4%)\n",
            "  adding: results/history/e1100/latent_stds.png (deflated 16%)\n",
            "  adding: results/history/e1100/latent_pca_values.png (deflated 15%)\n",
            "  adding: results/history/e1100/latent_means.npy (deflated 5%)\n",
            "  adding: results/history/e1100/latent_pca_vectors.npy (deflated 4%)\n",
            "  adding: results/history/e1100/test.mid (deflated 75%)\n",
            "  adding: results/history/e1100/random_vectors3.mid (deflated 3%)\n",
            "  adding: results/history/e1100/random_vectors9.mid (deflated 3%)\n",
            "  adding: results/history/e1100/random_vectors4.mid (deflated 3%)\n",
            "  adding: results/history/e1100/random_vectors6.mid (deflated 3%)\n",
            "  adding: results/history/e1100/random_vectors7.mid (deflated 3%)\n",
            "  adding: results/history/e1100/random_vectors8.mid (deflated 3%)\n",
            "  adding: results/history/e1100/latent_stds.npy (deflated 14%)\n",
            "  adding: results/history/e1100/random_vectors1.mid (deflated 3%)\n",
            "  adding: results/history/e1100/random_vectors2.mid (deflated 3%)\n",
            "  adding: results/history/e1100/random_vectors0.mid (deflated 3%)\n",
            "  adding: results/history/e1100/latent_means.png (deflated 12%)\n",
            "  adding: results/history/e1600/ (stored 0%)\n",
            "  adding: results/history/e1600/random_vectors5.mid (deflated 3%)\n",
            "  adding: results/history/e1600/test/ (stored 0%)\n",
            "  adding: results/history/e1600/test/s0.png (deflated 24%)\n",
            "  adding: results/history/e1600/test/s13.png (deflated 23%)\n",
            "  adding: results/history/e1600/test/s5.png (deflated 24%)\n",
            "  adding: results/history/e1600/test/s12.png (deflated 23%)\n",
            "  adding: results/history/e1600/test/s6.png (deflated 22%)\n",
            "  adding: results/history/e1600/test/s9.png (deflated 23%)\n",
            "  adding: results/history/e1600/test/s2.png (deflated 23%)\n",
            "  adding: results/history/e1600/test/s3.png (deflated 22%)\n",
            "  adding: results/history/e1600/test/s4.png (deflated 24%)\n",
            "  adding: results/history/e1600/test/s1.png (deflated 23%)\n",
            "  adding: results/history/e1600/test/s10.png (deflated 24%)\n",
            "  adding: results/history/e1600/test/s15.png (deflated 23%)\n",
            "  adding: results/history/e1600/test/s7.png (deflated 23%)\n",
            "  adding: results/history/e1600/test/s14.png (deflated 24%)\n",
            "  adding: results/history/e1600/test/s11.png (deflated 24%)\n",
            "  adding: results/history/e1600/test/s8.png (deflated 22%)\n",
            "  adding: results/history/e1600/latent_pca_values.npy (deflated 4%)\n",
            "  adding: results/history/e1600/latent_stds.png (deflated 14%)\n",
            "  adding: results/history/e1600/latent_pca_values.png (deflated 16%)\n",
            "  adding: results/history/e1600/latent_means.npy (deflated 5%)\n",
            "  adding: results/history/e1600/latent_pca_vectors.npy (deflated 4%)\n",
            "  adding: results/history/e1600/test.mid (deflated 37%)\n",
            "  adding: results/history/e1600/random_vectors3.mid (deflated 3%)\n",
            "  adding: results/history/e1600/random_vectors9.mid (deflated 3%)\n",
            "  adding: results/history/e1600/random_vectors4.mid (deflated 3%)\n",
            "  adding: results/history/e1600/random_vectors6.mid (deflated 3%)\n",
            "  adding: results/history/e1600/random_vectors7.mid (deflated 3%)\n",
            "  adding: results/history/e1600/random_vectors8.mid (deflated 3%)\n",
            "  adding: results/history/e1600/latent_stds.npy (deflated 13%)\n",
            "  adding: results/history/e1600/random_vectors1.mid (deflated 3%)\n",
            "  adding: results/history/e1600/random_vectors2.mid (deflated 3%)\n",
            "  adding: results/history/e1600/random_vectors0.mid (deflated 3%)\n",
            "  adding: results/history/e1600/latent_means.png (deflated 13%)\n",
            "  adding: results/retrained_feedforward_trained.mid (deflated 65%)\n",
            "  adding: results/02_random.mid (deflated 64%)\n",
            "  adding: results/retrained_feedforward_training_results.pdf (deflated 29%)\n",
            "  adding: results/feedforward_net_track_intervals_untrained.csv (deflated 77%)\n",
            "  adding: results/retrained_feedforward_snapshot.csv (deflated 80%)\n",
            "  adding: results/01_first_prediction_average.mid (deflated 77%)\n",
            "  adding: results/RNN_single_best.mid (deflated 57%)\n",
            "  adding: results/RNN_single_random.mid (deflated 34%)\n",
            "  adding: results/feedforward_net_track_intervals_snapshot.mp3 (deflated 5%)\n",
            "  adding: results/feedforward_trained.pth (deflated 9%)\n",
            "  adding: results/retrained_feedforward_untrained.mid (deflated 53%)\n",
            "  adding: results/retrained_feedforward_snapshot.pth (deflated 9%)\n",
            "  adding: results/retrained_feedforward_trained.csv (deflated 81%)\n",
            "  adding: results/feedforward_net_track_intervals_untrained.mp3 (deflated 7%)\n",
            "  adding: results/RNN_harmonization_early.mid (deflated 62%)\n",
            "  adding: results/feedforward_net_track.mid (deflated 75%)\n",
            "  adding: results/RNN_harmonization_real.mid (deflated 62%)\n",
            "  adding: results/feedforward_net_track_intervals_untrained.mid (deflated 48%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCvdYfV8EAX4",
        "colab_type": "code",
        "outputId": "4cf53467-3a8c-4d81-f0cf-489cad504aaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!cp results.zip \"drive/My Drive/music_results\" -rv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'results.zip' -> 'drive/My Drive/music_results/results.zip'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghrV1HApXlX8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oH9Iqhrq-wAw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp results/history/e2000 \"drive/My Drive/music_results/results/history/\" -r"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Wn1RPwfEDzh",
        "colab_type": "code",
        "outputId": "88a08c71-d380-4e9c-a5b3-d7027abb1ea3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!cp results/history/model.h5 \"drive/My Drive/music_results/results/history\" -v"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: cannot stat 'results/history/model.h5': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nt1ddbSFEOPd",
        "colab_type": "code",
        "outputId": "a8f15146-b2e1-4e64-eb8e-59791dcef895",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "!cp results/history/encoder.pkl \"drive/My Drive/music_results/results/history\" -v"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'results/history/encoder.pkl' -> 'drive/My Drive/music_results/results/history/encoder.pkl'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzfSkdqvYBXT",
        "colab_type": "code",
        "outputId": "aa0066b3-0f1d-4e04-abb8-6a26a512d0bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "!cp results/history/decoder.pkl \"drive/My Drive/music_results/results/history\" -v"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'results/history/decoder.pkl' -> 'drive/My Drive/music_results/results/history/decoder.pkl'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AERHLdkNYD8r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDJa36rjb_7g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp results \"drive/My Drive/music_\" -r"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}