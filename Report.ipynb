{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group project for *Advanced topics in machine learning* lecture (2019)\n",
    "### Benjamin Ellenberger, Nicolas Deperrois, Laura Kriener"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chosen Topic: **Music generation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation and State-of-the-Art\n",
    "\n",
    "The topics for the group project work was chosen in the very beginning of the course.\n",
    "As it was announced that the course will nearly always use images for demonstrations, examples and excercises, we decided that our project should be on a different form of data.\n",
    "We decided to work in the broad field of music generation with deep learing.\n",
    "\n",
    "To narrow down the topic we investigated what the state of the art in this field is.\n",
    "The most impressive recent results are produced by Google and OpenAI.\n",
    "The [Google Magenta project](https://magenta.tensorflow.org/) covers a wide range of applications such as harmonization, drum-machines and a music generating network using the transformer network architecture with attention.\n",
    "\n",
    "An other very recent result in the field of generating music was published by OpenAI. The [MuseNet](https://openai.com/blog/musenet/) uses the recently published GPT2-architecture which is a large-scale transformer network as well. \n",
    "\n",
    "The Google and OpenAI approaches as well as other (less famous) approaches have in common, that they employ very complicated network architectures in combination with the use of immense computational resources.\n",
    "\n",
    "As the required computational power is far out of our reach, we wondered if this level of complexity is really unavoidable.\n",
    "And so the question **How much can you do with how little?** became the leading theme for our project.\n",
    "We want to see, what results can be achieved using much simpler network architecutres (i.e. architectures within the scope of the lecture)?\n",
    "Which aspects of music generation can be achieved and which have to be ignored? For example can you generate a resonable melody line without considering the rhythm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges\n",
    "The main challenge is that music generation is a very broad topic. Before we can even start we have to answer a couple of questions:\n",
    "\n",
    "- What exactly do we want to generate? Melody? Rhythm? Harmony?\n",
    "\n",
    "This will depend on the network structures we try out. For example for simple feed-forward networks we only focus on melody, while we include rhythm in the LSTMs.\n",
    "\n",
    "- How to feed music into a network? Spectrum? Pitches? Intervals?\n",
    "\n",
    "We will not use spectrum or audio-data, instead we will work with pitches and intervals and note-lengths extracted from midi-files.\n",
    "\n",
    "- What kind of music?\n",
    "\n",
    "As we try to keep things as simple as possible, we decided to use the widely used bach chorale data-set (see `data/raw/bach`).\n",
    "\n",
    "- How do we evaluate the result?\n",
    "\n",
    "Music and the quality of music is a highly subjective topic. It is very difficult to find a metric that evaluates how good a produced piece of music is.\n",
    "We plan to evaluate the pieces of music produced by the different network architectures by comparison and by the networks ability to capture different aspects of music (e.g. if one architecture can only produce melodies without considering rhythm and the other includes rhythm the second one is better suited for music generation). Additionally a measure for quality can be the similarity to the music style it was trained on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data sets\n",
    "\n",
    "We will work on the Bach chorale dataset which we included in the repository (see `data/raw/bach`).\n",
    "Midi-files are binary, therefore it is difficult to modify and create them directly.\n",
    "We are using the python libaries `pianoroll` and `midicsv` which translates a midi-file into a human-readable (and modifiable) csv-string.\n",
    "From this we have written our own utility functions that allow us to extract high-level information about the music (e.g. tempo, tonality) and perform changes to the tracks and write them back to a midi file. The utility functions are located in the file `src/midi_utils.py`. A demonstration on how to use them can be found in the notebook `demo_midi_utils.ipynb`. The functions to create a Pytorch compatible dataset from the midi-data are in the file `src/dataset_utils.py`.\n",
    "\n",
    "******\n",
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Feed-Forward Networks**\n",
    "\n",
    "Starting from the simplest possible network architecture, we will first investigate the potential and limitations of feed-forward networks.\n",
    "Their architecture is not really suited to deal with time-series data, because one input sample is presented as a whole and at the same time to the network.\n",
    "Also there is no mechanism for memory of the past.\n",
    "Respecting these limitations, we simplify the task of \"music generation\" to **predicting the next note in a melody given the $n$ previous notes** (we only predict the pitch not the length of notes).\n",
    "\n",
    "### Generation mechanism\n",
    "\n",
    "This setup also allows for a generation mechanism for a longer melody as sketched in the figure below:\n",
    "We start the network of with a set of notes from a melody ($n1$ to $n10$) as input. \n",
    "The network then predicts the next note for the melody ($n11$). In the next step we give the network a new set of input which consists of the notes $n2$ to $n10$ **and** the note it predicted before $n11$. The network now predicts the twelth note ($n12$) which is then used in the input for the next round.\n",
    "\n",
    "The main goal of this generation task is to produce a series of notes which is not dissonant and stays in the tonality of the given starter notes.\n",
    "\n",
    "\n",
    "![Schematic drawing of generation procedure](graphics/forward3.png)\n",
    "\n",
    "\n",
    "### Network structure\n",
    "\n",
    "The figure below shows a sketch of the feed-forward architecture that produced the best results.\n",
    "Note that the inputs for the network are not the pitches of the starter-notes but the intervals between them.\n",
    "This is beneficial, as using the intervals automatically provides a form of normalization.\n",
    "The use of intervals for example removes for example the dependence on the key the melody is written in.\n",
    "For example the line C-D-E-F (C-Major) will produce `[2, 2, 2, 1]` (counting the intervals in half-tone steps).\n",
    "Even though the line D-E-F#-G (D-Major) is in a different key, it will produce the same intervals `[2, 2, 2, 1]`.\n",
    "\n",
    "The input layer of the network contains 10 units, each coding for one of the ten starter-intervals.\n",
    "The input is connected to two hidden layer with 128 units each.\n",
    "The output layer has 25 units. Each is coding for one possible interval (step-size: half-tones) in the range of $\\pm$ 1 Octave.\n",
    "We train the network by presenting it with 10-intervals pieces of the bach dataset and let it predict the 11th interval.\n",
    "The loss is calculated using the CrossEntropyLoss-function in pyTorch.\n",
    "In early experiments the output layer contained only one unit which was used to predict the pitch/interval value. \n",
    "The results were much worse compared to predicting the probability for each possible interval. \n",
    "In the generation mode of the network the interval with the highest probability is picked as the \"chosen interval\" by the network.\n",
    "\n",
    "![Feed-forward architecture](graphics/forward2.png)\n",
    "\n",
    "### Results\n",
    "\n",
    "![Feed-forward architecture](graphics/loss_forward_intervals.png)\n",
    "\n",
    "The figure above shows the training process of the described network architecture.\n",
    "We see a clear occurance of overfitting, as the loss on the validation dataset first plateaus and then increases while the loss on the training data continues to decrease.\n",
    "The 3 stars in the plot mark where snapshots of the network model were taken during training and saved for the generation of melodies.\n",
    "The notebook cells below show an exemplary melody generated by an untrained network, by the network at the end of the plateau in the validation loss and at the end of training, when the validation loss has already increased again.\n",
    "For the generation of the audio samples below the same 10 starter-notes were used.\n",
    "They are included in the audio file.\n",
    "\n",
    "\n",
    "#### **Alternative architectures / training methods and their impact**\n",
    "In the following we quickly describe some different variations of the described network architecture and why they were dismissed in favor of the one presented above.\n",
    "\n",
    "- more neurons/layers: lead to worse overfitting without increasing performance significantly\n",
    "- less neurons/layers: delay the overfitting but decrease performance\n",
    "- regularization methods: only delay overfitting while decreasing performance\n",
    "- more starter-notes (i.e. larger input layer): worse overfitting\n",
    "- less starter-notes (everything below 10): decreases overfitting but during generation the network will just produce a repeated loop of the starter-notes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audio sample of untrained network\n",
    "We had to stop the generation of this melody early because the notes predicted by the network drifted out of the available range in midi-files.\n",
    "The untrained network clearly does not stay in a fairly small pitch range (as Bach melodies usually do) and produces lines that sound unharmonic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "\n",
    "filename = 'results/feedforward_net_track_intervals_untrained.mid'\n",
    "pygame.init()\n",
    "pygame.mixer.music.load(filename)\n",
    "pygame.mixer.music.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.mixer.music.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audio sample of the network at the end of training\n",
    "The melody produced by this network stays within a reasonable pitch range of the starter notes.\n",
    "It produces a harmonic sounding line of notes and seems to stay within the tonality of the starter notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "\n",
    "filename = 'results/feedforward_net_track_intervals.mid'\n",
    "pygame.init()\n",
    "pygame.mixer.music.load(filename)\n",
    "pygame.mixer.music.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.mixer.music.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audio sample of network before severe overfitting\n",
    "The melody produced by this network stays within a reasonable pitch range of the starter notes.\n",
    "It produces a mostly harmonic sounding line of notes and mostly seems to stay within the tonality of the starter notes.\n",
    "There are some questionable choices of notes in the melody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "\n",
    "filename = 'results/feedforward_net_track_intervals_snapshot.mid'\n",
    "pygame.init()\n",
    "pygame.mixer.music.load(filename)\n",
    "pygame.mixer.music.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.mixer.music.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion of results\n",
    "Bearing in mind that we chose the feed-forward architecture as the most simple model, we can be content with the result. \n",
    "It was clear from the start that the feed-forward networks would only be able to learn a very restricted task, i.e. no rhythm and chords.\n",
    "Therefore we chose only to try to learn to produce short melody patches. This we were able to achieve.\n",
    "The overfitting which occured is a problem, that might be alieviated by a larger dataset. However, the larger sets available do contain different styles of music than Bach chorales which we deliberatly chose for their simple melody lines.\n",
    "\n",
    "We see that both trained snapshots of the network produce a melody that stays within a reasonable range of notes and does not drift off like the untrained version.\n",
    "Also they produce lines of notes that are consistent with the harmony style by Bach and they mostly stay within the tonality.\n",
    "\n",
    "One could argue that using the network which was trained with heavy overfitting is ok for music generation (but not for predicting the next note of a melody line), because it effectively does what a human composer does: It \"knows\" short patches of melody from the training set (i.e. the music it already knows) and reuses parts of these melody patches to generate new melodies. Human composes do this as well (a lot). For example, someone who knows a couple of Hans Zimmer movie soundtracks will very quickly recognize if a new soundtrack is written by Hans Zimmer, because he very often reuses certain themes and instrumentation schemes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplary usage of the code\n",
    "#### Training the network model and generating with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import src.feed_forward_utils as ff\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "try:\n",
    "    home_dir\n",
    "except NameError:\n",
    "    home_dir = os.getcwd()\n",
    "os.chdir(home_dir)\n",
    "\n",
    "data_dir = '/data/raw/bach'\n",
    "\n",
    "minibatch_size = 32\n",
    "train_valid_ratio = 0.8        # size ratio of the train- and validation dataset\n",
    "feature_qty = 10               # how many starter-intervals are presented to network\n",
    "prediction_qty = 1             # must be one if intervals are used\n",
    "interval_range = [-12, 12]     # in halftones -> 1 octave\n",
    "snapshot_epoch = 50            # epoch at which a snapshot of the model is created during training\n",
    "n_epochs = 100\n",
    "learning_rate = 0.001\n",
    "hidden_size = 128\n",
    "\n",
    "tracks = ff.load_tracks(data_dir, 1)\n",
    "train_set, valid_set, train_loader, valid_loader = ff.generate_dataloaders(tracks, minibatch_size, train_valid_ratio, \n",
    "                                                                           feature_qty, prediction_qty, interval_range)\n",
    "input_size = len(train_set[0][0])   # get input size\n",
    "input_example = train_set[0][0]\n",
    "output_size = len(range(interval_range[0], interval_range[1]+1, 1))     # get output size\n",
    "output_example = train_set[0][1]\n",
    "print(\"Training set size\", len(train_set))\n",
    "print(\"Input size {}/ output size {}/ learning rate {}\".format(input_size, output_size, learning_rate))\n",
    "print(\"Input example {}\".format(input_example))\n",
    "print(\"Output example (idx of next interval) {}\".format(output_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "forward_model = ff.FeedForward(input_size, hidden_size, output_size)\n",
    "untrained_model = copy.deepcopy(forward_model)\n",
    "untrained_model = untrained_model.to(device)\n",
    "forward_model = forward_model.to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(forward_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, val_losses, snapshot_model = ff.train(forward_model, train_loader, valid_loader, \n",
    "                                                    optimizer, n_epochs, criterion, device=device, \n",
    "                                                    verbose=True, snapshot=snapshot_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(home_dir)\n",
    "result_dir = home_dir + '/results/'\n",
    "result_name = 'retrained_feedforward_'\n",
    "ff.save_results(result_dir, result_name, untrained_model, snapshot_model, \n",
    "                forward_model, train_losses, val_losses, snapshot_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = 45\n",
    "predict_length = 30\n",
    "\n",
    "filename_untrained = result_dir + result_name + 'untrained.mid'\n",
    "ff.generate_melody(device, valid_set, sample_idx, untrained_model, predict_length, interval_range, filename_untrained)\n",
    "\n",
    "filename_snapshot = result_dir + result_name + 'snapshot.mid'\n",
    "ff.generate_melody(device, valid_set, sample_idx, snapshot_model, predict_length, interval_range, filename_snapshot)\n",
    "\n",
    "filename_trained = result_dir + result_name + 'trained.mid'\n",
    "ff.generate_melody(device, valid_set, sample_idx, forward_model, predict_length, interval_range, filename_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "\n",
    "filename = filename_trained # or filename_trained or filename_snapshot\n",
    "pygame.init()\n",
    "pygame.mixer.music.load(filename)\n",
    "pygame.mixer.music.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.mixer.music.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the pretrained model and generating with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import src.feed_forward_utils as ff\n",
    "\n",
    "try:\n",
    "    home_dir\n",
    "except NameError:\n",
    "    home_dir = os.getcwd()\n",
    "os.chdir(home_dir)\n",
    "\n",
    "data_dir = '/data/raw/bach'\n",
    "\n",
    "minibatch_size = 32\n",
    "train_valid_ratio = 0.8        # size ratio of the train- and validation dataset\n",
    "feature_qty = 10               # how many starter-intervals are presented to network\n",
    "prediction_qty = 1             # must be one if intervals are used\n",
    "interval_range = [-12, 12]     # in halftones -> 1 octave\n",
    "snapshot_epoch = 50            # epoch at which a snapshot of the model is created during training\n",
    "\n",
    "tracks = ff.load_tracks(data_dir, 1)\n",
    "train_set, valid_set, train_loader, valid_loader = ff.generate_dataloaders(tracks, minibatch_size, train_valid_ratio, \n",
    "                                                                           feature_qty, prediction_qty, interval_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(home_dir)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_filename = 'results/feedforward_trained.pth'\n",
    "model_loaded = torch.load(model_filename)\n",
    "model_loaded.to(device)\n",
    "model_loaded.eval()\n",
    "\n",
    "sample_idx = 42\n",
    "predict_length = 25\n",
    "result_dir = 'results/'\n",
    "result_name = 'loaded_feedforward_'\n",
    "filename_loaded = result_dir + result_name + 'generating.mid'\n",
    "ff.generate_melody(device, valid_set, sample_idx, model_loaded, predict_length, interval_range, filename_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "\n",
    "pygame.init()\n",
    "pygame.mixer.music.load(filename_loaded)\n",
    "pygame.mixer.music.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.mixer.music.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*********\n",
    "*********\n",
    "\n",
    "# **LSTMs** - including rhythms and generate harmonization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will try to generate melodies from the same dataset (Bach Chorals) using recurrent neural networks (RNN) with long short term memory units (LSTM) as in the tutorial 08 of the class, based on the two following blog posts:\n",
    "\n",
    "- http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "- https://towardsdatascience.com/writing-like-shakespeare-with-machine-learning-in-pytorch-d77f851d910c\n",
    "\n",
    "The idea would be to extend the character-to-character LSTM network to midi files. We will use another representation of the data, named 'pianorolls', where a single melody is stored in an array containing the melody pitches across time. This is one of the most frequent representations used, although it has some limitations. For instance, there is no way to differentiate between a long note (1 half-note) and 4 short-ones (4 eight-notes). However, it brings a considerable advantage compared to the previous used representations as it can store rhythm and silences, depending on the chosen time-step. \n",
    "\n",
    "Moreover, each possible note is considered as a distinct element of a vocabulary. We will then use $N$ input nodes, where $N$ is the size of the vocabulary (i.e., the number of distinct notes, including silences). \n",
    "Thus, these pianorolls arrays will be converted into one-hot vectors containing the corresponding pitch (or silence, represented by 0) for each time step, and will be fed to the RNN network defined below. \n",
    "\n",
    "All methods for this part are stored in the python module `src.RNN_utils`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a single melody with LSTMs\n",
    "\n",
    "### Data representation\n",
    "We use the pypianoroll library - https://salu133445.github.io/pypianoroll/ - to convert a midi file into a multitrack object, get the soprano track, transpose it to C major, create a new multitrack object out of it, and finally write it to a new midi file. \n",
    "We first import midi files, choose a voice, and concatenate all melodies into a big sequence of notes. We also calculate the pitch ranges. In order to minimize the size of one-hot encode vectors, we restrict the pitches to values next to 0, using the minimum pitch found in the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network structure: LSTM network as for note-to-note melody generation\n",
    "We now build a network to implement note-to-note melody generation using LSTMs units **without** drop-out of the output. This network is highly inspired from the Tutorial 08 of the class, by replacing characters by notes. \n",
    "\n",
    "For training the model, we declare a function, where we define an optimizer (Adam) and loss (cross entropy). Then, training and validation data are separated and the hidden state of the RNN is initaliazed. Looping over the training melody batches, we use the functions `get_pianoroll_batches` and `one_hot_encode_batch` in order to build batches and feed them to the network input. This function is also adapted to harmonization that will be defined below. Every once a while, we generate some loss statistics (training loss and validation loss) to let us know if the model is training correctly.\n",
    "\n",
    "As a first attempt to generate a melody using LSTMs with pianorolls, we will simply try to generate a single voice (soprano, alto, tenor or bass), training the network on the whole midi file dataset concatenated into a long sequence of notes all translated to the same tonality (here, C Major or its relative A minor). The later operation will particularly facilitate learning on such a small dataset as all melodies will be rescaled to the same tonality, thus preventing abrupt changes among melody batches. \n",
    "\n",
    "### Results \n",
    "In order to evaluate the training efficiency of the note-to-note generating task with LSTMs, we plot the evolution of training and validation losses across epochs. As in the previous section, the 3 stars in the plot represent snapshots of the LSTM model taken during training and saved for the generation of melodies. Interestingly, we can also notice a similar evolution of the validation loss, first decreasing, then platauing and finally increasing again, illustrating considerable overfitting. \n",
    "![RNN-architecture](graphics/RNN_single_plot.png)\n",
    "\n",
    "In the cells below, we present generated melodies for the selected voice using the NoteRNN network at different stages of learning: at the beginning (random), at early-stopping (early) and at the end of learning (overfit). As in text generation, a starting sequence of 4 eighth notes is imposed to the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audio sample of untrained network\n",
    "Let's first try melody generation with a random initial network (no training). \n",
    "As expected, it does not sound like a Bach melody, as it is totally out of any possible key. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import os\n",
    "import glob\n",
    "try:\n",
    "    home_dir\n",
    "except NameError:\n",
    "    home_dir = os.getcwd()\n",
    "os.chdir(home_dir + \"/data/raw/bach\")  # go to a folder relative to home dir\n",
    "\n",
    "filename = home_dir + \"/results/RNN_single_random.mid\"\n",
    "pygame.init()\n",
    "pygame.mixer.music.load(filename)\n",
    "pygame.mixer.music.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.mixer.music.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audio sample of network before severe overfitting\n",
    "The melody produced by this network is quite coherent as it catches a certain tonality and respects the associated harmony, most of the time C Major, as imposed by the starting note. In addition to the feedforward case, it also includes rhythm, even if most of the time quarter and half notes are produced, while it could also generate eigth notes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = home_dir + \"/results/RNN_single_early.mid\"\n",
    "pygame.init()\n",
    "pygame.mixer.music.load(filename)\n",
    "pygame.mixer.music.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.mixer.music.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audio sample of network at the end of training\n",
    "As in the previous case, the melody produced by the network at the end of training with severe overfitting is totally in the pitch range of the initial tonality, and rhythms choice are a bit more coherent than in the previous case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = home_dir + \"/results/RNN_single_overfit.mid\"\n",
    "pygame.init()\n",
    "pygame.mixer.music.load(filename)\n",
    "pygame.mixer.music.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.mixer.music.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion of the results \n",
    "As in the feedforward case, we obtain a typical evolution of the validation loss which triggered the interest to qualitatively evaluate the results at 3 essential stages of learning. Surprisingly, we obtain similar results than with feedforward network in terms of pitch and tonality, but using pianorolls enabled us to also generate rhythms in the melody, even if they are quite slow and do not fully respect the patterns of a Bach Choral melody.  \n",
    "We can thus conclude that to perform such a single task as generating melodies, the use of the timing component from recurrent networks is not really beneficial, and should be test on more complex tasks as shown below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating harmonization with LSTMs\n",
    "\n",
    "We observed above that it is possible to generate a coherent melody using a slightly modified version of the original character-to-character RNN. Here, we will investigate whether such a network can perform more complex task, such as harmonizing a given voice. For instance, from the soprano sequence, we would like to generate the associated alto voice. \n",
    "\n",
    "### Network structure: LSTM network as for note-to-note melody harmonization\n",
    "\n",
    "In this section, we will use the same RNN network as above, with slight modifications in the learning rate, number of units and batch size. The only noticeable change is that the training process is now specialized to harmonization. Indeed, two long sequences of notes (pianorolls) are now given to the network. The input sequence, from which we want to generate harmonization (in our case, the soprano voice), and the target sequence, that we wish to generate to properly harmonize the input voice. The idea is to keep the note-to-note structure, but instead of predicting the next note a sequence, we predict the note from another voice at the same time step. \n",
    "Three networks were used to perform harmonization of the soprano voice, each corresponding to a given lower voice: alto, tenor and bass. Here again, we recorded the 3 essential learning stages and evaluate them for harmonization. \n",
    "\n",
    "### Results \n",
    "\n",
    "The figure below shows the evolution of training and validation losses across epochs for each harmonizing RNN: alto, tenor and bass. Intertingly, they show a similar shape as for melody generation, all first reaching a plateau and severely overfitting. Furthermore, it looks like training and validation losses adversarial: when the training loss increases again, the validation loss decreases. This shows that overfitting totally prevent the network from generating the original voices. \n",
    "\n",
    "![RNN-architecture-harm](graphics/RNN_harmonization_plot.png)\n",
    "\n",
    "It is now time to test the quality of the RNNs trained to harmonize a soprano voice by generating separately alto, tenor and bass voices. We will test it on one given Bach midi file in the browser. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audio sample of untrained network\n",
    "\n",
    "Let's first generate harmonization with a ramdom initial network, without training. As expected, the harmonization is totally random, thus all voices are not following the soprano melody and generate random melodies independently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = home_dir + \"/results/RNN_harmonization_random.mid\"\n",
    "pygame.init()\n",
    "pygame.mixer.music.load(filename)\n",
    "pygame.mixer.music.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.mixer.music.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audio sample of network before severe overfitting \n",
    "We now test the quality of harmonization on the early-stopped network. \n",
    "The obtained track sounds much better and start to harmonize the soprano voice correctly, especially by trying to match the soprano rhythm. However, it is not optimal as we cannot hear any coherent harmonies and cadences. \n",
    "Let's finally try on the highly-overfitting network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = home_dir + \"/results/RNN_harmonization_early.mid\"\n",
    "pygame.init()\n",
    "pygame.mixer.music.load(filename)\n",
    "pygame.mixer.music.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.mixer.music.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audio sample of network at the end of training\n",
    "\n",
    "The overfitting networks sounds to generate voices that match much better with the soprano voice than the previous networks! Even if at the beginning, the voices are not really coherent, they start to catch some relevant melodies that vertically harmonize the soprano voice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_midi_filename = home_dir + \"/results/RNN_harmonization_overfit.mid\"\n",
    "pygame.init()\n",
    "pygame.mixer.music.load(modified_midi_filename)\n",
    "pygame.mixer.music.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.mixer.music.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion of the results from the overfitting case\n",
    "\n",
    "We hypothesize that such surprising results might be specific to music generation. Bach Chorales are quite similar, especially when transcripted to C Major (or its relative A minor), and overfitting the training set can lead to optimal network features to generate 3 voices on a soprano voice which harmonization was never learned (validation set). Even if the validation error is huge compared to the early stopping case, it does not mean that the generated voices are wrong. The overfitted network generate complete different alto/tenor/bass melodies from the original ones, which might explain this huge validation error, but these melodies vertically chords and cadences that can turn out to be really coherent!\n",
    "As an example, think of the original track ending up with an interrupted cadence, in a total different tonality than C Major (let's say, A Major), which has been seen less often that a perfect cadence (C Major) in the training set. The overfitting network will unless try to recover this perfect cadence as it learned to do by overfitting, and thus generate complete different melodies than the target ones, but vertically coherent with each other to produce such a perfect cadence. \n",
    "\n",
    "As an illustration of such a phenomenon, let's listen to original track (real alto/tenor/bass voices) and compare to the overfitted track at the same time steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_midi_filename = home_dir + \"/results/RNN_harmonization_real.mid\"\n",
    "pygame.init()\n",
    "pygame.mixer.music.load(modified_midi_filename)\n",
    "pygame.mixer.music.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.mixer.music.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplary usage of code for harmonization\n",
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "from pypianoroll import Multitrack, Track\n",
    "import pypianoroll\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "%pylab inline\n",
    "import src.RNN_utils as rnn\n",
    "\n",
    "try:\n",
    "    home_dir\n",
    "except NameError:\n",
    "    home_dir = os.getcwd()\n",
    "os.chdir(home_dir + \"/data/raw/bach\")  # go to a folder relative to home dir\n",
    "\n",
    "beat_resolution = 2\n",
    "# get entire pianoroll sequences for each voice\n",
    "all_pianorolls_soprano, midi_files = rnn.get_all_pianorolls(0, home_dir, beat_resolution=beat_resolution)\n",
    "all_pianorolls_alto,_ = rnn.get_all_pianorolls(1, home_dir, beat_resolution=beat_resolution)\n",
    "all_pianorolls_tenor,_ = rnn.get_all_pianorolls(2, home_dir, beat_resolution=beat_resolution)\n",
    "all_pianorolls_bass,_ = rnn.get_all_pianorolls(3, home_dir, beat_resolution=beat_resolution)\n",
    "\n",
    "# calculate pitch range \n",
    "list_pianorolls = [all_pianorolls_soprano, all_pianorolls_alto, all_pianorolls_tenor, all_pianorolls_bass]\n",
    "global_lower, global_upper, n_notes = rnn.get_extremum_pitches(list_pianorolls)\n",
    "print('Global lower note : '+ str(global_lower))\n",
    "print('Global upper note : '+ str(global_upper))\n",
    "print('Number of notes : '+ str(n_notes))\n",
    "\n",
    "# scale pianoroll to 0 \n",
    "all_pianorolls_soprano = rnn.scale_pianoroll(all_pianorolls_soprano, global_lower)\n",
    "all_pianorolls_alto = rnn.scale_pianoroll(all_pianorolls_alto, global_lower)\n",
    "all_pianorolls_tenor = rnn.scale_pianoroll(all_pianorolls_tenor, global_lower)\n",
    "all_pianorolls_bass = rnn.scale_pianoroll(all_pianorolls_bass, global_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and print the net\n",
    "n_hidden=512\n",
    "n_layers=2\n",
    "\n",
    "net_alto = rnn.NoteRNN(n_notes, n_hidden, n_layers)\n",
    "net_tenor = rnn.NoteRNN(n_notes, n_hidden, n_layers)\n",
    "net_bass = rnn.NoteRNN(n_notes, n_hidden, n_layers)\n",
    "\n",
    "# Declaring the hyperparameters\n",
    "batch_size = 64\n",
    "seq_length = 32\n",
    "n_epochs = 100 # start smaller if you are just testing initial behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model for alto\n",
    "train_losses_alto, val_losses_alto, best_net_alto, best_epoch_alto = rnn.train(net_alto, data=all_pianorolls_soprano, \n",
    "                                                       data2 = all_pianorolls_alto, harmonization=True,\n",
    "                                                       epochs=n_epochs, batch_size=batch_size,\n",
    "                                                       seq_length=seq_length, lr=0.005)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model for tenor\n",
    "train_losses_tenor, val_losses_tenor, best_net_tenor, best_epoch_tenor = rnn.train(net_tenor, data=all_pianorolls_soprano, \n",
    "                                                       data2 = all_pianorolls_tenor, harmonization=True,\n",
    "                                                       epochs=n_epochs, batch_size=batch_size,\n",
    "                                                       seq_length=seq_length, lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model for bass\n",
    "train_losses_bass, val_losses_bass, best_net_bass, best_epoch_bass = rnn.train(net_bass, data=all_pianorolls_soprano, \n",
    "                                                       data2 = all_pianorolls_bass, harmonization=True,\n",
    "                                                       epochs=n_epochs, batch_size=batch_size,\n",
    "                                                       seq_length=seq_length, lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = figure(figsize(10,7))\n",
    "ax1 = fig.add_subplot(2,2,1)\n",
    "rnn.display_losses(ax1, train_losses_alto, val_losses_alto, best_epoch_alto)\n",
    "ax2 = fig.add_subplot(2,2,2)\n",
    "rnn.display_losses(ax2, train_losses_tenor, val_losses_tenor, best_epoch_tenor)\n",
    "ax3 = fig.add_subplot(2,2,3)\n",
    "rnn.display_losses(ax3, train_losses_bass, val_losses_bass, best_epoch_bass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtain overfitting samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Generating harmonization\n",
    "os.chdir(home_dir + \"/data/raw/bach\")  # go to a folder relative to home dir\n",
    "\n",
    "\n",
    "# get one voice from the dataset, select until -15 to make sure that it is a soprano from validation set\n",
    "midi_filename = midi_files[-4]\n",
    "print(\"Let's generate alto, tenor and bass from \"+ midi_filename + \" soprano\")\n",
    "\n",
    "start_size = 8 # sequence size that we keep intact for the beginning\n",
    "\n",
    "real_tracks = []\n",
    "for voice in range(4):\n",
    "    track = rnn.get_track(midi_filename, voice, beat_resolution=beat_resolution, transpose=True)\n",
    "    real_tracks.append(track)\n",
    "    \n",
    "# store all networks \n",
    "networks = [None, net_alto, net_tenor, net_bass]\n",
    "\n",
    "# Overfitting network test \n",
    "generated_tracks_overfit = [real_tracks[0]]\n",
    "for voice_togenerate in range(1,4):\n",
    "    generated_track = rnn.process_harmonization(midi_filename, networks, \n",
    "                                            global_lower, real_tracks, voice_togenerate, start_size)\n",
    "    generated_tracks_overfit.append(generated_track)\n",
    "multitrack_overfit = Multitrack(tracks=generated_tracks_overfit, \n",
    "                            tempo = 90, beat_resolution=beat_resolution)\n",
    "\n",
    "#write to midifile \n",
    "pypianoroll.write(multitrack_overfit, home_dir + \"/results/RNN_harmonization_overfit_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_midi_filename = home_dir + \"/results/RNN_harmonization_overfit_test.mid\"\n",
    "pygame.init()\n",
    "pygame.mixer.music.load(modified_midi_filename)\n",
    "pygame.mixer.music.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.mixer.music.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*******\n",
    "*******\n",
    "\n",
    "## **Auto-Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
