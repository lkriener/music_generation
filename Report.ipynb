{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group project for *Advanced topics in machine learning* lecture (2019)\n",
    "#### Benjamin Ellenberger, Nicolas Deperrois, Laura Kriener"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chosen Topic: **Music generation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Motivation and State-of-the-Art**\n",
    "\n",
    "The topics for the group project work was chosen in the very beginning of the course.\n",
    "As it was announced that the course will nearly always use images for demonstrations, examples and excercises, we decided that our project should be on a different form of data.\n",
    "We decided to work in the broad field of music generation with deep learing.\n",
    "\n",
    "To narrow down the topic we investigated what the state of the art in this field is.\n",
    "The most impressive recent results are produced by Google and OpenAI.\n",
    "The [Google Magenta project](https://magenta.tensorflow.org/) covers a wide range of applications such as harmonization, drum-machines and a music generating network using the transformer network architecture with attention.\n",
    "\n",
    "An other very recent result in the field of generating music was published by OpenAI. The [MuseNet](https://openai.com/blog/musenet/) uses the recently published GPT2-architecture which is a large-scale transformer network as well. \n",
    "\n",
    "The Google and OpenAI approaches as well as other (less famous) approaches have in common, that they employ very complicated network architectures in combination with the use of immense computational resources.\n",
    "\n",
    "As the required computational power is far out of our reach, we wondered if this level of complexity is really unavoidable.\n",
    "And so the question **How much can you do with how little?** became the leading theme for our project.\n",
    "We want to see, what results can be achieved using much simpler network architecutres (i.e. architectures within the scope of the lecture)?\n",
    "Which aspects of music generation can be achieved and which have to be ignored? For example can you generate a resonable melody line without considering the rhythm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges\n",
    "The main challenge is that music generation is a very broad topic. Before we can even start we have to answer a couple of questions:\n",
    "\n",
    "- What exactly do we want to generate? Melody? Rhythm? Harmony?\n",
    "\n",
    "This will depend on the network structures we try out. For example for simple feed-forward networks we only focus on melody, while we include rhythm in the LSTMs.\n",
    "\n",
    "- How to feed music into a network? Spectrum? Pitches? Intervals?\n",
    "\n",
    "We will not use spectrum or audio-data, instead we will work with pitches and intervals and note-lengths extracted from midi-files.\n",
    "\n",
    "- What kind of music?\n",
    "\n",
    "As we try to keep things as simple as possible, we decided to use the widely used bach chorale data-set (see `data/raw/bach`).\n",
    "\n",
    "- How do we evaluate the result?\n",
    "\n",
    "Music and the quality of music is a highly subjective topic. It is very difficult to find a metric that evaluates how good a produced piece of music is.\n",
    "We plan to evaluate the pieces of music produced by the different network architectures by comparison and by the networks ability to capture different aspects of music (e.g. if one architecture can only produce melodies without considering rhythm and the other includes rhythm the second one is better suited for music generation). Additionally a measure for quality can be the similarity to the music style it was trained on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data sets\n",
    "\n",
    "We will work on the Bach chorale dataset which we included in the repository (see `data/raw/bach`).\n",
    "Midi-files are binary, therefore it is difficult to modify and create them directly.\n",
    "We are using the python libaries `pianoroll` and `midicsv` which translates a midi-file into a human-readable (and modifiable) csv-string.\n",
    "From this we have written our own utility functions that allow us to extract high-level information about the music (e.g. tempo, tonality) and perform changes to the tracks and write them back to a midi file. The utility functions are located in the file `src/midi_utils.py`. A demonstration on how to use them can be found in the notebook `demo_midi_utils.ipynb`. The functions to create a Pytorch compatible dataset from the midi-data are in the file `src/dataset_utils.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Feed-Forward Networks**\n",
    "\n",
    "Starting from the simplest possible network architecture, we will first investigate the potential and limitations of feed-forward networks.\n",
    "Their architecture is not really suited to deal with time-series data, because one input sample is presented as a whole and at the same time to the network.\n",
    "Also there is no mechanism for memory of the past.\n",
    "Respecting these limitations, we simplify the task of \"music generation\" to **predicting the next note in a melody given the $n$ previous notes** (we only predict the pitch not the length of notes).\n",
    "\n",
    "#### Generation mechanism\n",
    "\n",
    "This setup also allows for a generation mechanism for a longer melody as sketched in the figure below:\n",
    "We start the network of with a set of notes from a melody ($n1$ to $n10$) as input. \n",
    "The network then predicts the next note for the melody ($n11$). In the next step we give the network a new set of input which consists of the notes $n2$ to $n10$ **and** the note it predicted before $n11$. The network now predicts the twelth note ($n12$) which is then used in the input for the next round.\n",
    "\n",
    "The main goal of this generation task is to produce a series of notes which is not dissonant and stays in the tonality of the given starter notes.\n",
    "\n",
    "\n",
    "![Schematic drawing of generation procedure](graphics/forward3.png)\n",
    "\n",
    "\n",
    "#### Network structure\n",
    "\n",
    "The figure below shows a sketch of the feed-forward architecture that produced the best results.\n",
    "Note that the inputs for the network are not the pitches of the starter-notes but the intervals between them.\n",
    "This is beneficial, as using the intervals automatically provides a form of normalization.\n",
    "The use of intervals for example removes for example the dependence on the key the melody is written in.\n",
    "For example the line C-D-E-F (C-Major) will produce `[2, 2, 2, 1]` (counting the intervals in half-tone steps).\n",
    "Even though the line D-E-F#-G (D-Major) is in a different key, it will produce the same intervals `[2, 2, 2, 1]`.\n",
    "\n",
    "![Feed-forward architecture](graphics/forward2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **LSTMs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Auto-Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
