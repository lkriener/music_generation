{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group project for *Advanced topics in machine learning* lecture (2019)\n",
    "### Benjamin Ellenberger, Nicolas Deperrois, Laura Kriener"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chosen Topic: **Music generation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation and State-of-the-Art\n",
    "\n",
    "The topics for the group project work was chosen in the very beginning of the course.\n",
    "As it was announced that the course will nearly always use images for demonstrations, examples and excercises, we decided that our project should be on a different form of data.\n",
    "We decided to work in the broad field of music generation with deep learing.\n",
    "\n",
    "To narrow down the topic we investigated what the state of the art in this field is.\n",
    "The most impressive recent results are produced by Google and OpenAI.\n",
    "The [Google Magenta project](https://magenta.tensorflow.org/) covers a wide range of applications such as harmonization, drum-machines and a music generating network using the transformer network architecture with attention.\n",
    "\n",
    "An other very recent result in the field of generating music was published by OpenAI. The [MuseNet](https://openai.com/blog/musenet/) uses the recently published GPT2-architecture which is a large-scale transformer network as well. \n",
    "\n",
    "The Google and OpenAI approaches as well as other (less famous) approaches have in common, that they employ very complicated network architectures in combination with the use of immense computational resources.\n",
    "\n",
    "As the required computational power is far out of our reach, we wondered if this level of complexity is really unavoidable.\n",
    "And so the question **How much can you do with how little?** became the leading theme for our project.\n",
    "We want to see, what results can be achieved using much simpler network architecutres (i.e. architectures within the scope of the lecture)?\n",
    "Which aspects of music generation can be achieved and which have to be ignored? For example can you generate a resonable melody line without considering the rhythm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges\n",
    "The main challenge is that music generation is a very broad topic. Before we can even start we have to answer a couple of questions:\n",
    "\n",
    "- What exactly do we want to generate? Melody? Rhythm? Harmony?\n",
    "\n",
    "This will depend on the network structures we try out. For example for simple feed-forward networks we only focus on melody, while we include rhythm in the LSTMs.\n",
    "\n",
    "- How to feed music into a network? Spectrum? Pitches? Intervals?\n",
    "\n",
    "We will not use spectrum or audio-data, instead we will work with pitches and intervals and note-lengths extracted from midi-files.\n",
    "\n",
    "- What kind of music?\n",
    "\n",
    "As we try to keep things as simple as possible, we decided to use the widely used bach chorale data-set (see `data/raw/bach`).\n",
    "\n",
    "- How do we evaluate the result?\n",
    "\n",
    "Music and the quality of music is a highly subjective topic. It is very difficult to find a metric that evaluates how good a produced piece of music is.\n",
    "We plan to evaluate the pieces of music produced by the different network architectures by comparison and by the networks ability to capture different aspects of music (e.g. if one architecture can only produce melodies without considering rhythm and the other includes rhythm the second one is better suited for music generation). Additionally a measure for quality can be the similarity to the music style it was trained on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data sets\n",
    "\n",
    "We will work on the Bach chorale dataset which we included in the repository (see `data/raw/bach`).\n",
    "Midi-files are binary, therefore it is difficult to modify and create them directly.\n",
    "We are using the python libaries `pianoroll` and `midicsv` which translates a midi-file into a human-readable (and modifiable) csv-string.\n",
    "From this we have written our own utility functions that allow us to extract high-level information about the music (e.g. tempo, tonality) and perform changes to the tracks and write them back to a midi file. The utility functions are located in the file `src/midi_utils.py`. A demonstration on how to use them can be found in the notebook `demo_midi_utils.ipynb`. The functions to create a Pytorch compatible dataset from the midi-data are in the file `src/dataset_utils.py`.\n",
    "\n",
    "******\n",
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Feed-Forward Networks**\n",
    "\n",
    "Starting from the simplest possible network architecture, we will first investigate the potential and limitations of feed-forward networks.\n",
    "Their architecture is not really suited to deal with time-series data, because one input sample is presented as a whole and at the same time to the network.\n",
    "Also there is no mechanism for memory of the past.\n",
    "Respecting these limitations, we simplify the task of \"music generation\" to **predicting the next note in a melody given the $n$ previous notes** (we only predict the pitch not the length of notes).\n",
    "\n",
    "### Generation mechanism\n",
    "\n",
    "This setup also allows for a generation mechanism for a longer melody as sketched in the figure below:\n",
    "We start the network of with a set of notes from a melody ($n1$ to $n10$) as input. \n",
    "The network then predicts the next note for the melody ($n11$). In the next step we give the network a new set of input which consists of the notes $n2$ to $n10$ **and** the note it predicted before $n11$. The network now predicts the twelth note ($n12$) which is then used in the input for the next round.\n",
    "\n",
    "The main goal of this generation task is to produce a series of notes which is not dissonant and stays in the tonality of the given starter notes.\n",
    "\n",
    "\n",
    "![Schematic drawing of generation procedure](graphics/forward3.png)\n",
    "\n",
    "\n",
    "### Network structure\n",
    "\n",
    "The figure below shows a sketch of the feed-forward architecture that produced the best results.\n",
    "Note that the inputs for the network are not the pitches of the starter-notes but the intervals between them.\n",
    "This is beneficial, as using the intervals automatically provides a form of normalization.\n",
    "The use of intervals for example removes for example the dependence on the key the melody is written in.\n",
    "For example the line C-D-E-F (C-Major) will produce `[2, 2, 2, 1]` (counting the intervals in half-tone steps).\n",
    "Even though the line D-E-F#-G (D-Major) is in a different key, it will produce the same intervals `[2, 2, 2, 1]`.\n",
    "\n",
    "The input layer of the network contains 10 units, each coding for one of the ten starter-intervals.\n",
    "The input is connected to two hidden layer with 128 units each.\n",
    "The output layer has 25 units. Each is coding for one possible interval (step-size: half-tones) in the range of $\\pm$ 1 Octave.\n",
    "We train the network by presenting it with 10-intervals pieces of the bach dataset and let it predict the 11th interval.\n",
    "The loss is calculated using the CrossEntropyLoss-function in pyTorch.\n",
    "In early experiments the output layer contained only one unit which was used to predict the pitch/interval value. \n",
    "The results were much worse compared to predicting the probability for each possible interval. \n",
    "In the generation mode of the network the interval with the highest probability is picked as the \"chosen interval\" by the network.\n",
    "\n",
    "![Feed-forward architecture](graphics/forward2.png)\n",
    "\n",
    "### Results\n",
    "\n",
    "![Feed-forward architecture](graphics/loss_forward_intervals.png)\n",
    "\n",
    "The figure above shows the training process of the described network architecture.\n",
    "We see a clear occurance of overfitting, as the loss on the validation dataset first plateaus and then increases while the loss on the training data continues to decrease.\n",
    "The 3 stars in the plot mark where snapshots of the network model were taken during training and saved for the generation of melodies.\n",
    "The notebook cells below show an exemplary melody generated by an untrained network, by the network at the end of the plateau in the validation loss and at the end of training, when the validation loss has already increased again.\n",
    "For the generation of the audio samples below the same 10 starter-notes were used.\n",
    "They are included in the audio file.\n",
    "\n",
    "\n",
    "#### **Alternative architectures / training methods and their impact**\n",
    "In the following we quickly describe some different variations of the described network architecture and why they were dismissed in favor of the one presented above.\n",
    "\n",
    "- more neurons/layers: lead to worse overfitting without increasing performance significantly\n",
    "- less neurons/layers: delay the overfitting but decrease performance\n",
    "- regularization methods: only delay overfitting while decreasing performance\n",
    "- more starter-notes (i.e. larger input layer): worse overfitting\n",
    "- less starter-notes (everything below 10): decreases overfitting but during generation the network will just produce a repeated loop of the starter-notes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audio sample of untrained network\n",
    "We had to stop the generation of this melody early because the notes predicted by the network drifted out of the available range in midi-files.\n",
    "The untrained network clearly does not stay in a fairly small pitch range (as Bach melodies usually do) and produces lines that sound unharmonic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "\n",
    "filename = 'results/feedforward_net_track_intervals_untrained.mid'\n",
    "pygame.init()\n",
    "pygame.mixer.music.load(filename)\n",
    "pygame.mixer.music.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.mixer.music.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audio sample of the network at the end of training\n",
    "The melody produced by this network stays within a reasonable pitch range of the starter notes.\n",
    "It produces a harmonic sounding line of notes and seems to stay within the tonality of the starter notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "\n",
    "filename = 'results/feedforward_net_track_intervals.mid'\n",
    "pygame.init()\n",
    "pygame.mixer.music.load(filename)\n",
    "pygame.mixer.music.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.mixer.music.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audio sample of network before severe overfitting\n",
    "The melody produced by this network stays within a reasonable pitch range of the starter notes.\n",
    "It produces a mostly harmonic sounding line of notes and mostly seems to stay within the tonality of the starter notes.\n",
    "There are some questionable choices of notes in the melody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "\n",
    "filename = 'results/feedforward_net_track_intervals_snapshot.mid'\n",
    "pygame.init()\n",
    "pygame.mixer.music.load(filename)\n",
    "pygame.mixer.music.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.mixer.music.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion of results\n",
    "Bearing in mind that we chose the feed-forward architecture as the most simple model, we can be content with the result. \n",
    "It was clear from the start that the feed-forward networks would only be able to learn a very restricted task, i.e. no rhythm and chords.\n",
    "Therefore we chose only to try to learn to produce short melody patches. This we were able to achieve.\n",
    "The overfitting which occured is a problem, that might be alieviated by a larger dataset. However, the larger sets available do contain different styles of music than Bach chorales which we deliberatly chose for their simple melody lines.\n",
    "\n",
    "We see that both trained snapshots of the network produce a melody that stays within a reasonable range of notes and does not drift off like the untrained version.\n",
    "Also they produce lines of notes that are consistent with the harmony style by Bach and they mostly stay within the tonality.\n",
    "\n",
    "One could argue that using the network which was trained with heavy overfitting is ok for music generation (but not for predicting the next note of a melody line), because it effectively does what a human composer does: It \"knows\" short patches of melody from the training set (i.e. the music it already knows) and reuses parts of these melody patches to generate new melodies. Human composes do this as well (a lot). For example, someone who knows a couple of Hans Zimmer movie soundtracks will very quickly recognize if a new soundtrack is written by Hans Zimmer, because he very often reuses certain themes and instrumentation schemes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplary usage of the code\n",
    "#### Training the network model and generating with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import src.feed_forward_utils as ff\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "try:\n",
    "    home_dir\n",
    "except NameError:\n",
    "    home_dir = os.getcwd()\n",
    "os.chdir(home_dir)\n",
    "\n",
    "data_dir = '/data/raw/bach'\n",
    "\n",
    "minibatch_size = 32\n",
    "train_valid_ratio = 0.8        # size ratio of the train- and validation dataset\n",
    "feature_qty = 10               # how many starter-intervals are presented to network\n",
    "prediction_qty = 1             # must be one if intervals are used\n",
    "interval_range = [-12, 12]     # in halftones -> 1 octave\n",
    "snapshot_epoch = 50            # epoch at which a snapshot of the model is created during training\n",
    "n_epochs = 100\n",
    "learning_rate = 0.001\n",
    "hidden_size = 128\n",
    "\n",
    "tracks = ff.load_tracks(data_dir, 1)\n",
    "train_set, valid_set, train_loader, valid_loader = ff.generate_dataloaders(tracks, minibatch_size, train_valid_ratio, \n",
    "                                                                           feature_qty, prediction_qty, interval_range)\n",
    "input_size = len(train_set[0][0])   # get input size\n",
    "input_example = train_set[0][0]\n",
    "output_size = len(range(interval_range[0], interval_range[1]+1, 1))     # get output size\n",
    "output_example = train_set[0][1]\n",
    "print(\"Training set size\", len(train_set))\n",
    "print(\"Input size {}/ output size {}/ learning rate {}\".format(input_size, output_size, learning_rate))\n",
    "print(\"Input example {}\".format(input_example))\n",
    "print(\"Output example (idx of next interval) {}\".format(output_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "forward_model = ff.FeedForward(input_size, hidden_size, output_size)\n",
    "untrained_model = copy.deepcopy(forward_model)\n",
    "untrained_model = untrained_model.to(device)\n",
    "forward_model = forward_model.to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(forward_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, val_losses, snapshot_model = ff.train(forward_model, train_loader, valid_loader, \n",
    "                                                    optimizer, n_epochs, criterion, device=device, \n",
    "                                                    verbose=True, snapshot=snapshot_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(home_dir)\n",
    "result_dir = home_dir + '/results/'\n",
    "result_name = 'retrained_feedforward_'\n",
    "ff.save_results(result_dir, result_name, untrained_model, snapshot_model, \n",
    "                forward_model, train_losses, val_losses, snapshot_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = 45\n",
    "predict_length = 30\n",
    "\n",
    "filename_untrained = result_dir + result_name + 'untrained.mid'\n",
    "ff.generate_melody(device, valid_set, sample_idx, untrained_model, predict_length, interval_range, filename_untrained)\n",
    "\n",
    "filename_snapshot = result_dir + result_name + 'snapshot.mid'\n",
    "ff.generate_melody(device, valid_set, sample_idx, snapshot_model, predict_length, interval_range, filename_snapshot)\n",
    "\n",
    "filename_trained = result_dir + result_name + 'trained.mid'\n",
    "ff.generate_melody(device, valid_set, sample_idx, forward_model, predict_length, interval_range, filename_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "\n",
    "filename = filename_trained # or filename_trained or filename_snapshot\n",
    "pygame.init()\n",
    "pygame.mixer.music.load(filename)\n",
    "pygame.mixer.music.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.mixer.music.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the pretrained model and generating with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import src.feed_forward_utils as ff\n",
    "\n",
    "try:\n",
    "    home_dir\n",
    "except NameError:\n",
    "    home_dir = os.getcwd()\n",
    "os.chdir(home_dir)\n",
    "\n",
    "data_dir = '/data/raw/bach'\n",
    "\n",
    "minibatch_size = 32\n",
    "train_valid_ratio = 0.8        # size ratio of the train- and validation dataset\n",
    "feature_qty = 10               # how many starter-intervals are presented to network\n",
    "prediction_qty = 1             # must be one if intervals are used\n",
    "interval_range = [-12, 12]     # in halftones -> 1 octave\n",
    "snapshot_epoch = 50            # epoch at which a snapshot of the model is created during training\n",
    "\n",
    "tracks = ff.load_tracks(data_dir, 1)\n",
    "train_set, valid_set, train_loader, valid_loader = ff.generate_dataloaders(tracks, minibatch_size, train_valid_ratio, \n",
    "                                                                           feature_qty, prediction_qty, interval_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(home_dir)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_filename = 'results/feedforward_trained.pth'\n",
    "model_loaded = torch.load(model_filename)\n",
    "model_loaded.to(device)\n",
    "model_loaded.eval()\n",
    "\n",
    "sample_idx = 42\n",
    "predict_length = 25\n",
    "result_dir = 'results/'\n",
    "result_name = 'loaded_feedforward_'\n",
    "filename_loaded = result_dir + result_name + 'generating.mid'\n",
    "ff.generate_melody(device, valid_set, sample_idx, model_loaded, predict_length, interval_range, filename_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "\n",
    "pygame.init()\n",
    "pygame.mixer.music.load(filename_loaded)\n",
    "pygame.mixer.music.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.mixer.music.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*********\n",
    "*********\n",
    "\n",
    "# **LSTMs** - including rhythms and generate harmonization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will try to generate melodies from the same dataset (Bach Chorals) using recurrent neural networks (RNN) with long short term memory units (LSTM) as in the tutorial 08 of the class, based on the two following blog posts:\n",
    "\n",
    "- http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "- https://towardsdatascience.com/writing-like-shakespeare-with-machine-learning-in-pytorch-d77f851d910c\n",
    "\n",
    "The idea would be to extend the character-to-character LSTM network to midi files. We will use another representation of the data, named 'pianorolls', where a single melody is stored in an array containing the melody pitches across time. This is one of the most frequent representations used, although it has some limitations. For instance, there is no way to differentiate between a long note (1 half-note) and 4 short-ones (4 eight-notes). However, it brings a considerable advantage compared to the previous used representations as it can store rhythm and silences, depending on the chosen time-step. \n",
    "\n",
    "Moreover, each possible note is considered as a distinct element of a vocabulary. We will then use $N$ input nodes, where $N$ is the size of the vocabulary (i.e., the number of distinct notes, including silences). \n",
    "Thus, these pianorolls arrays will be converted into one-hot vectors containing the corresponding pitch (or silence, represented by 0) for each time step, and will be fed to the RNN network defined below. \n",
    "\n",
    "All methods for this part are stored in the python module `src.RNN_utils`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a single melody with LSTMs\n",
    "\n",
    "### Data representation\n",
    "We use the pypianoroll library - https://salu133445.github.io/pypianoroll/ - to convert a midi file into a multitrack object, get the soprano track, transpose it to C major, create a new multitrack object out of it, and finally write it to a new midi file. \n",
    "We first import midi files, choose a voice, and concatenate all melodies into a big sequence of notes. We also calculate the pitch ranges. In order to minimize the size of one-hot encode vectors, we restrict the pitches to values next to 0, using the minimum pitch found in the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network structure: LSTM network as for note-to-note melody generation\n",
    "We now build a network to implement note-to-note melody generation using LSTMs units **without** drop-out of the output. This network is highly inspired from the Tutorial 08 of the class, by replacing characters by notes. \n",
    "\n",
    "For training the model, we declare a function, where we define an optimizer (Adam) and loss (cross entropy). Then, training and validation data are separated and the hidden state of the RNN is initaliazed. Looping over the training melody batches, we use the functions `get_pianoroll_batches` and `one_hot_encode_batch` in order to build batches and feed them to the network input. This function is also adapted to harmonization that will be defined below. Every once a while, we generate some loss statistics (training loss and validation loss) to let us know if the model is training correctly.\n",
    "\n",
    "As a first attempt to generate a melody using LSTMs with pianorolls, we will simply try to generate a single voice (soprano, alto, tenor or bass), training the network on the whole midi file dataset concatenated into a long sequence of notes all translated to the same tonality (here, C Major or its relative A minor). The later operation will particularly facilitate learning on such a small dataset as all melodies will be rescaled to the same tonality, thus preventing abrupt changes among melody batches. \n",
    "\n",
    "![RNN-architecture-harm](graphics/RNN_single_network.png)\n",
    "\n",
    "\n",
    "### Results \n",
    "In order to evaluate the training efficiency of the note-to-note generating task with LSTMs, we plot the evolution of training and validation losses across epochs. As in the previous section, the 3 stars in the plot represent snapshots of the LSTM model taken during training and saved for the generation of melodies. Interestingly, we can also notice a similar evolution of the validation loss, first decreasing, then platauing and finally increasing again, illustrating considerable overfitting. \n",
    "![RNN-architecture](graphics/RNN_single_plot.png)\n",
    "\n",
    "In the cells below, we present generated melodies for the selected voice using the NoteRNN network at different stages of learning: at the beginning (random), at early-stopping (early) and at the end of learning (overfit). As in text generation, a starting sequence of 4 eighth notes is imposed to the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audio sample of untrained network\n",
    "Let's first try melody generation with a random initial network (no training). \n",
    "As expected, it does not sound like a Bach melody, as it is totally out of any possible key. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import os\n",
    "import glob\n",
    "try:\n",
    "    home_dir\n",
    "except NameError:\n",
    "    home_dir = os.getcwd()\n",
    "os.chdir(home_dir + \"/data/raw/bach\")  # go to a folder relative to home dir\n",
    "\n",
    "filename = home_dir + \"/results/RNN_single_random.mid\"\n",
    "pygame.init()\n",
    "pygame.mixer.music.load(filename)\n",
    "pygame.mixer.music.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.mixer.music.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audio sample of network before severe overfitting\n",
    "The melody produced by this network is quite coherent as it catches a certain tonality and respects the associated harmony, most of the time C Major, as imposed by the starting note. In addition to the feedforward case, it also includes rhythm, even if most of the time quarter and half notes are produced, while it could also generate eigth notes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = home_dir + \"/results/RNN_single_early.mid\"\n",
    "pygame.init()\n",
    "pygame.mixer.music.load(filename)\n",
    "pygame.mixer.music.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.mixer.music.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audio sample of network at the end of training\n",
    "As in the previous case, the melody produced by the network at the end of training with severe overfitting is totally in the pitch range of the initial tonality, and rhythms choice are a bit more coherent than in the previous case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = home_dir + \"/results/RNN_single_overfit.mid\"\n",
    "pygame.init()\n",
    "pygame.mixer.music.load(filename)\n",
    "pygame.mixer.music.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.mixer.music.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion of the results \n",
    "As in the feedforward case, we obtain a typical evolution of the validation loss which triggered the interest to qualitatively evaluate the results at 3 essential stages of learning. Surprisingly, we obtain similar results than with feedforward network in terms of pitch and tonality, but using pianorolls enabled us to also generate rhythms in the melody, even if they are quite slow and do not fully respect the patterns of a Bach Choral melody.  \n",
    "We can thus conclude that to perform such a single task as generating melodies, the use of the timing component from recurrent networks is not really beneficial, and should be test on more complex tasks as shown below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating harmonization with LSTMs\n",
    "\n",
    "We observed above that it is possible to generate a coherent melody using a slightly modified version of the original character-to-character RNN. Here, we will investigate whether such a network can perform more complex task, such as harmonizing a given voice. For instance, from the soprano sequence, we would like to generate the associated alto voice. \n",
    "\n",
    "### Network structure: LSTM network as for note-to-note melody harmonization\n",
    "\n",
    "In this section, we will use the same RNN network as above, with slight modifications in the learning rate, number of units and batch size. The only noticeable change is that the training process is now specialized to harmonization. Indeed, two long sequences of notes (pianorolls) are now given to the network. The input sequence, from which we want to generate harmonization (in our case, the soprano voice), and the target sequence, that we wish to generate to properly harmonize the input voice. The idea is to keep the note-to-note structure, but instead of predicting the next note a sequence, we predict the note from another voice at the same time step. \n",
    "Three networks were used to perform harmonization of the soprano voice, each corresponding to a given lower voice: alto, tenor and bass. Here again, we recorded the 3 essential learning stages and evaluate them for harmonization. \n",
    "\n",
    "![RNN-architecture-harm](graphics/RNN_harmonization_network.png)\n",
    "\n",
    "### Results \n",
    "\n",
    "The figure below shows the evolution of training and validation losses across epochs for each harmonizing RNN: alto, tenor and bass. Intertingly, they show a similar shape as for melody generation, all first reaching a plateau and severely overfitting. Furthermore, it looks like training and validation losses adversarial: when the training loss increases again, the validation loss decreases. This shows that overfitting totally prevent the network from generating the original voices. \n",
    "\n",
    "![RNN-architecture-plot](graphics/RNN_harmonization_plot.png)\n",
    "\n",
    "It is now time to test the quality of the RNNs trained to harmonize a soprano voice by generating separately alto, tenor and bass voices. We will test it on one given Bach midi file in the browser. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audio sample of untrained network\n",
    "\n",
    "Let's first generate harmonization with a ramdom initial network, without training. As expected, the harmonization is totally random, thus all voices are not following the soprano melody and generate random melodies independently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = home_dir + \"/results/RNN_harmonization_random.mid\"\n",
    "pygame.init()\n",
    "pygame.mixer.music.load(filename)\n",
    "pygame.mixer.music.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.mixer.music.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audio sample of network before severe overfitting \n",
    "We now test the quality of harmonization on the early-stopped network. \n",
    "The obtained track sounds much better and start to harmonize the soprano voice correctly, especially by trying to match the soprano rhythm. However, it is not optimal as we cannot hear any coherent harmonies and cadences. \n",
    "Let's finally try on the highly-overfitting network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = home_dir + \"/results/RNN_harmonization_early.mid\"\n",
    "pygame.init()\n",
    "pygame.mixer.music.load(filename)\n",
    "pygame.mixer.music.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.mixer.music.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audio sample of network at the end of training\n",
    "\n",
    "The overfitting networks sounds to generate voices that match much better with the soprano voice than the previous networks! Even if at the beginning, the voices are not really coherent, they start to catch some relevant melodies that vertically harmonize the soprano voice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_midi_filename = home_dir + \"/results/RNN_harmonization_overfit.mid\"\n",
    "pygame.init()\n",
    "pygame.mixer.music.load(modified_midi_filename)\n",
    "pygame.mixer.music.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.mixer.music.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion of the results from the overfitting case\n",
    "\n",
    "We hypothesize that such surprising results might be specific to music generation. Bach Chorales are quite similar, especially when transcripted to C Major (or its relative A minor), and overfitting the training set can lead to optimal network features to generate 3 voices on a soprano voice which harmonization was never learned (validation set). Even if the validation error is huge compared to the early stopping case, it does not mean that the generated voices are wrong. The overfitted network generate complete different alto/tenor/bass melodies from the original ones, which might explain this huge validation error, but these melodies vertically chords and cadences that can turn out to be really coherent!\n",
    "As an example, think of the original track ending up with an interrupted cadence, in a total different tonality than C Major (let's say, A Major), which has been seen less often that a perfect cadence (C Major) in the training set. The overfitting network will unless try to recover this perfect cadence as it learned to do by overfitting, and thus generate complete different melodies than the target ones, but vertically coherent with each other to produce such a perfect cadence. \n",
    "\n",
    "As an illustration of such a phenomenon, let's listen to original track (real alto/tenor/bass voices) and compare to the overfitted track at the same time steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_midi_filename = home_dir + \"/results/RNN_harmonization_real.mid\"\n",
    "pygame.init()\n",
    "pygame.mixer.music.load(modified_midi_filename)\n",
    "pygame.mixer.music.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.mixer.music.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplary usage of code for harmonization\n",
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "from pypianoroll import Multitrack, Track\n",
    "import pypianoroll\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "%pylab inline\n",
    "import src.RNN_utils as rnn\n",
    "\n",
    "try:\n",
    "    home_dir\n",
    "except NameError:\n",
    "    home_dir = os.getcwd()\n",
    "os.chdir(home_dir + \"/data/raw/bach\")  # go to a folder relative to home dir\n",
    "\n",
    "beat_resolution = 2\n",
    "# get entire pianoroll sequences for each voice\n",
    "all_pianorolls_soprano, midi_files = rnn.get_all_pianorolls(0, home_dir, beat_resolution=beat_resolution)\n",
    "all_pianorolls_alto,_ = rnn.get_all_pianorolls(1, home_dir, beat_resolution=beat_resolution)\n",
    "all_pianorolls_tenor,_ = rnn.get_all_pianorolls(2, home_dir, beat_resolution=beat_resolution)\n",
    "all_pianorolls_bass,_ = rnn.get_all_pianorolls(3, home_dir, beat_resolution=beat_resolution)\n",
    "\n",
    "# calculate pitch range \n",
    "list_pianorolls = [all_pianorolls_soprano, all_pianorolls_alto, all_pianorolls_tenor, all_pianorolls_bass]\n",
    "global_lower, global_upper, n_notes = rnn.get_extremum_pitches(list_pianorolls)\n",
    "print('Global lower note : '+ str(global_lower))\n",
    "print('Global upper note : '+ str(global_upper))\n",
    "print('Number of notes : '+ str(n_notes))\n",
    "\n",
    "# scale pianoroll to 0 \n",
    "all_pianorolls_soprano = rnn.scale_pianoroll(all_pianorolls_soprano, global_lower)\n",
    "all_pianorolls_alto = rnn.scale_pianoroll(all_pianorolls_alto, global_lower)\n",
    "all_pianorolls_tenor = rnn.scale_pianoroll(all_pianorolls_tenor, global_lower)\n",
    "all_pianorolls_bass = rnn.scale_pianoroll(all_pianorolls_bass, global_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and print the net\n",
    "n_hidden=512\n",
    "n_layers=2\n",
    "\n",
    "net_alto = rnn.NoteRNN(n_notes, n_hidden, n_layers)\n",
    "net_tenor = rnn.NoteRNN(n_notes, n_hidden, n_layers)\n",
    "net_bass = rnn.NoteRNN(n_notes, n_hidden, n_layers)\n",
    "\n",
    "# Declaring the hyperparameters\n",
    "batch_size = 64\n",
    "seq_length = 32\n",
    "n_epochs = 100 # start smaller if you are just testing initial behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model for alto\n",
    "train_losses_alto, val_losses_alto, best_net_alto, best_epoch_alto = rnn.train(net_alto, data=all_pianorolls_soprano, \n",
    "                                                       data2 = all_pianorolls_alto, harmonization=True,\n",
    "                                                       epochs=n_epochs, batch_size=batch_size,\n",
    "                                                       seq_length=seq_length, lr=0.005)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model for tenor\n",
    "train_losses_tenor, val_losses_tenor, best_net_tenor, best_epoch_tenor = rnn.train(net_tenor, data=all_pianorolls_soprano, \n",
    "                                                       data2 = all_pianorolls_tenor, harmonization=True,\n",
    "                                                       epochs=n_epochs, batch_size=batch_size,\n",
    "                                                       seq_length=seq_length, lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model for bass\n",
    "train_losses_bass, val_losses_bass, best_net_bass, best_epoch_bass = rnn.train(net_bass, data=all_pianorolls_soprano, \n",
    "                                                       data2 = all_pianorolls_bass, harmonization=True,\n",
    "                                                       epochs=n_epochs, batch_size=batch_size,\n",
    "                                                       seq_length=seq_length, lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = figure(figsize(10,7))\n",
    "ax1 = fig.add_subplot(2,2,1)\n",
    "rnn.display_losses(ax1, train_losses_alto, val_losses_alto, best_epoch_alto)\n",
    "ax2 = fig.add_subplot(2,2,2)\n",
    "rnn.display_losses(ax2, train_losses_tenor, val_losses_tenor, best_epoch_tenor)\n",
    "ax3 = fig.add_subplot(2,2,3)\n",
    "rnn.display_losses(ax3, train_losses_bass, val_losses_bass, best_epoch_bass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtain overfitting samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Generating harmonization\n",
    "os.chdir(home_dir + \"/data/raw/bach\")  # go to a folder relative to home dir\n",
    "\n",
    "\n",
    "# get one voice from the dataset, select until -15 to make sure that it is a soprano from validation set\n",
    "midi_filename = midi_files[-4]\n",
    "print(\"Let's generate alto, tenor and bass from \"+ midi_filename + \" soprano\")\n",
    "\n",
    "start_size = 8 # sequence size that we keep intact for the beginning\n",
    "\n",
    "real_tracks = []\n",
    "for voice in range(4):\n",
    "    track = rnn.get_track(midi_filename, voice, beat_resolution=beat_resolution, transpose=True)\n",
    "    real_tracks.append(track)\n",
    "    \n",
    "# store all networks \n",
    "networks = [None, net_alto, net_tenor, net_bass]\n",
    "\n",
    "# Overfitting network test \n",
    "generated_tracks_overfit = [real_tracks[0]]\n",
    "for voice_togenerate in range(1,4):\n",
    "    generated_track = rnn.process_harmonization(midi_filename, networks, \n",
    "                                            global_lower, real_tracks, voice_togenerate, start_size)\n",
    "    generated_tracks_overfit.append(generated_track)\n",
    "multitrack_overfit = Multitrack(tracks=generated_tracks_overfit, \n",
    "                            tempo = 90, beat_resolution=beat_resolution)\n",
    "\n",
    "#write to midifile \n",
    "pypianoroll.write(multitrack_overfit, home_dir + \"/results/RNN_harmonization_overfit_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_midi_filename = home_dir + \"/results/RNN_harmonization_overfit_test.mid\"\n",
    "pygame.init()\n",
    "pygame.mixer.music.load(modified_midi_filename)\n",
    "pygame.mixer.music.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.mixer.music.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*******\n",
    "*******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Auto-Encoder Networks**\n",
    "\n",
    "In this last segment, let us look at yet another approach on how to model music, which was originally presented in [Generating Songs with Neural Networks, (2018), GitHub, https://github.com/HackerPoet/Composer](https://github.com/HackerPoet/Composer). Our contribution is a migration of the project to python3, allow training and loading of pytorch models, migration of the original model \"double autoencoder\" to pytorch and a comparison in code, training, performance and textual form of the original model to an ordinary autoencoder. The GUI application allowing the exploration of the latent space and further data processing and initial model in keras is hereby used with appropriate mention of the original author. This part again uses the pianoroll representation, but this time reduces songs of arbitrary length to songs of a fixed number of measures $m_1$ to $m_n$. Each measure is encoded into a window of size $96 x 96$, in which the pitch of a note is encoded as a 1 in the matrix. This represents a single strike without any length. The shape of the window was chosen due to 96 smallest common denominator for all commonly used time signatures.\n",
    "\n",
    "<img src=\"graphics/representation1.png\" width=\"40%\" alt=\"Single measure pianoroll representation\"/>\n",
    "\n",
    "In order to account for repetition of notes and similar motives across multiple measures, we introduce a third dimension, which is the number of measures.\n",
    "\n",
    "<img src=\"graphics/representation2.png\" width=\"40%\" />\n",
    "<img src=\"graphics/representation3.png\" width=\"40%\" />\n",
    "\n",
    "For the experiments below, the number of measures was chosen to be $16$ to capture most of the songs' lengths. The ones shorter than this were dropped in the preprocessing, the longer ones kept with all additional measures and added as samples with shifted starting points.\n",
    "\n",
    "<img src=\"graphics/representation4.png\" width=\"40%\" />\n",
    "\n",
    "This representation allows to encode arbitrary types of melodies, given that it can represent notes being played at the same time (chords) and the same notes several times in a row (length), although this was left out in the experiments below, as it reduces the simplicity of the songs. Instead, every note was extended until the next note is played, so that the piano strikes sound for longer and the chords can be heard more easily.\n",
    "\n",
    "### Network structure\n",
    "\n",
    "The 16 measures of pianoroll measures were now fed through an autoencoder with some special architecture, which could be considered as a double autoencoder. The general idea of it is to first encode every measure into a latent space of $200$ dimensions (encoder1), and from there take the latent spaces of all measures and encode them into an overall latent space of $120$ dimensions (encoder2). To again reach the 16 measures, the latent space is decoded into again 16 latent spaces of size 200, one for each measure (decoder2) and then decoded back into the actual measure windows (decoder1). It must be noted that the encoder1 and decoder1 each share the weights across all measures. This is done in order to make it learn a proper representation independent of measure index. Below is a graphic that represents the coarse structure of the network.\n",
    "\n",
    "<img src=\"graphics/arch.png\" width=\"40%\" />\n",
    "\n",
    "More details about the double autoencoder can be taken from the following table:\n",
    "\n",
    "```\n",
    "    _________________________________________________________________\n",
    "    Layer (type)                 Output Shape              Param #   \n",
    "    =================================================================\n",
    "    input_1 (InputLayer)         (None, 16, 96, 96)        0         \n",
    "    _________________________________________________________________\n",
    "    reshape_1 (Reshape)          (None, 16, 9216)          0         \n",
    "    _________________________________________________________________\n",
    "    time_distributed_1 (TimeDist (None, 16, 2000)          18434000  \n",
    "    _________________________________________________________________\n",
    "    time_distributed_2 (TimeDist (None, 16, 200)           400200    \n",
    "    _________________________________________________________________\n",
    "    flatten_1 (Flatten)          (None, 3200)              0         \n",
    "    _________________________________________________________________\n",
    "    dense_3 (Dense)              (None, 1600)              5121600   \n",
    "    _________________________________________________________________\n",
    "    dense_4 (Dense)              (None, 120)               192120    \n",
    "    _________________________________________________________________\n",
    "    encoder (BatchNormalization) (None, 120)               480       \n",
    "    _________________________________________________________________\n",
    "    decoder (Dense)              (None, 1600)              193600    \n",
    "    _________________________________________________________________\n",
    "    batch_normalization_1 (Batch (None, 1600)              6400      \n",
    "    _________________________________________________________________\n",
    "    activation_1 (Activation)    (None, 1600)              0         \n",
    "    _________________________________________________________________\n",
    "    dropout_1 (Dropout)          (None, 1600)              0         \n",
    "    _________________________________________________________________\n",
    "    dense_5 (Dense)              (None, 3200)              5123200   \n",
    "    _________________________________________________________________\n",
    "    reshape_2 (Reshape)          (None, 16, 200)           0         \n",
    "    _________________________________________________________________\n",
    "    time_distributed_3 (TimeDist (None, 16, 200)           800       \n",
    "    _________________________________________________________________\n",
    "    activation_2 (Activation)    (None, 16, 200)           0         \n",
    "    _________________________________________________________________\n",
    "    dropout_2 (Dropout)          (None, 16, 200)           0         \n",
    "    _________________________________________________________________\n",
    "    time_distributed_4 (TimeDist (None, 16, 2000)          402000    \n",
    "    _________________________________________________________________\n",
    "    time_distributed_5 (TimeDist (None, 16, 2000)          8000      \n",
    "    _________________________________________________________________\n",
    "    activation_3 (Activation)    (None, 16, 2000)          0         \n",
    "    _________________________________________________________________\n",
    "    dropout_3 (Dropout)          (None, 16, 2000)          0         \n",
    "    _________________________________________________________________\n",
    "    time_distributed_6 (TimeDist (None, 16, 9216)          18441216  \n",
    "    _________________________________________________________________\n",
    "    reshape_3 (Reshape)          (None, 16, 96, 96)        0         \n",
    "    =================================================================\n",
    "    Total params: 48,323,616\n",
    "    Trainable params: 48,315,776\n",
    "    Non-trainable params: 7,840\n",
    "    _________________________________________________________________\n",
    "```\n",
    "\n",
    "### Why not convolutions?\n",
    "\n",
    "Given the representation of the data, it could seem obvious that the choice to process it would be convolutions. However, convolutions are not as suitable for the task since the pianoroll representation is actually rather sparse, meaning the relevant notes forming a melody together are actually quite far apart. Therefore, it would require many convolutions to compress the representation into a dense one. Furthermore, pooling layers, as often used in CNNs, could not be used as the exact position of a note in the pianoroll is actually quite relevant compared to the location of a pixel in an image where we favor positional invariance over exact positions.\n",
    "\n",
    "<img src=\"graphics/not-convolution1.png\" width=\"80%\" />\n",
    "<img src=\"graphics/not-convolution2.png\" width=\"80%\" />\n",
    "\n",
    "Shown above are the more typical relations between notes in the pianoroll representation to form a proper melody and rhythm. To account for this difference, we use the shared encoder1 to turn all measures into dense representations, from which a larger encoder2 can learn to produce a dense representation. For this reason, the double autoencoder architecture was chosen. Its ability to share weights to encode and decode measures makes it a very suitable structure to learn from pianoroll representations of songs. In the alternative architectures section we will dive further into the reason why the double autoencoder is superior to a simple autoencoder.\n",
    "\n",
    "### Data Preprocessing\n",
    "\n",
    "To make the samples use a common range, the songs were shifted such that the song's center pitch conincides with the center pitch all songs in the dataset. By doing this, the number of samples for a certain pitch is increased. From there it is transformed into the pianoroll representation described above.\n",
    "\n",
    "\n",
    "### Generation mechanism\n",
    "\n",
    "Using this architecture, the network was trained to be able to encode the song into the latent space of 120 dimensions and from there to reconstruct the song into its original 16 measures, as it is the standard setting of training an autoencoder. In order to generate new songs, the decoder part of the autoencoder (hereafter called the decoder, even though it actually consists of decoder1 and decoder2) was kept as a song generator and the latent space was explored for interesting songs. After trying to apply variational methods to make navigation in the latent space easier, instead we stored some metrics on the latent space during training, namely mean and standard deviantions as if it were a normal distributed space, and pca vectors and values to know the axes of largest variation. Using these statistics and a small GUI to tune the latent space parameters along the PCA axes, new songs can be generated in an interactive manner. Furthermore, the note playing threshold can be varied, such that notes with a lower certainty of being played are kept silent instead. Certainty of a note comes from the fact that all notes of the reconstructed pianoroll are numbers $n \\in [0,1]$. Thus, a 1 can be interpreted as a note played with high certainty and a 0 can be interpreted as a note that should not be played.\n",
    "\n",
    "<img src=\"graphics/composer.png\" width=\"80%\" />\n",
    "\n",
    "### Results\n",
    "\n",
    "We present the results by showing generated examples from different steps during the training as it was done in the previous sections. Different from the previous sections, we do not provide validation or test losses because they make less sense except maybe to see if the autoencoder has found a more general representation in the latent space. But since the other two approaches already have a brittle verification for validation and test loss, we found it superfluous to add it here. \n",
    "\n",
    "<img src=\"graphics/losses-autoencoder.png\" width=\"60%\" />\n",
    "\n",
    "The figure above shows the training process of the described network architecture. Through a delicate reduction of the learning rate, we reach a binary crossentropy loss of $0.0008$ within 2000 epochs. Below we provide songs generated in the beginning and at the end of the training. \n",
    "\n",
    "\n",
    "#### **Alternative architectures / training methods and their impact**\n",
    "\n",
    "Before settling with the double autoencoder architecture, multiple versions of a simple autoencoder were tested to see how well songs can be reproduced using an ordinary autoencoder. Below are the tested architectures and the main reasons why they were discarded in favor of the double autoencoder.\n",
    "\n",
    "* Simple autoencoder (same layer sizes)\n",
    "\n",
    "The specification of the simple autoencoder using the exact same layer sizes as the double autoencoder is shown below:\n",
    "\n",
    "```\n",
    "    _________________________________________________________________\n",
    "    Layer (type)                 Output Shape              Param #   \n",
    "    =================================================================\n",
    "    input_1 (InputLayer)         (None, 16, 96, 96)        0         \n",
    "    _________________________________________________________________\n",
    "    reshape_1 (Reshape)          (None, 16, 9216)          0         \n",
    "    _________________________________________________________________\n",
    "    flatten_1 (Flatten)          (None, 147456)            0         \n",
    "    _________________________________________________________________\n",
    "    dense_1 (Dense)              (None, 2000)              294914000 \n",
    "    _________________________________________________________________\n",
    "    dense_2 (Dense)              (None, 200)               400200    \n",
    "    _________________________________________________________________\n",
    "    dense_3 (Dense)              (None, 1600)              321600    \n",
    "    _________________________________________________________________\n",
    "    dense_4 (Dense)              (None, 120)               192120    \n",
    "    _________________________________________________________________\n",
    "    encoder (BatchNormalization) (None, 120)               480       \n",
    "    _________________________________________________________________\n",
    "    decoder (Dense)              (None, 1600)              193600    \n",
    "    _________________________________________________________________\n",
    "    batch_normalization_1 (Batch (None, 1600)              6400      \n",
    "    _________________________________________________________________\n",
    "    activation_1 (Activation)    (None, 1600)              0         \n",
    "    _________________________________________________________________\n",
    "    dropout_1 (Dropout)          (None, 1600)              0         \n",
    "    _________________________________________________________________\n",
    "    dense_5 (Dense)              (None, 3200)              5123200   \n",
    "    _________________________________________________________________\n",
    "    batch_normalization_2 (Batch (None, 3200)              12800     \n",
    "    _________________________________________________________________\n",
    "    activation_2 (Activation)    (None, 3200)              0         \n",
    "    _________________________________________________________________\n",
    "    dropout_2 (Dropout)          (None, 3200)              0         \n",
    "    _________________________________________________________________\n",
    "    dense_6 (Dense)              (None, 2000)              6402000   \n",
    "    _________________________________________________________________\n",
    "    batch_normalization_3 (Batch (None, 2000)              8000      \n",
    "    _________________________________________________________________\n",
    "    activation_3 (Activation)    (None, 2000)              0         \n",
    "    _________________________________________________________________\n",
    "    dropout_3 (Dropout)          (None, 2000)              0         \n",
    "    _________________________________________________________________\n",
    "    dense_7 (Dense)              (None, 147456)            295059456 \n",
    "    _________________________________________________________________\n",
    "    reshape_2 (Reshape)          (None, 16, 96, 96)        0         \n",
    "    =================================================================\n",
    "    Total params: 602,633,856\n",
    "    Trainable params: 602,620,016\n",
    "    Non-trainable params: 13,840\n",
    "    _________________________________________________________________\n",
    "```\n",
    "\n",
    "A problem that comes to mind quickly when reading above specification is the immense number of parameters this model features. Similar to the problem one can solve with convolutions compared to dense layers, this network model gets big problems when learning from the pianoroll representation into a 2000 neuron layer densely connected to it (see number of parameters for dense_1 and dense_7 layers). As expected, this model did not fit into GPU memory for the hardware we had available. For that reason, it had to be discarded.\n",
    "\n",
    "* Simple autoencoder (reduced layer size nearest to the data representation)\n",
    "\n",
    "To reduce the size of the autoencoder model shown above, we tried to reduce the size of the dense layers nearest to the original piano representation from 2000 neurons to 400 neurons (compare the number of parameters for dense_1 and dense_7 in the specification above with this one here). \n",
    "\n",
    "```\n",
    "    _________________________________________________________________\n",
    "    Layer (type)                 Output Shape              Param #   \n",
    "    =================================================================\n",
    "    input_1 (InputLayer)         (None, 16, 96, 96)        0         \n",
    "    _________________________________________________________________\n",
    "    reshape_1 (Reshape)          (None, 16, 9216)          0         \n",
    "    _________________________________________________________________\n",
    "    flatten_1 (Flatten)          (None, 147456)            0         \n",
    "    _________________________________________________________________\n",
    "    dense_1 (Dense)              (None, 400)               58982800  \n",
    "    _________________________________________________________________\n",
    "    dense_2 (Dense)              (None, 200)               80200     \n",
    "    _________________________________________________________________\n",
    "    dense_3 (Dense)              (None, 1600)              321600    \n",
    "    _________________________________________________________________\n",
    "    dense_4 (Dense)              (None, 120)               192120    \n",
    "    _________________________________________________________________\n",
    "    encoder (BatchNormalization) (None, 120)               480       \n",
    "    _________________________________________________________________\n",
    "    decoder (Dense)              (None, 1600)              193600    \n",
    "    _________________________________________________________________\n",
    "    batch_normalization_1 (Batch (None, 1600)              6400      \n",
    "    _________________________________________________________________\n",
    "    activation_1 (Activation)    (None, 1600)              0         \n",
    "    _________________________________________________________________\n",
    "    dropout_1 (Dropout)          (None, 1600)              0         \n",
    "    _________________________________________________________________\n",
    "    dense_5 (Dense)              (None, 3200)              5123200   \n",
    "    _________________________________________________________________\n",
    "    batch_normalization_2 (Batch (None, 3200)              12800     \n",
    "    _________________________________________________________________\n",
    "    activation_2 (Activation)    (None, 3200)              0         \n",
    "    _________________________________________________________________\n",
    "    dropout_2 (Dropout)          (None, 3200)              0         \n",
    "    _________________________________________________________________\n",
    "    dense_6 (Dense)              (None, 400)               1280400   \n",
    "    _________________________________________________________________\n",
    "    batch_normalization_3 (Batch (None, 400)               1600      \n",
    "    _________________________________________________________________\n",
    "    activation_3 (Activation)    (None, 400)               0         \n",
    "    _________________________________________________________________\n",
    "    dropout_3 (Dropout)          (None, 400)               0         \n",
    "    _________________________________________________________________\n",
    "    dense_7 (Dense)              (None, 147456)            59129856  \n",
    "    _________________________________________________________________\n",
    "    reshape_2 (Reshape)          (None, 16, 96, 96)        0         \n",
    "    =================================================================\n",
    "    Total params: 125,325,056\n",
    "    Trainable params: 125,314,416\n",
    "    Non-trainable params: 10,640\n",
    "    _________________________________________________________________\n",
    "```\n",
    "\n",
    "This model still has $\\approx 2.6$ times the parameters of the double autoencoder, but at least it can be trained to a binary crossentropy of $0.0043$ and produces some decent pianorolls. But it shows that prior knowledge (such as knowing that measures of a pianoroll repeat notes and provide rhythm) about the problem can easily reduce the size of the model by many parameters and can speed up the training and split the amount of hardware required for training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audio sample of untrained network\n",
    "A sample of the untrained network is nearly impossible to listen to since the reconstruction of a song is basically random noise. Depending on the threshold of certainty, most notes are played or most notes are not played. In most cases the notes are predicted with low certainty and thus not played at all. Below are two samples of how the pianorolls look depending on the threshold (control in red).\n",
    "\n",
    "<img src=\"graphics/composer-untrained.png\" width=\"50%\" />\n",
    "\n",
    "<img src=\"graphics/composer-untrained2.png\" width=\"50%\" />\n",
    "\n",
    "\n",
    "Below a sample is provided which can not be accurately played on most midi sequencers and thus sounds different depending on the player used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "\n",
    "filename = 'results/double_autoencoder_untrained.mid'\n",
    "pygame.init()\n",
    "pygame.mixer.music.load(filename)\n",
    "pygame.mixer.music.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.mixer.music.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audio samples of the network at the end of training\n",
    "After 2000 epochs, the training reaches a binary cross entropy loss of $0.0008$. The model now produces interesting songs such as the samples below. Moreover, varying configuration of setting the PCA components produce good examples, which is not always to be expected from a non-restricted latent space as in the case of an autoencoder. Below is an depiction of the composer while playing one of the songs.\n",
    "\n",
    "<img src=\"graphics/composer-trained.png\" width=\"50%\" />\n",
    "\n",
    "\n",
    "Furthermore we provide three examples of generated songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pygame\n",
    "\n",
    "pygame.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'results/double_autoencoder_trained.mid'\n",
    "pygame.mixer.music.load(filename)\n",
    "pygame.mixer.music.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'results/double_autoencoder_trained2.mid'\n",
    "pygame.mixer.music.load(filename)\n",
    "pygame.mixer.music.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'results/double_autoencoder_trained3.mid'\n",
    "pygame.mixer.music.load(filename)\n",
    "pygame.mixer.music.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.mixer.music.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion of results\n",
    "\n",
    "Listening to the results of this model as well as navigating through the latent space of the decoder to produce new music definitely is an interesting and quite fun sort of task. Given the simplicity of the model, although not as simple as the two other approaches, one wonders how easy it seems to for the model to understand music and produce music.\n",
    "\n",
    "As admitted before, there are some well-chosen assumptions to how notes fit together in piano roll or that the overall structure of a song is not random. However, it still does not seize to amaze that there is such a rigorous structure in a song that we can compress the large amount of played notes into 120 dimensions.\n",
    "\n",
    "It can definitely be argued that an autoencoder might do an overfit on its training data, and the generated music just copies a lot of the learned passages into the newly generated songs. It is probably true to some extent, but it is rather impossible to deny that famous artists, each on their own and among themselves, do the same. Thus, experiments such as this one give some insight into what creativity maybe really is - a unexpected turn in the journey of the mind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplary usage of the code\n",
    "#### Training the network model and generating with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "H6WgmON2NCw_",
    "outputId": "af0fd05b-61a8-4efb-e477-4e701fbc063f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading songs...\n",
      "Saving 5856 samples...\n",
      "Done:  148 succeded, 170 ignored, 39 failed of 357 in total\n"
     ]
    }
   ],
   "source": [
    "from src.preprocess_songs import preprocess_songs\n",
    "\n",
    "data_folder = ['./data/raw/bach']\n",
    "preprocess_songs(data_folder, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1814
    },
    "colab_type": "code",
    "id": "_uMBk0HlNCxG",
    "outputId": "70d31a5d-a210-40d4-f7dc-6014b7cd77ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n",
      "Loaded 5856 samples from 296 songs.\n",
      "5856\n",
      "Preparing song samples, padding songs...\n",
      "Building model...\n",
      "autoencoder with [18432000, 2000, 400000, 200, 5120000, 1600, 192000, 120, 120, 120, 192000, 1600, 1600, 1600, 5120000, 3200, 200, 200, 400000, 2000, 2000, 2000, 18432000, 9216] parameters (total 48315776)\n",
      "Referencing sub-models...\n",
      "Training model...\n",
      "Training epoch:  0 of 2000\n",
      "Train loss: 0.7016901\n",
      "Latent Mean values:  [ 0.35031584 -0.6837522   0.0716726   0.5131773  -0.28833637 -0.6338653 ]\n",
      "Latent PCA values:  [7.4548001  4.48367349 1.43075411 0.35562098 0.17477476 0.12961965]\n",
      "Training epoch:  100 of 2000\n",
      "Train loss: 0.0061040055\n",
      "Latent Mean values:  [-0.11352142 -0.47778505 -0.04603821 -0.07469188 -0.05634077 -0.01631509]\n",
      "Latent PCA values:  [5.50997962 3.07182901 1.75132933 1.04471669 0.36450786 0.23621654]\n",
      "Training epoch:  200 of 2000\n",
      "Train loss: 0.005805918\n",
      "Latent Mean values:  [-0.03825781 -0.2942176   0.0614741   0.0461343  -0.19231619 -0.04163292]\n",
      "Latent PCA values:  [5.38525987 2.24391116 1.63856949 1.41931778 0.90838175 0.53801173]\n",
      "Training epoch:  300 of 2000\n",
      "Train loss: 0.005627179\n",
      "Latent Mean values:  [-0.04470701 -0.12542942 -0.01712596  0.0518306   0.21508345 -0.03999072]\n",
      "Latent PCA values:  [5.28536724 1.96239685 1.48579645 1.42509849 1.19020138 0.68281979]\n",
      "Training epoch:  400 of 2000\n",
      "Train loss: 0.0054608844\n",
      "Latent Mean values:  [ 0.04948261  0.08169031  0.08518728 -0.0896747  -0.15981555  0.05043337]\n",
      "Latent PCA values:  [4.95622041 2.0015272  1.42079198 1.38660163 1.23912143 0.83841895]\n",
      "Training epoch:  500 of 2000\n",
      "Train loss: 0.0052916966\n",
      "Latent Mean values:  [ 0.10718772  0.09590552 -0.00173828  0.06143896  0.00686981 -0.04389905]\n",
      "Latent PCA values:  [4.40644533 1.94406945 1.52592222 1.39818475 1.29067403 1.0366622 ]\n",
      "Training epoch:  600 of 2000\n",
      "Train loss: 0.005057904\n",
      "Latent Mean values:  [ 0.02654425  0.04100018 -0.02034719  0.0790131   0.06155061  0.0769737 ]\n",
      "Latent PCA values:  [4.05261918 1.89120086 1.62203302 1.39413523 1.35266963 1.20410354]\n",
      "Training epoch:  700 of 2000\n",
      "Train loss: 0.0046871295\n",
      "Latent Mean values:  [ 0.12768373  0.04149197  0.11976857  0.19295688 -0.02211795 -0.05330378]\n",
      "Latent PCA values:  [3.81086357 1.77324172 1.67673533 1.57250412 1.36750622 1.28974321]\n",
      "Training epoch:  800 of 2000\n",
      "Train loss: 0.0043140734\n",
      "Latent Mean values:  [ 0.11566233  0.06478174 -0.0138248   0.0823385   0.11906825 -0.13493708]\n",
      "Latent PCA values:  [3.14589493 1.88849965 1.64421573 1.59584702 1.46814351 1.31249948]\n",
      "Training epoch:  900 of 2000\n",
      "Train loss: 0.0039009757\n",
      "Latent Mean values:  [ 0.31134862  0.06828686  0.03875175  0.13882796  0.04090066 -0.16764471]\n",
      "Latent PCA values:  [2.89860205 1.7785696  1.71423016 1.59967899 1.45550443 1.35891265]\n",
      "Training epoch:  1000 of 2000\n",
      "Train loss: 0.003574095\n",
      "Latent Mean values:  [ 0.01409863 -0.11117499 -0.01940465 -0.11684772  0.0760522   0.03319116]\n",
      "Latent PCA values:  [2.68919736 1.8628711  1.71620749 1.64176449 1.43493741 1.38182383]\n",
      "Training epoch:  1100 of 2000\n",
      "Train loss: 0.0032472832\n",
      "Latent Mean values:  [ 0.17662683  0.17996965 -0.0634294   0.03768057  0.08145173 -0.07757968]\n",
      "Latent PCA values:  [2.34697913 1.89681574 1.71367765 1.68547347 1.4984671  1.365017  ]\n",
      "Training epoch:  1200 of 2000\n",
      "Train loss: 0.0029403137\n",
      "Latent Mean values:  [ 0.14112008 -0.10468852 -0.0276693   0.01710643 -0.15032457  0.07804718]\n",
      "Latent PCA values:  [2.29876872 1.84966865 1.67490074 1.63966302 1.54222945 1.35264109]\n",
      "Training epoch:  1300 of 2000\n",
      "Train loss: 0.0026673223\n",
      "Latent Mean values:  [ 0.15082867 -0.07839765  0.00274067  0.07877706 -0.11910306 -0.07546919]\n",
      "Latent PCA values:  [2.12057481 1.77411807 1.69339543 1.59373829 1.47308217 1.35374345]\n",
      "Training epoch:  1400 of 2000\n",
      "Train loss: 0.002362284\n",
      "Latent Mean values:  [ 0.1721249  -0.05061528  0.05885211  0.04608156 -0.06638826 -0.07363226]\n",
      "Latent PCA values:  [2.14897079 1.82029583 1.68054662 1.56392651 1.49111539 1.34394209]\n",
      "Training epoch:  1500 of 2000\n",
      "Train loss: 0.0021556618\n",
      "Latent Mean values:  [ 0.00990191 -0.03542675 -0.06046242 -0.0577714   0.05593542 -0.02290881]\n",
      "Latent PCA values:  [2.07338391 1.7112743  1.66545646 1.59673406 1.44106692 1.35317691]\n",
      "Training epoch:  1600 of 2000\n",
      "Train loss: 0.0019849285\n",
      "Latent Mean values:  [-0.04163764 -0.0536213  -0.01792208 -0.04225955  0.01586799 -0.03600778]\n",
      "Latent PCA values:  [1.91536396 1.72717698 1.66039114 1.55952441 1.48135762 1.36559833]\n",
      "Training epoch:  1700 of 2000\n",
      "Train loss: 0.0017878404\n",
      "Latent Mean values:  [ 0.14056253 -0.02487973 -0.06727844 -0.01387037  0.1307027  -0.03119221]\n",
      "Latent PCA values:  [1.90742157 1.6932927  1.64591033 1.55414461 1.4694518  1.32419784]\n",
      "Training epoch:  1800 of 2000\n",
      "Train loss: 0.0015942794\n",
      "Latent Mean values:  [ 0.06345963 -0.16989839 -0.00687807 -0.05319322 -0.10336883 -0.06980252]\n",
      "Latent PCA values:  [1.84518845 1.65069131 1.6137704  1.53081829 1.44844123 1.35066747]\n",
      "Training epoch:  1900 of 2000\n",
      "Train loss: 0.001504882\n",
      "Latent Mean values:  [ 0.02204046 -0.12890011  0.04100069  0.09677642 -0.03476395 -0.02335605]\n",
      "Latent PCA values:  [1.84365831 1.69140053 1.59482125 1.54803984 1.47025463 1.31634389]\n",
      "...Done.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQYa7JDXIcJekBhnuktSg/wNTc34DvyN+DAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.train_pytorch import train as train_pytorch\n",
    "train_keras = False\n",
    "\n",
    "if train_keras:\n",
    "    trained_keras_model, training_losses = train_keras('data/interim/samples.npy', 'data/interim/lengths.npy', use_double_autoencoder=False, epochs_qty=2000)\n",
    "else:\n",
    "    trained_pytorch_model, training_losses = train_pytorch('data/interim/samples.npy', 'data/interim/lengths.npy', 2000, verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    },
    "colab_type": "code",
    "id": "84MkfycpNCxK",
    "outputId": "3afa73d8-0645-42ef-bdc8-b16af4a35ef4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f3eea636160>"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjkAAAFBCAYAAACVcr5cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHDMzM2skd3LMzMyskf4HiEDLbo1bLrsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 648x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot loss\n",
    "plt.figure(figsize=(9,5))\n",
    "x = range(len(training_losses))\n",
    "plt.ylim([0.0, 0.009])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Binary Cross Entropy Loss')\n",
    "plt.plot(x, training_losses, label='Train')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading encoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras version: 2.2.4\n",
      "Loading gaussian/pca statistics...\n",
      "Loading songs...\n"
     ]
    }
   ],
   "source": [
    "# run the composer and play with it.\n",
    "#\n",
    "# Controls:\n",
    "#  Right Click - Reset all sliders\n",
    "#          'E' - Randomize Song (2 std deviations)\n",
    "#          'R' - Random Song (1 std deviation)\n",
    "#          'T' - Randomize off-screen sliders\n",
    "#          'M' - Save song as .mid file\n",
    "#          'W' - Save song as .wav file\n",
    "#      'Space' - Play/Pause\n",
    "#        'Tab' - Seek to start of song\n",
    "#          '1' - Square wave instrument\n",
    "#          '2' - Sawtooth wave instrument\n",
    "#          '3' - Triangle wave instrument\n",
    "#          '4' - Sine wave instrument\n",
    "#\n",
    "\n",
    "from src.composer import play\n",
    "\n",
    "if train_keras:\n",
    "    framework='keras'\n",
    "else:\n",
    "    framework='torch'\n",
    "\n",
    "play(framework=framework, model_folder_name=\"results/history\", pca_stats_folder_name=\"results/history/e2000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the pretrained model and generating with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and unzip model file\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "from src.download_gdrive import download_file_from_google_drive\n",
    "\n",
    "if not os.path.exists('results/history'):\n",
    "    os.makedirs('results/history')\n",
    "\n",
    "# download model zip\n",
    "# if this does not work out, just download the model zip manually here:\n",
    "# https://drive.google.com/file/d/17tzVhyYrDoNt5GLo0j089XyWSUk33oXH/view?usp=sharing\n",
    "file_id = '17tzVhyYrDoNt5GLo0j089XyWSUk33oXH'\n",
    "#download_file_from_google_drive(file_id, './results/history/model-keras.zip')\n",
    "\n",
    "# move model and epoch stats to results/history if stats and model are not already available\n",
    "if os.path.exists('results/history/e2000') or os.path.exists('results/history/model.h5'):\n",
    "    raise Exception('There is already a stats folder or a model.h5 in {}. Please move them.'.format(move_to_dir))\n",
    "\n",
    "os.chdir('results/history')\n",
    "# extract it into the results/history folder\n",
    "with ZipFile('model-keras.zip', 'r') as zip_obj:\n",
    "   # extract all the contents of zip file in current directory\n",
    "   zip_obj.extractall()\n",
    "    \n",
    "os.chdir('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading encoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras version: 2.2.4\n",
      "Loading gaussian/pca statistics...\n",
      "Loading songs...\n"
     ]
    }
   ],
   "source": [
    "# run the composer and play with it.\n",
    "#\n",
    "# Controls:\n",
    "#  Right Click - Reset all sliders\n",
    "#          'E' - Randomize Song (2 std deviations)\n",
    "#          'R' - Random Song (1 std deviation)\n",
    "#          'T' - Randomize off-screen sliders\n",
    "#          'M' - Save song as .mid file\n",
    "#          'W' - Save song as .wav file\n",
    "#      'Space' - Play/Pause\n",
    "#        'Tab' - Seek to start of song\n",
    "#          '1' - Square wave instrument\n",
    "#          '2' - Sawtooth wave instrument\n",
    "#          '3' - Triangle wave instrument\n",
    "#          '4' - Sine wave instrument\n",
    "#\n",
    "\n",
    "from src.composer import play\n",
    "\n",
    "load_keras = True\n",
    "\n",
    "if load_keras:\n",
    "    framework='keras'\n",
    "else:\n",
    "    framework='torch'\n",
    "\n",
    "play(framework=framework, model_folder_name=\"results/history\", pca_stats_folder_name=\"results/history/e2000\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
