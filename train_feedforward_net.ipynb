{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train feedforward net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import src.midi_utils as midi_utils\n",
    "\n",
    "import pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_filename = 'midi_data/bwv104.6.mid'\n",
    "# midi_filename = 'midi_data/pkgsc_azalea.mid'\n",
    "pygame.init()\n",
    "pygame.mixer.music.load(midi_filename)\n",
    "pygame.mixer.music.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.mixer.music.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tracks: 357\n",
      "[3.52558786 3.51216718 3.51722354 3.52443981 3.52945475 3.53726793\n",
      " 3.53997164 3.54263103 3.54202568 3.54776059]\n",
      "Mean of the dataset: [70.62322716 70.66461288 70.68623576 70.70378982 70.71535689 70.71698442\n",
      " 70.71221809 70.70268542 70.68745641 70.65606836]\n",
      "Number of samples: 17204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\i0325777\\appdata\\local\\continuum\\miniconda3\\envs\\tensorflow-base\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "c:\\users\\i0325777\\appdata\\local\\continuum\\miniconda3\\envs\\tensorflow-base\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "c:\\users\\i0325777\\appdata\\local\\continuum\\miniconda3\\envs\\tensorflow-base\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "c:\\users\\i0325777\\appdata\\local\\continuum\\miniconda3\\envs\\tensorflow-base\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "c:\\users\\i0325777\\appdata\\local\\continuum\\miniconda3\\envs\\tensorflow-base\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "c:\\users\\i0325777\\appdata\\local\\continuum\\miniconda3\\envs\\tensorflow-base\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "from src.dataset_utils import TrackDataset, get_dataset_representation_from_tracks\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "feature_scaler = StandardScaler()\n",
    "label_scaler = StandardScaler()\n",
    "\n",
    "tracks = []\n",
    "# iterate over all midi files of folder\n",
    "import glob\n",
    "import os\n",
    "\n",
    "try:\n",
    "    home_dir\n",
    "except NameError:\n",
    "    home_dir = os.getcwd()\n",
    "\n",
    "os.chdir(home_dir + \"/midi_data/bach\")  # go to a folder relative to home dir\n",
    "for midi_file in glob.glob(\"*.mid\"):\n",
    "    # get a list of all soprano tracks\n",
    "    ## load midi file\n",
    "    csv_text = midi_utils.load_to_csv(midi_file)\n",
    "\n",
    "    ## Split into tracks\n",
    "    track_dict = midi_utils.split_tracks(csv_text)\n",
    "    track_nr = '1'\n",
    "\n",
    "    ## Generating numpy array with notes\n",
    "    track = midi_utils.midi_track_to_numpy(track_dict[track_nr])\n",
    "    tracks.append(track)\n",
    "    \n",
    "print(\"Number of tracks: \" + str(len(tracks)))\n",
    "\n",
    "x, y = get_dataset_representation_from_tracks(tracks)\n",
    "\n",
    "x = np.stack(x)\n",
    "x = x[:,:,0]\n",
    "\n",
    "y = np.stack(y)\n",
    "y = y[:,:,0]\n",
    "\n",
    "feature_scaler.fit(x)\n",
    "\n",
    "print(\"Mean of the dataset: \" + str(feature_scaler.mean_))\n",
    "\n",
    "x = feature_scaler.fit_transform(x)\n",
    "\n",
    "label_scaler.fit(y)\n",
    "\n",
    "y = label_scaler.fit_transform(y)\n",
    "\n",
    "\n",
    "print(\"Number of samples: \" + str(len(x)))\n",
    "\n",
    "mini_batch_size = 32\n",
    "\n",
    "# for now, we only train on the pitches of the notes\n",
    "train_dataset = TrackDataset(x, y, drop_length=False)  # make training dataset\n",
    "#validation_dataset = TrackDataset(val_images, val_centers)  # make validation dataset\n",
    "#test_dataset = TrackDataset(test_images, test_centers)  # make test dataset\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=mini_batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(train_dataset, batch_size=mini_batch_size, shuffle=True) # TODO TODO TODO: CHANGE TO A SUITABLE VALIDATIONSET\n",
    "#test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size 17204\n",
      "Input size 10/ output size 1/ learning rate 0.001\n",
      "Input example tensor([0.9578, 1.8038, 0.9422, 0.9352, 0.9306, 0.9281, 1.2112, 0.9308, 0.3706,\n",
      "        0.3788])\n",
      "Output example tensor([-0.1690])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "print(\"Training set size\", len(train_dataset))\n",
    "\n",
    "input_size = len(train_dataset[0][0])  # get input size\n",
    "input_example = train_dataset[0][0]\n",
    "output_size = len(train_dataset[0][1])  # get output size\n",
    "output_example = train_dataset[0][1]\n",
    "learning_rate = 0.001\n",
    "\n",
    "print(\"Input size {}/ output size {}/ learning rate {}\".format(input_size, output_size, learning_rate))\n",
    "print(\"Input example {}\".format(input_example))\n",
    "print(\"Output example {}\".format(output_example))\n",
    "\n",
    "\n",
    "class LinearModel(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dimension, output_dimension):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.fc = torch.nn.Linear(input_dimension, output_dimension, bias=True)  # linear layer with parameters A, b\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        output = self.fc(input_data)  # applies out = input * A + b. A, b are parameters of nn.Linear that we want to learn\n",
    "        return output\n",
    "    \n",
    "\n",
    "class MLPModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLPModel, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.layers(input)\n",
    "    \n",
    "device = torch.device('cpu')\n",
    "    \n",
    "# linear_model = LinearModel(input_size, output_size)\n",
    "# \n",
    "# linear_model = linear_model.to(device)\n",
    "# \n",
    "mlp_model = MLPModel(input_size, 64, output_size)\n",
    "\n",
    "mlp_model = mlp_model.to(device)\n",
    "\n",
    "model = mlp_model\n",
    "\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader, optimizer, n_epochs, loss_function, device=torch.device('cpu'), verbose=1):\n",
    "    # We will monitor loss functions as the training progresses\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # training phase\n",
    "        model.train()\n",
    "        # Iterate mini batches over training dataset\n",
    "        losses = []\n",
    "        for x, y in train_dataloader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            output = model(x)  # predict output from input\n",
    "            \n",
    "            # set gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = loss_function(output, y)\n",
    "            if verbose > 2:\n",
    "                print(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Metrics\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "        train_losses.append(np.mean(np.array(losses)))\n",
    "\n",
    "        # Evaluation phase\n",
    "        model.eval()\n",
    "        # iterate mini batches over validation set\n",
    "        # We don't need gradients\n",
    "        losses = []\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_dataloader:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                output = model(x)\n",
    "                loss = loss_function(output, y)\n",
    "                if verbose > 1:\n",
    "                    print(loss.item())\n",
    "\n",
    "                losses.append(loss.item())\n",
    "        val_losses.append(np.mean(np.array(losses)))\n",
    "        \n",
    "        if verbose > 0:\n",
    "            print('Epoch {}/{}: train_loss: {:.4f}, val_loss: {:.4f}'.format(epoch + 1, n_epochs, train_losses[-1], val_losses[-1]))\n",
    "    return train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: train_loss: 0.9173, val_loss: 0.8160\n",
      "Epoch 2/50: train_loss: 0.7116, val_loss: 0.6203\n",
      "Epoch 3/50: train_loss: 0.5658, val_loss: 0.5228\n",
      "Epoch 4/50: train_loss: 0.4958, val_loss: 0.4738\n",
      "Epoch 5/50: train_loss: 0.4603, val_loss: 0.4492\n",
      "Epoch 6/50: train_loss: 0.4421, val_loss: 0.4353\n",
      "Epoch 7/50: train_loss: 0.4311, val_loss: 0.4265\n",
      "Epoch 8/50: train_loss: 0.4235, val_loss: 0.4205\n",
      "Epoch 9/50: train_loss: 0.4187, val_loss: 0.4166\n",
      "Epoch 10/50: train_loss: 0.4158, val_loss: 0.4141\n",
      "Epoch 11/50: train_loss: 0.4131, val_loss: 0.4121\n",
      "Epoch 12/50: train_loss: 0.4113, val_loss: 0.4104\n",
      "Epoch 13/50: train_loss: 0.4103, val_loss: 0.4099\n",
      "Epoch 14/50: train_loss: 0.4094, val_loss: 0.4084\n",
      "Epoch 15/50: train_loss: 0.4085, val_loss: 0.4075\n",
      "Epoch 16/50: train_loss: 0.4077, val_loss: 0.4068\n",
      "Epoch 17/50: train_loss: 0.4069, val_loss: 0.4062\n",
      "Epoch 18/50: train_loss: 0.4062, val_loss: 0.4060\n",
      "Epoch 19/50: train_loss: 0.4061, val_loss: 0.4055\n",
      "Epoch 20/50: train_loss: 0.4058, val_loss: 0.4051\n",
      "Epoch 21/50: train_loss: 0.4051, val_loss: 0.4049\n",
      "Epoch 22/50: train_loss: 0.4048, val_loss: 0.4043\n",
      "Epoch 23/50: train_loss: 0.4045, val_loss: 0.4036\n",
      "Epoch 24/50: train_loss: 0.4042, val_loss: 0.4038\n",
      "Epoch 25/50: train_loss: 0.4037, val_loss: 0.4033\n",
      "Epoch 26/50: train_loss: 0.4033, val_loss: 0.4031\n",
      "Epoch 27/50: train_loss: 0.4031, val_loss: 0.4027\n",
      "Epoch 28/50: train_loss: 0.4030, val_loss: 0.4025\n",
      "Epoch 29/50: train_loss: 0.4027, val_loss: 0.4021\n",
      "Epoch 30/50: train_loss: 0.4025, val_loss: 0.4018\n",
      "Epoch 31/50: train_loss: 0.4020, val_loss: 0.4021\n",
      "Epoch 32/50: train_loss: 0.4019, val_loss: 0.4017\n",
      "Epoch 33/50: train_loss: 0.4018, val_loss: 0.4011\n",
      "Epoch 34/50: train_loss: 0.4015, val_loss: 0.4011\n",
      "Epoch 35/50: train_loss: 0.4010, val_loss: 0.4025\n",
      "Epoch 36/50: train_loss: 0.4012, val_loss: 0.4007\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-a6af6ff2222e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtrain_losses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_losses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-74-da6a4c7876ed>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, val_dataloader, optimizer, n_epochs, loss_function, device, verbose)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# predict output from input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the linear model and plot how the loss changes as the \n",
    "# training progresses for both training and validation set.\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "train_losses, val_losses = train(model, train_loader, validation_loader, optimizer, 50, criterion, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[74. 70.]\n",
      " [71. 72.]\n",
      " [74. 75.]\n",
      " [71. 73.]\n",
      " [74. 72.]\n",
      " [64. 68.]\n",
      " [69. 69.]\n",
      " [74. 75.]\n",
      " [71. 72.]\n",
      " [73. 73.]\n",
      " [73. 69.]\n",
      " [72. 73.]\n",
      " [76. 75.]\n",
      " [74. 74.]\n",
      " [71. 71.]\n",
      " [65. 68.]\n",
      " [77. 74.]\n",
      " [66. 67.]\n",
      " [75. 75.]\n",
      " [72. 74.]\n",
      " [65. 68.]\n",
      " [71. 67.]\n",
      " [69. 67.]\n",
      " [77. 77.]\n",
      " [64. 67.]\n",
      " [73. 73.]\n",
      " [74. 75.]\n",
      " [66. 67.]\n",
      " [67. 68.]\n",
      " [67. 66.]\n",
      " [76. 74.]\n",
      " [67. 70.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xd4nNWZ/vHvo1HvvVlWcZEbxQbb2IbQQsCU4EBoBodkQ8ImWUhCwm7gl/wSwm7CpmzKFUpCCGGTUI0pDjGYZgIGNxnce5GLZKva6l1n/5ixEUa2BYw8mpn7c11zad5XRzPPC+LW4Zwz5zXnHCIiEloiAl2AiIj4n8JdRCQEKdxFREKQwl1EJAQp3EVEQpDCXUQkBCncRURCkMJdRCQEKdxFREJQZKDeODMz0xUXFwfq7UVEgtLKlStrnXNZx2sXsHAvLi6mrKwsUG8vIhKUzGzXQNppWEZEJAQp3EVEQpDCXUQkBCncRURCkMJdRCQEKdxFREKQwl1EJAQFXbivKK/nv1/chG4PKCJydEEX7usqGvj9P7dT19IZ6FJERIasAYW7mc00s81mts3M7ujn+0Vm9pqZrTGzN8yswP+lehVnJACwq65lsN5CRCToHTfczcwD3AdcDIwHZpvZ+COa/RL4i3PuFOBu4B5/F3pIUUY8AOW1rYP1FiIiQW8gPfepwDbn3A7nXCfwBDDriDbjgdd8zxf1832/KUiLJ8LUcxcROZaBhPswYE+f472+c32tBj7ve34FkGRmGZ+8vA+LjoxgWFoc5XXquYuIHM1Awt36OXfkUpXbgXPM7D3gHKAC6P7QC5ndbGZlZlZWU1PzkYs9pDgjQT13EZFjGEi47wWG9zkuACr7NnDOVTrnrnTOTQK+7zvXcOQLOecedM5Nds5Nzso67nbER1WckaCeu4jIMQwk3FcAo82sxMyigeuA+X0bmFmmmR16rTuBh/1b5gcVZcTT0NbFAS2HFBHp13HD3TnXDdwCLAQ2Ak8559ab2d1mdrmv2bnAZjPbAuQAPxmkeoH3l0OWa2hGRKRfA7oTk3NuAbDgiHM/7PP8aeBp/5Z2dMWZ3uWQu+pamVSYdqLeVkQkaATdJ1TBuxzSTD13EZGjCcpwj43ykJ8Sxy5NqoqI9Csowx28QzPquYuI9C9ow70oI4HyWoW7iEh/gjbcizPiOdDaRUNrV6BLEREZcoI23IsO7Q5Zr967iMiRgjbc31/rrklVEZEjBW24F6b71rpr3F1E5EOCNtzjoj3kJseq5y4i0o+gDXfQckgRkaMJ7nDX1r8iIv0K6nAvykigtrmTpnYthxQR6Suow7044/0NxERE5H1BHe6H17or3EVEPiDIw93bc9ekqojIBwV1uCfERJKVFKNJVRGRIwRfuO96BxZ+H5z3Ht0lGQmU12pYRkSkr+AL9/3rYMm90FgBeIdmNCwjIvJBwRfu+RO9XytXAVCcmUB1Uwetnd0BLEpEZGgJvnDPOQksAvZ5w71IyyFFRD4k+MI9Oh6yxr3fcz+8HFJDMyIihwRfuIN3aKbyPXCOwsPLIdVzFxE5JDjDPW8itNZCYwXJsVFkJETrlnsiIn0EZ7j3M6mqFTMiIu8LznDvZ1JVE6oiIu8LznCPjoessR+YVN3X0E57V0+ACxMRGRqCM9zBO+6+bxU4d3g55O569d5FRCCYwz1/IrTUQGPl+zfL1qSqiAgQzOGe55tU3beqz1p39dxFRCCYwz33ZO+kauUqUuKjSI2PYqdWzIiIAMEc7ocmVfe9P6mqT6mKiHgFb7iDd2im0jupWpwRr61/RUR8gjvc8ydCSzU07aMoI4HKhjY6urUcUkRkQOFuZjPNbLOZbTOzO/r5fqGZLTKz98xsjZld4v9S+3FoUrXyPYoz43EO9tS3nZC3FhEZyo4b7mbmAe4DLgbGA7PNbPwRzX4APOWcmwRcB9zv70L71WdSdVRWEgDrKxtOyFuLiAxlA+m5TwW2Oed2OOc6gSeAWUe0cUCy73kKUOm/Eo8hOh4yx8C+VYzLSyIpNpKlO+pOyFuLiAxlAwn3YcCePsd7fef6uguYY2Z7gQXArf29kJndbGZlZlZWU1PzMcrtR753UjUywjijJIN3tivcRUQGEu7Wzzl3xPFs4BHnXAFwCfBXM/vQazvnHnTOTXbOTc7Kyvro1fYn7/1J1ekjM9hV10rFQY27i0h4G0i47wWG9zku4MPDLjcBTwE455YAsUCmPwo8rvxJ3q+Vq5gxMgOAJeq9i0iYG0i4rwBGm1mJmUXjnTCdf0Sb3cCnAcxsHN5w99O4y3EcmlTdt4oxOUmkxUfxzvbaE/LWIiJD1XHD3TnXDdwCLAQ24l0Vs97M7jazy33Nvgt81cxWA48DX3LOHTl0MzgOTapWriIiwpg+MoOl2+s4UW8vIjIURQ6kkXNuAd6J0r7nftjn+QbgTP+W9hHkT4TtrwMwfWQmC9buZ1ddK8WZCQErSUQkkIL7E6qH5E2E5ipo3Mf0Eb5xdy2JFJEwFhrhnv/+9r8jsxLITorRkkgRCWuhEe59Pqlq5h13X6JxdxEJY6ER7tEJkFl6ePvfGSMzqG3uYFt1c4ALExEJjNAId/Bt//seADNGepfYa2hGRMJV6IR7/qTDk6rD0+MZlhqnDzOJSNgKnXAvmOz9uuttwDs0s2RHHb29GncXkfATOuGefxok5sKG5wGYPjKDhrYuNuxrDHBhIiInXuiEe0QEjPssbH0FOluY7ttnRlsAi0g4Cp1wBxg/C7rbYOvL5KXEMSIzQZOqIhKWQivci2ZAQtbhoZlpIzNYvrOe7p7eABcmInJihVa4R3i8QzNbXobOVmaMzKC5o5u1Fbr1noiEl9AKd/AOzXS1wPbXmKZ9ZkQkTIVeuBedBXHpsP45MhNjGJOTpPXuIhJ2Qi/cPZEw7jLY8hJ0tTN9ZAYryuvp6O4JdGUiIidM6IU7wPjPQWczbH+d6SMzaO/qZfUejbuLSPgIzXAvORtiU2HD80wrycATYby2sSrQVYmInDChGe6eKBh7GWx+kZToXj49Npt57+6ls1tLIkUkPIRmuIN31UxHA+x4g9lnFFLb3MkrG9R7F5HwELrhPuJciEmBDc9z9ugshqXG8fjy3YGuSkTkhAjdcI+MhrGXwKYX8PR2cd2U4SzeVsuuupZAVyYiMuhCN9zBOzTT3gDlb3L15OF4IozHl+8JdFUiIoMutMN9xHkQnQTrnyM3JZbzx2bz9Mo9mlgVkZAX2uEeFQtjZsKmF6Cni+t9E6uvalmkiIS40A538A7NtB2A8rc0sSoiYSP0w33UBRCbAmUP44kwrp0ynLe2amJVREJb6Id7VBxM/VfY+Heo3sQ1vonVJ1ZoYlVEQlfohzvAGV+DqHhY/OvDE6tzyzSxKiKhKzzCPSEDTv8XWDsXDpRz/VTvxKr2mxGRUBUe4Q4w4xbvnZre/i1nl2aRnxLLY5pYFZEQFT7hnpwPE6+H9/6Gp6WKa6cU8tbWWnbXtQa6MhERvwufcAc481vQ2w1L7uXaKcOJMHh0+a5AVyUi4ncDCnczm2lmm81sm5nd0c/3f21mq3yPLWZ20P+l+kH6CDjpKljxMLlRrVx8ch5/XbKL6qb2QFcmIuJXxw13M/MA9wEXA+OB2WY2vm8b59xtzrmJzrmJwO+AZwajWL846zbvDbSX/YHbLxxDZ3cvv3l1a6CrEhHxq4H03KcC25xzO5xzncATwKxjtJ8NPO6P4gZFzngYcyks+z0lSb3MmVbEkyv2sK26KdCViYj4zUDCfRjQ9xM/e33nPsTMioAS4PVPXtog+tR3of0glP2ZW88fRXyUh/9+cXOgqxIR8ZuBhLv1c84dpe11wNPOuZ5+X8jsZjMrM7OympqagdbofwWne2/mseReMmIcXzt3JK9urGLZjrrA1SQi4kcDCfe9wPA+xwVA5VHaXscxhmSccw865yY75yZnZWUNvMrB8KnvQnMVrPobN51VQl5KLD9dsBHnjvZ3S0QkeAwk3FcAo82sxMyi8Qb4/CMbmdkYIA1Y4t8SB0nxp2D4NFh0D7HdjXznM6Ws3tvAC2v2BboyEZFP7Ljh7pzrBm4BFgIbgaecc+vN7G4zu7xP09nAEy5Yur5mcOkvvdsBv3oXV55WwNjcJH6+cBMd3f2OKomIBI0BrXN3zi1wzpU650Y6537iO/dD59z8Pm3ucs59aA38kJZ7Mkz7Oqx8BM/e5dx5yTj21Lfxt6XalkBEglt4fUK1P+feCcnD4IXbOGdkKp8ancnvXt9KQ1tXoCsTEfnYFO4xiXDxz6F6PSx9gDsuHktDWxf3L9oW6MpERD42hTvA2Euh9GJ44x4mxDdw5aQCHn57J+srGwJdmYjIx6JwB+/k6iU/9z5/8Xv84NJxpMZH850nV2tyVUSCksL9kNRCOPcO2LyAtN0v87PPn8zmqiZ+/Yr2nRGR4KNw72vaNyB7Arz4H5xfksB1U4bz4JvbKSuvD3RlIiIficK9L08UXPZraKyART/lB5eNJz81ju/OXU1LR3egqxMRGTCF+5EKz/Deb3Xp/STufoNfXn0qu+tbuefFjYGuTERkwBTu/bnop5A9Hp75CtPSW7npzBL+tnQ3/9wSwM3OREQ+AoV7f6Lj4Zq/QE83zP0it3+6mNHZifzH06tpaNWHm0Rk6FO4H03mKPjc/VCxktjXf8ivrplIXXMnP5q/LtCViYgcl8L9WMZfDtNvgRV/5OT6l7n1/NE8t6qSR5fpptoiMrQp3I/ngrugcDr8/ZvcclIX543J4kfPr2fJdt3YQ0SGLoX78Xii4Ko/Q3QCnrk38tsrR1GcmcDXH13J7rrWQFcnItIvhftAJOfBVQ9D/XaSX/4OD33hdJyDr/xlBU3tmmAVkaFH4T5QJWfD+f8f1j9L8drfcP8Np7G9poXbnlxFT29w3J9ERMKHwv2jOOs2mDQH3vwFZx54nh99djyvbqzmly9vDnRlIiIfoHD/KMzgst/C6Itgwe18IXUt159RyANvbOfZ9/YGujoRkcMU7h+VJxKu/jPkT8LmfYW7JzVxRkk635u3lhXaYExEhgiF+8cRnQDXz4WUAiKfmM0fZiZSkBrHv/x5Bav3HAx0dSIiCvePLSED5syDyBhS513H47MLSUuI4saHl7NxX2OgqxORMKdw/yTSiuGGudDeQM7zN/D4nLHER3uY89AytlU3B7o6EQljCvdPKu9UuPavULeNgr/P5vE5YzAzbnhoKbvqWgJdnYiEKYW7P4w8D679G1RvoHjBDTz+hTF0dvdy/R+XUXGwLdDViUgYUrj7S+lFcO2jUL2B0S/dwGM3jKGxvYsb/riU6sb2QFcnImFG4e5PpRf6An4j416Zw1+vL6W6qYNr/rCEvQe0D42InDgKd38rvRCuewxqNjPx9Rt5bM4Y6ls6ueb3S9heo0lWETkxFO6DYfRn+gT8F5h741g6e3q55vdLWF/ZEOjqRCQMKNwHy+gLYPZjULOFMQuuZt71hcRERnDdg0tZuUufZBWRwaVwH0yjLvB+0KlpP0XPzuLZq9PITIxhzkPLWby1NtDViUgIU7gPtpJPwZdfBCBn7iyevaSXoox4vvzICl5atz/AxYlIqFK4nwg5E+CmVyApj9R51zLv7GomDEvm64+u5ME3t+Oc9oMXEf9SuJ8oqcPhyy9B/mkkzP8KT01czSUn5fHTBZu4Y95aOrt7A12hiISQAYW7mc00s81mts3M7jhKm2vMbIOZrTezx/xbZoiIT4cbn4OxlxL18p3cmzGPb55XwpNle7jx4WUcbO0MdIUiEiKOG+5m5gHuAy4GxgOzzWz8EW1GA3cCZzrnJgDfHoRaQ0NUHFzzF5jyVWzpvXyn5ofce2UJ7+46yBX3v8MOrYUXET8YSM99KrDNObfDOdcJPAHMOqLNV4H7nHMHAJxz1f4tM8REeODSX8Jlv4Ydi7hs2Y3MuyaTxrYurrj/Hd7ZppU0IvLJDCTchwF7+hzv9Z3rqxQoNbO3zWypmc30V4EhbfKX4cb50FbPyQs+z0uXtpOdFMMXHl7OI2/v1ESriHxsAwl36+fckakTCYwGzgVmAw+ZWeqHXsjsZjMrM7Oympqaj1praCo+E25+A1ILyZr/Bf5++krOK83irr9v4Pa5a2jv6gl0hSIShAYS7nuB4X2OC4DKfto875zrcs7tBDbjDfsPcM496Jyb7JybnJWV9XFrDj2phXDTQhg/i9hFP+aPSQ/y3XMLmPfuXq79wxL2NWjbYBH5aAYS7iuA0WZWYmbRwHXA/CPaPAecB2BmmXiHaXb4s9CQF50AVz8C5/0AWzuXW7d9lUcvT2J7TQuf/d1ilu/UlgUiMnDHDXfnXDdwC7AQ2Ag85Zxbb2Z3m9nlvmYLgToz2wAsAv7dOVc3WEWHLDM459+9yyXb6jnz9Wt47fw9JMdGcf0fl/LXJeUahxeRAbFAhcXkyZNdWVlZQN47KDTth3lfgfK36DzpOr7ZeAMvbWli5oRcfnrlyaQnRAe6QhEJADNb6ZybfLx2+oTqUJWUCzc+D+fcQfS6J3mg9XZ+cXYUr22qYuZv3uSfWzQhLSJHp3AfyiI8cN6dcONzWFs9V797I2+evYm0OA9ffHg5d81fr9U0ItIvhXswGHEufG0xlHyKvCU/5sXUn/OdyZE88k45l/1uMesqdAMQEfkghXuwSMqF65+CWfcTUbWeb27+Eq+dtYnmtg6uuP9tfvfaVrp6tPmYiHgp3IOJGUy6Ab6xBIrOZGTZ3byV+2uuH93L/7yyhVn3vq1evIgACvfglDIMbpgLs+4jqnodP674Ki9NXU19Uwuz7nubXyzcpLF4kTCncA9WZjBpDnxjKZSczdg1P+PttB/z7TH13LdoO5f9bjHv7j4Q6CpFJEAU7sEuZRjMfgKufRRPRyO37vw3lkx4lsj2A3z+gXe4a/56Gtu7Al2liJxgCvdQYAbjLoN/WwYzvknezmdZ4LmNX41ey1+W7ODT//NPnl9VoU+3ioQRhXsoiUmEC/8T/vUtIrLGcMXue1hb+FumJlTxrSdWMedPy9ium4GIhAWFeyjKGQ9fWgCz7iOhcTv3Nn6TF8a9xpa91cz8zZv8cuFm2jo14SoSyhTuoSoiwjvheksZdsq1nLTzTyxN+T7/PmI39y7axgW/8g7V9PZqqEYkFCncQ11CBnzufvjSP/BExXLznu+xcszfGBHbyLeeWMUVD7xDWbm2ExYJNQr3cFF8lncLg/N+QMbe1/hL89dYOOFVWg9Wc9Xvl/CNR1eyu6410FWKiJ9oy99wVL8T3rgH1jyFi05gWc5sbi2fQUNvHF+cUcQ3zh1FmrYUFhmSBrrlr8I9nFVvhEU/hY3z6Y1J5aWUa7h9zzQ80YncfPYIvnxWCQkxkYGuUkT6ULjLwFWu8ob81oV0x2UyN/5aflQxleTEBG49fzSzpxYSHakRPJGhQOEuH93uZfD6f3rv/pSQz58jr+bnVZPJS0vktgtKmTUxn0iPQl4kkBTu8vHteANe+0+oKKM1sYh73TU8UHcqw9O9wzVXnV5AbJQn0FWKhCWFu3wyzsGWl+D1/4KqdTSllPKH3it4oOYk0hLjuemsEm6YVkhybFSgKxUJKwp38Y/eXlj/DPzzZ1C7hbakQh6P/Bw/23ca0THxzJlexJfPLCErKSbQlYqEBYW7+FdvL2xeAIt/BRUr6YrL4oWEK/hhxVQ6PIlcM7mAfz17JMPT4wNdqUhIU7jL4HAOyt+Ct34FOxbRE53Mmymf5fuVZ1Ll0rj81Hy+fu5ISnOSAl2pSEhSuMvgq3wPFv8GNs7HRUSyOu1CflB9Hus687hgXA5fO2cEpxelYWaBrlQkZCjc5cSp3wFL7oP3HoXuNnakncV/HfwMr7eNYkJ+Cl+cUczlp+ZrhY2IHyjc5cRrqYMVf4TlD0JrHfXJ4/hr53k8ePB0YhJSmD11OHOmFZGXEhfoSkWClsJdAqezFVY/DmUPQ9U6eiLjeTvuPH5RN50NjOTC8TnMmVbEjJEZGrIR+YgU7hJ4zkHFSlj5Z1j3DHS1si9+DA+1ncOTbVPJzszi+jMKuer0AlLjtVGZyEAo3GVoaW+ANU/Bykegah3dnjj+GXUW9zWcyXrPGC47ZRjXn1HIaYWp6s2LHIPCXYYm56DiXXj3f2HdPOhspiqmmEfazuaJzhmkZ+Vx5WkFXHnaMI3Ni/RD4S5DX0eTd7jm3b9ARRk9FsnK6Cn8qekM3nCTmDoqj8+fVsBFE3KJi9ZKGxFQuEuw2b8OVj0Ga+dCSzXtkcm85Kbx19bpbI4ez0UT8vjcpHymj8jQzpQS1hTuEpx6ur27Uq55ArfxBay7jbqofJ7pnMoznWdQEz+az07M53MTh3FKQYrG5yXs+DXczWwm8FvAAzzknPvvI77/JeAXQIXv1L3OuYeO9ZoKdzmujibY+AKseRK3803M9bAvqpCn26fwXPc0ejNKueTkXC4cn6ugl7Dht3A3Mw+wBfgMsBdYAcx2zm3o0+ZLwGTn3C0DLVDhLh9JSy1seB7WP4srX4zh2B1Vwry2yfyjZwrNSaO4cEIOF47P5YwR6URp6EZC1EDDfSA3yJwKbHPO7fC98BPALGDDMX9KxJ8SMmHKTTDlJqxpP2x4nsJ1z/DtPU9zW+Rc9rlCnlt5OvcsncKemJF8elwuM0/K5ezRWZqMlbA0kJ77VcBM59xXfMdfAM7o20v39dzvAWrw9vJvc87t6ee1bgZuBigsLDx9165dfroMCVtN+2Hj372bl5UvxlwvdVF5/KPrdF7sPJV1nvGcNSaPmSflct7YbN1cRIKeP4dlrgYuOiLcpzrnbu3TJgNods51mNnXgGucc+cf63U1LCN+11ILm/7hDfqdb2I9nbRHJLDYncKCjlN52yYxZuQIzinN4pzSTEZmJWqcXoKOP8N9OnCXc+4i3/GdAM65e47S3gPUO+dSjvW6CncZVB3N3lU3WxfitizEmqtwGBsjSlnYcRL/7D2V2qRxnDUml3NKs5gxKpOUOPXqZejzZ7hH4h1q+TTe1TArgOudc+v7tMlzzu3zPb8C+J5zbtqxXlfhLidMby/sXw1bFsLWl3EV72I4miOSebPnZF7tOpnFnEpRYTHnlGZx7phsxuclExGhXr0MPf5eCnkJ8Bu8SyEfds79xMzuBsqcc/PN7B7gcqAbqAe+7pzbdKzXVLhLwLTUwY5FsO1V3LZXsZYaAHZ6Sni5YzyLe09mR9wpTBtTwLljsjhzVCbpCdrYTIYGfYhJZCB6e6FqLWx7FbYvwu1eivV20WVRrHRjeaNrAu/0TqAn5xSmj8pmxqgMppZkkBgzkIVmIv6ncBf5ODpbYNc73qDfsQir9q74bbV4VvSU8k7POMoYj2fYRE4fkc2U4jROK0zTlsVywijcRfyhaT/sehvKF9NbvpiI2i0AtFkcK3tGsaKnlDJXSmPGRE4qyef0onSmFKdRmB6vlTgyKBTuIoOhuRrKF8Out+ndtQSr3oDh6CGCzRSxrLuUst4x7Iw/ieKSUUwuSmdKcTrj8pK04Zn4hcJd5ERoOwh7y2DPUtyuJbiKMiK62wGotGyW+sJ+nWcsycNPYlJRBqcVpjGpMFVDOfKxKNxFAqGnC/avgd1LYfdSenYtwdPqXY3TbAms7ilhde8IVveO4GDaSRQUjua04jROLUilNCeJ6Ej17uXYFO4iQ4FzcGCnN+z3LKe34l2oWk+E6wagllRW9ZSwwRWxlWK6ssaTVTiWkwvSObkghVHZidoETT5A4S4yVHW1Q9U6qHgXV7mSrj3vEnlgOxGuB4BWF8MmN5yNvUVstiKaUsYRlT+BkvxcxuYmMSY3ibyUWE3YhimFu0gw6WqDmk2wfy1u/1ra964hsno9Ud1NAPRilPfmsNEVsrG3iN1RI+jNGkdmwWjG5iUzNi+Z0pxE4qO1/j7U+XPLXxEZbFFxkD8J8idhQBx4h3Qa9sD+dUTsX8vwyjXk71vLpU3LvT9TA03VcWx2w1nXO5x5rpCGpFF4csYyLH84pblJlOYkMiIzUWP5YUg9d5Fg097o7eVXrcNVbaCjYi2emg1EdTUeblLvEtnmhrGtN58dFNCSPILonDHkDB/FmLxUxuYlk6+hnaCkYRmRcOIcNFZCzUao2UJP9SY69m3AU7+VmM6Dh5t1uCh2ulx2uDz2eobRkTyC3ozRxOSOJS8nm6KMBIoz4rVMcwjTsIxIODGDlGHex6gL8ADxh77XUgu1W6B2K1a1mdx9m8ir305iy0o8jT3QCOyE/S6N7b35PO/yqYgcTkdSEZYxgsTsERRmpVCYEU9RRjw5SbHaMTMIKNxFQl1CpvdRNINo4HCfvKcLDpRD7Ra6qjYTV7GBCbVbmNKwhOieV6AJaIKenUaly2SXy+Z1l8v+iGy6EvKJTCsgPruIjNxiirJTKc5IIDspRsE/RCjcRcKVJwoyR0PmaKLGXsrhu+s4B81VUL8D6ndidTtIr95OSt0OJjeuJLbrILThfVRCrzNqSKHCZbKCbBpi8+lILMTSiojNGkFqbgk5aQnkJMeSnRSryd0TROEuIh9kBkm53kfRDCKAhL7f72j2ju837KHn4F6aq8rprdtN/sFdjGjeSVLnUjz1vd47O2yHbhfBftKpdBmUuQwORObQGpdHV+IwItMLScguIjszm4K0OIalxZGREK2JXj9QuIvIRxOTCFmlkFWKB0jxPQ7r6YLGCjiwi9bqHbRUbSfywB6KG/cypqWcxI5leFp7oBWoBjZBo4un0mWwxmVQZZm0xGTTHptNd0IOlpRLZEo+8alZ5KTEk5cSS35qHFmJGgI6FoW7iPiXJwrSiiGtmPgR57w/sXtIb493d82GPdCwl/a6XXTX7CK9fjdZTRXEtS4nvqsRuvCO++/3/lin81BDKjUuhTUulTpLpS06g574bEjMITI5h9i0fBIz8klPSyU7KZbs5BiSYiLD8v8EFO4icmJFeCA5z/sYPpVYIPbINl3t0LwfmqqgaR+9jfvoOVBBwsF9xDZWUdhSTXR7OfFdB4hodN4VP5Xv/3iTi6PGpbCRVA5YCi1R6XTGZNATl4klZhOVnENkUhYxyVlsCcLGAAAFk0lEQVTEJaeRHBdDSlwUyXFRpMRFERvlOXH/PAaJwl1Ehp6o2MO9f4AIvJ/ajTuyXU83tNZB835cczXtB/bRUl9J58F9RDdVMbylmpHt+4nr3EBCSxO0ALUffIluF8FBEjngkigniXqXRIOl0ByZSnt0Ol0x6XTHZRKRmElcSjaJadlkpSSSnRxDdlIsWUkxQ/KPgcJdRIKXJxKSciAp5/C2DR/6A3BIdye01kJzNd1NVbQdrKajqZbuplp6W+pIaK0jqa2e0R11xHRuI677IBHtDtqBhg++VKOLp94lUUMSW1wSzZZAe1QKXVHJ9Mak4GLTsPg0IuLTiUzMIDopk9jkdFLiY0mJi2J4Wjwp8VGD+o9G4S4i4SEyGpLzITmfSCDJ9ziq3h5orff+QWipgdY6elvqaDtYRVdDNXHNteS31DG8vZ6ozv3EdDcR197c7x8D8C4ZbSCBAy6RNZNu41NXfG1QLvMQhbuISH8iPJCY5X0wznsK77LQhKP9TE83dDRC2wFoO4hrraOjqZb2xlq6mmrpaa4jrrWe0uLiQS9f4S4i4i+eSIhP9z4Ag/4njE8AfVRMRCQEKdxFREKQwl1EJAQp3EVEQpDCXUQkBCncRURCkMJdRCQEKdxFREJQwG6QbWY1wK6P+eOZfGj7n7AQrtcN4Xvtuu7wMpDrLnLOZR3vhQIW7p+EmZUN5O7foSZcrxvC99p13eHFn9etYRkRkRCkcBcRCUHBGu4PBrqAAAnX64bwvXZdd3jx23UH5Zi7iIgcW7D23EVE5BiCLtzNbKaZbTazbWZ2R6DrGSxm9rCZVZvZuj7n0s3sFTPb6vuaFsgaB4OZDTezRWa20czWm9m3fOdD+trNLNbMlpvZat91/9h3vsTMlvmu+0kziw50rYPBzDxm9p6ZveA7DvnrNrNyM1trZqvMrMx3zm+/50EV7mbmAe4DLgbGA7PNbHxgqxo0jwAzjzh3B/Cac2408JrvONR0A991zo0DpgH/5vt3HOrX3gGc75w7FZgIzDSzacDPgF/7rvsAcFMAaxxM3wI29jkOl+s+zzk3sc/yR7/9ngdVuANTgW3OuR3OuU7gCWBWgGsaFM65N4H6I07PAv7X9/x/gc+d0KJOAOfcPufcu77nTXj/gx9GiF+782r2HUb5Hg44H3jadz7krhvAzAqAS4GHfMdGGFz3Ufjt9zzYwn0YsKfP8V7fuXCR45zbB94QBLIDXM+gMrNiYBKwjDC4dt/QxCqgGngF2A4cdM51+5qE6u/7b4D/AHp9xxmEx3U74GUzW2lmN/vO+e33PNjuoWr9nNNynxBkZonAPODbzrlGb2cutDnneoCJZpYKPMuhuzIf0ezEVjW4zOwyoNo5t9LMzj10up+mIXXdPmc65yrNLBt4xcw2+fPFg63nvhcY3ue4AKgMUC2BUGVmeQC+r9UBrmdQmFkU3mB/1Dn3jO90WFw7gHPuIPAG3jmHVDM71AkLxd/3M4HLzawc7zDr+Xh78qF+3TjnKn1fq/H+MZ+KH3/Pgy3cVwCjfTPp0cB1wPwA13QizQe+6Hv+ReD5ANYyKHzjrX8CNjrnftXnWyF97WaW5euxY2ZxwAV45xsWAVf5moXcdTvn7nTOFTjnivH+9/y6c+4GQvy6zSzBzJIOPQcuBNbhx9/zoPsQk5ldgvcvuwd42Dn3kwCXNCjM7HHgXLy7xFUBPwKeA54CCoHdwNXOuSMnXYOamZ0FvAWs5f0x2P+Hd9w9ZK/dzE7BO4Hmwdvpeso5d7eZjcDbo00H3gPmOOc6Alfp4PENy9zunLss1K/bd33P+g4jgceccz8xswz89HsedOEuIiLHF2zDMiIiMgAKdxGREKRwFxEJQQp3EZEQpHAXEQlBCncRkRCkcBcRCUEKdxGREPR/+RrxnI3O36IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check predictions from samples\n",
    "\n",
    "for x, y in train_loader:\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    output = model(x)  # predict output from input\n",
    "    \n",
    "    scaled_y = label_scaler.inverse_transform(y.detach().numpy())\n",
    "    scaled_output = label_scaler.inverse_transform(output.detach().numpy())\n",
    "\n",
    "    print(np.concatenate((scaled_y, np.round(scaled_output)), axis=1))\n",
    "    break\n",
    "\n",
    "\n",
    "plt.axis('on')\n",
    "x = range(len(train_losses))\n",
    "plt.plot(x, train_losses, x, val_losses)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[69. 71. 69. 69. 69. 67. 72. 74. 76. 74. 73. 73. 72. 72. 72. 72. 72. 71.\n",
      " 71. 71. 70. 70. 70. 70. 70. 70. 70. 70. 70. 69. 69. 69. 69. 69. 69. 69.\n",
      " 69. 69. 69. 69.]\n"
     ]
    }
   ],
   "source": [
    "# predict midi from init samples\n",
    "\n",
    "init_index = 1000\n",
    "\n",
    "generated_track = train_dataset[init_index][0].clone()\n",
    "\n",
    "# predict for a certain length\n",
    "predict_length = 30  # TODO: Model the end of the songs as well through a terminator.\n",
    "\n",
    "x, y = train_dataset[init_index]\n",
    "x = x.to(device)\n",
    "for i in range(predict_length):\n",
    "    _, y = train_dataset[init_index + i]\n",
    "    y = y.to(device)\n",
    "    output = model(x)  # predict output from input\n",
    "    x = torch.cat((x[1:], output))  # shift the input by one by adding the prediction\n",
    "    generated_track = torch.cat((generated_track, output))  # append prediction to generated track\n",
    "    \n",
    "    \n",
    "track = label_scaler.inverse_transform(generated_track.detach().numpy()).round()\n",
    "\n",
    "print(track)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write numpy to midi track\n",
    "\n",
    "numpy_notes = midi_utils.prediction_to_numpy(track, 1024)\n",
    "\n",
    "# create midi track\n",
    "new_track = midi_utils.numpy_to_midi_track(numpy_notes, 1, 'Modified')\n",
    "\n",
    "#print(numpy_notes)\n",
    "\n",
    "os.chdir(home_dir)\n",
    "\n",
    "# make new song with the new track\n",
    "new_track_dict = {}\n",
    "new_track_dict['0'] = track_dict['0']\n",
    "new_track_dict['1'] = new_track\n",
    "modified_midi_filename = 'midi_data/test_modified_track.mid'\n",
    "modified_csv_list = midi_utils.track_dict_to_csv(new_track_dict)\n",
    "midi_utils.write_to_midi(modified_csv_list, modified_midi_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.init()\n",
    "pygame.mixer.music.load(modified_midi_filename)\n",
    "pygame.mixer.music.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.mixer.music.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
