{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5568,
     "status": "ok",
     "timestamp": 1557343235863,
     "user": {
      "displayName": "Nicolas Deperrois",
      "photoUrl": "https://lh5.googleusercontent.com/-Jfkdb6w84hk/AAAAAAAAAAI/AAAAAAAACRA/bqUAiv0shAM/s64/photo.jpg",
      "userId": "11633758183366338150"
     },
     "user_tz": -120
    },
    "id": "kfpjADwL4oUx",
    "outputId": "631f1476-4f3d-42d3-f9cd-0bc2c0088060"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'music_generation'...\n",
      "remote: Enumerating objects: 482, done.\u001b[K\n",
      "remote: Counting objects: 100% (482/482), done.\u001b[K\n",
      "remote: Compressing objects: 100% (445/445), done.\u001b[K\n",
      "remote: Total 482 (delta 55), reused 454 (delta 30), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (482/482), 390.83 KiB | 530.00 KiB/s, done.\n",
      "Resolving deltas: 100% (55/55), done.\n"
     ]
    }
   ],
   "source": [
    "# Uncomment the line below and run this cell to get your data from github into colab (only runnable in colab, not ordinary jupyter notebook):\n",
    "#! git clone https://github.com/lkriener/music_generation.git && mv music_generation/* . && rm music_generation -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14036,
     "status": "ok",
     "timestamp": 1557343244715,
     "user": {
      "displayName": "Nicolas Deperrois",
      "photoUrl": "https://lh5.googleusercontent.com/-Jfkdb6w84hk/AAAAAAAAAAI/AAAAAAAACRA/bqUAiv0shAM/s64/photo.jpg",
      "userId": "11633758183366338150"
     },
     "user_tz": -120
    },
    "id": "sLhyMMRj4tNz",
    "outputId": "77c34eea-a869-49d2-bba8-35b15bc4567e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pygame==1.9.6 (from -r colab_requirements.txt (line 2))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/24/ede6428359f913ed9cd1643dd5533aefeb5a2699cc95bea089de50ead586/pygame-1.9.6-cp36-cp36m-manylinux1_x86_64.whl (11.4MB)\n",
      "\u001b[K     |████████████████████████████████| 11.4MB 24.7MB/s \n",
      "\u001b[?25hCollecting py_midicsv==1.9.0 (from -r colab_requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/1f/eb/3133f65bd34dafcbae37508d290ebf540832430cbe2aef23629cc6a6197f/py_midicsv-1.9.0-py3-none-any.whl\n",
      "Collecting pypianoroll==0.5.0 (from -r colab_requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/aa/33/fa38c07909e425add987146cb0f8d5ad80262f6a72cc820bf7e5f690d527/pypianoroll-0.5.0.tar.gz\n",
      "Requirement already satisfied: six<2.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pypianoroll==0.5.0->-r colab_requirements.txt (line 4)) (1.12.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pypianoroll==0.5.0->-r colab_requirements.txt (line 4)) (1.16.3)\n",
      "Requirement already satisfied: scipy<2.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pypianoroll==0.5.0->-r colab_requirements.txt (line 4)) (1.2.1)\n",
      "Requirement already satisfied: pretty_midi<1.0,>=0.2.8 in /usr/local/lib/python3.6/dist-packages (from pypianoroll==0.5.0->-r colab_requirements.txt (line 4)) (0.2.8)\n",
      "Requirement already satisfied: mido>=1.1.16 in /usr/local/lib/python3.6/dist-packages (from pretty_midi<1.0,>=0.2.8->pypianoroll==0.5.0->-r colab_requirements.txt (line 4)) (1.2.6)\n",
      "Building wheels for collected packages: pypianoroll\n",
      "  Building wheel for pypianoroll (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Stored in directory: /root/.cache/pip/wheels/ed/f6/fb/5d070524ecf7ba9ed201247a293c01945cfd7f840f8ef338c0\n",
      "Successfully built pypianoroll\n",
      "Installing collected packages: pygame, py-midicsv, pypianoroll\n",
      "Successfully installed py-midicsv-1.9.0 pygame-1.9.6 pypianoroll-0.5.0\n"
     ]
    }
   ],
   "source": [
    "# Uncomment line to install requirements\n",
    "#! pip install -r colab_requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 18985,
     "status": "ok",
     "timestamp": 1557343251781,
     "user": {
      "displayName": "Nicolas Deperrois",
      "photoUrl": "https://lh5.googleusercontent.com/-Jfkdb6w84hk/AAAAAAAAAAI/AAAAAAAACRA/bqUAiv0shAM/s64/photo.jpg",
      "userId": "11633758183366338150"
     },
     "user_tz": -120
    },
    "id": "0DaVKNWw4lLr",
    "outputId": "d6f14186-4b55-410c-b37b-d367ec561bf4"
   },
   "outputs": [],
   "source": [
    "import src.midi_utils as midi_utils\n",
    "import pygame\n",
    "from pypianoroll import Multitrack, Track\n",
    "import pypianoroll\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test of pypianoroll library\n",
    "We will first use the pypianoroll library - https://salu133445.github.io/pypianoroll/ - to convert a midi file into a multitrack object, get the soprano track, transpose it to C major, create a new multitrack object out of it, and finally write it to a new midi file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 871,
     "status": "ok",
     "timestamp": 1557344147267,
     "user": {
      "displayName": "Nicolas Deperrois",
      "photoUrl": "https://lh5.googleusercontent.com/-Jfkdb6w84hk/AAAAAAAAAAI/AAAAAAAACRA/bqUAiv0shAM/s64/photo.jpg",
      "userId": "11633758183366338150"
     },
     "user_tz": -120
    },
    "id": "IugBxMoP4lLy",
    "outputId": "5efbf031-b629-47ca-c242-2092cbf093d6"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    home_dir\n",
    "except NameError:\n",
    "    home_dir = os.getcwd()\n",
    "\n",
    "os.chdir(home_dir + \"/data/raw/bach\")  # go to a folder relative to home dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 871,
     "status": "ok",
     "timestamp": 1557344147267,
     "user": {
      "displayName": "Nicolas Deperrois",
      "photoUrl": "https://lh5.googleusercontent.com/-Jfkdb6w84hk/AAAAAAAAAAI/AAAAAAAACRA/bqUAiv0shAM/s64/photo.jpg",
      "userId": "11633758183366338150"
     },
     "user_tz": -120
    },
    "id": "IugBxMoP4lLy",
    "outputId": "5efbf031-b629-47ca-c242-2092cbf093d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semitones to C major : 2\n",
      "Soprano pianoroll matrix: \n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Soprano flattened pianoroll :\n",
      "[ 0  0  0  0 71 71 73 73 74 74 74 74 73 73 73 73 71 71 71  0 71 71 73 73\n",
      " 74 74 74 74 73 73 73 73 71 71 71 71 78 78 78 78 76 76 76 76 74 74 74 74\n",
      " 73 73 73  0 73 73 73 73 74 74 74  0 74 74 74 74 76 76 76  0 76 76 76 76\n",
      " 78 78 78  0 78 78 78 78 76 76 76 76 74 74 74 74 73 73 73 73 73 73 73 73\n",
      " 71 71 71 71 78 78 78 78 76 76 76 76 74 74 74 74 73 73 73 73 78 78 78 78\n",
      " 76 76 76 76 74 74 74 74 73 73 73  0 73 73 73 73 74 74 74  0 74 74 74 74\n",
      " 76 76 76  0 76 76 76 76 78 78 78  0 78 78 78 78 76 76 76 76 74 74 74 74\n",
      " 73 73 73 73 73 73 73 73 71 71 71 71]\n",
      "Transposed soprano transposed pianoroll :\n",
      "[ 0  0  0  0 69 69 71 71 72 72 72 72 71 71 71 71 69 69 69  0 69 69 71 71\n",
      " 72 72 72 72 71 71 71 71 69 69 69 69 76 76 76 76 74 74 74 74 72 72 72 72\n",
      " 71 71 71  0 71 71 71 71 72 72 72  0 72 72 72 72 74 74 74  0 74 74 74 74\n",
      " 76 76 76  0 76 76 76 76 74 74 74 74 72 72 72 72 71 71 71 71 71 71 71 71\n",
      " 69 69 69 69 76 76 76 76 74 74 74 74 72 72 72 72 71 71 71 71 76 76 76 76\n",
      " 74 74 74 74 72 72 72 72 71 71 71  0 71 71 71 71 72 72 72  0 72 72 72 72\n",
      " 74 74 74  0 74 74 74 74 76 76 76  0 76 76 76 76 74 74 74 74 72 72 72 72\n",
      " 71 71 71 71 71 71 71 71 69 69 69 69]\n"
     ]
    }
   ],
   "source": [
    "midi_filename = 'bwv110.7.mid'\n",
    "csv_text = midi_utils.load_to_csv(midi_filename)\n",
    "# get semitones to C major \n",
    "semitones,_ = midi_utils.get_semitones_to_C(csv_text)\n",
    "print('Semitones to C major : ' +str(semitones))\n",
    "# get multitrack object from midi \n",
    "multitrack = pypianoroll.parse(midi_filename, beat_resolution=4)\n",
    "# get the soprano track\n",
    "track_soprano = multitrack.tracks[0]\n",
    "print(\"Soprano pianoroll matrix: \")\n",
    "print(track_soprano.pianoroll)\n",
    "# transpose the track to C Major \n",
    "transposed_track_soprano = pypianoroll.transpose(track_soprano,-semitones)\n",
    "\n",
    "print(\"Soprano flattened pianoroll :\")\n",
    "print(midi_utils.flatten_one_hot_pianoroll(track_soprano.pianoroll))\n",
    "print(\"Transposed soprano transposed pianoroll :\")\n",
    "print(midi_utils.flatten_one_hot_pianoroll(transposed_track_soprano.pianoroll))\n",
    "\n",
    "# create a multitrack from the modified track (can be done with a modified pianoroll too)\n",
    "new_multitrack = Multitrack(tracks=[transposed_track_soprano], beat_resolution=4)\n",
    "#write to midi file \n",
    "pypianoroll.write(new_multitrack, home_dir + \"/data/tests_midi/new_multitrack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BrAiT6ot4lL2"
   },
   "outputs": [],
   "source": [
    "from src.dataset_utils import TrackDataset, get_dataset_representation_from_tracks\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "feature_scaler = StandardScaler()\n",
    "label_scaler = StandardScaler()\n",
    "\n",
    "# iterate over all midi files of folder\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    home_dir\n",
    "except NameError:\n",
    "    home_dir = os.getcwd()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zVNrDFfI4lL4"
   },
   "outputs": [],
   "source": [
    "# determine boundaries in all tracks\n",
    "lower_notes = []\n",
    "upper_notes = []\n",
    "list_pianorolls = []\n",
    "os.chdir(home_dir + \"/data/raw/bach\")  # go to a folder relative to home dir\n",
    "for midi_file in glob.glob(\"*.mid\"):\n",
    "    ## load midi file\n",
    "    csv_text = midi_utils.load_to_csv(midi_file)\n",
    "    # get semitones to C major \n",
    "    semitones,_ = midi_utils.get_semitones_to_C(csv_text)\n",
    "    # convert to multitrack object\n",
    "    multitrack = pypianoroll.parse(midi_file, beat_resolution=2)\n",
    "    # get the soprano track object and transpose it to C major \n",
    "    track = pypianoroll.transpose(multitrack.tracks[0], -semitones)\n",
    "    \n",
    "    # get pitch range \n",
    "    pitch_range = track.get_active_pitch_range()\n",
    "    lower_notes.append(pitch_range[0])\n",
    "    upper_notes.append(pitch_range[1])\n",
    "    \n",
    "    # get the flattened representation of pianoroll\n",
    "    pianoroll_flattened = midi_utils.flatten_one_hot_pianoroll(track.pianoroll)\n",
    "    # add it to the global list of all tracks\n",
    "    list_pianorolls.append(pianoroll_flattened)\n",
    "\n",
    "# convert into an array \n",
    "all_pianorolls = np.concatenate(list_pianorolls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6079,
     "status": "ok",
     "timestamp": 1557344162436,
     "user": {
      "displayName": "Nicolas Deperrois",
      "photoUrl": "https://lh5.googleusercontent.com/-Jfkdb6w84hk/AAAAAAAAAAI/AAAAAAAACRA/bqUAiv0shAM/s64/photo.jpg",
      "userId": "11633758183366338150"
     },
     "user_tz": -120
    },
    "id": "8EnWFvKA4lL7",
    "outputId": "d5cfb3ca-203d-4b56-e654-9436e0bce814"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global lower note : 57\n",
      "Global upper note : 83\n",
      "Number of notes : 28\n"
     ]
    }
   ],
   "source": [
    "# get lower and upper bounds \n",
    "global_lower = min(lower_notes)\n",
    "global_upper = max(upper_notes)\n",
    "global_diff = global_upper - global_lower\n",
    "n_notes = global_diff + 2 # we include the silence \n",
    "print('Global lower note : '+ str(global_lower))\n",
    "print('Global upper note : '+ str(global_upper))\n",
    "print('Number of notes : '+ str(n_notes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5547,
     "status": "ok",
     "timestamp": 1557344162436,
     "user": {
      "displayName": "Nicolas Deperrois",
      "photoUrl": "https://lh5.googleusercontent.com/-Jfkdb6w84hk/AAAAAAAAAAI/AAAAAAAACRA/bqUAiv0shAM/s64/photo.jpg",
      "userId": "11633758183366338150"
     },
     "user_tz": -120
    },
    "id": "P3Tts_gs4lL9",
    "outputId": "878ea69a-8e58-4858-cfe8-e43c222ce045"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0 70  0 70 70 68 68 66 66 65 65 65 65 63 63 63 63 70  0 70 70 72 72\n",
      " 74 74 75 75 75 75 74 74 74 74 75 75 78 78 77 77 77  0 75 75 75 75 75 75\n",
      " 75 75 70  0 70 70 68 68 66 66 65 65 65 65 63 63 63 63 70  0 70 70 72 72\n",
      " 74 74 75 75 75 75 74 74 74 74 75 75 78 78 77 77 77  0 75 75 75 75 75 75\n",
      " 75 75 70  0]\n"
     ]
    }
   ],
   "source": [
    "print(all_pianorolls[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5062,
     "status": "ok",
     "timestamp": 1557344162437,
     "user": {
      "displayName": "Nicolas Deperrois",
      "photoUrl": "https://lh5.googleusercontent.com/-Jfkdb6w84hk/AAAAAAAAAAI/AAAAAAAACRA/bqUAiv0shAM/s64/photo.jpg",
      "userId": "11633758183366338150"
     },
     "user_tz": -120
    },
    "id": "ayfyARad4lL_",
    "outputId": "5b4b3d39-17ba-445c-9406-d8c4e9f2b86e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0 14  0 14 14 12 12 10 10  9  9  9  9  7  7  7  7 14  0 14 14 16 16\n",
      " 18 18 19 19 19 19 18 18 18 18 19 19 22 22 21 21 21  0 19 19 19 19 19 19\n",
      " 19 19 14  0 14 14 12 12 10 10  9  9  9  9  7  7  7  7 14  0 14 14 16 16\n",
      " 18 18 19 19 19 19 18 18 18 18 19 19 22 22 21 21 21  0 19 19 19 19 19 19\n",
      " 19 19 14  0]\n"
     ]
    }
   ],
   "source": [
    "# scale pianoroll to 0 \n",
    "all_pianorolls_scaled = midi_utils.scale_pianoroll(all_pianorolls, global_lower)\n",
    "print(all_pianorolls_scaled[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mNOwMHKR4lMA"
   },
   "source": [
    "## Define a recurrent-network model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3877,
     "status": "ok",
     "timestamp": 1557344162438,
     "user": {
      "displayName": "Nicolas Deperrois",
      "photoUrl": "https://lh5.googleusercontent.com/-Jfkdb6w84hk/AAAAAAAAAAI/AAAAAAAACRA/bqUAiv0shAM/s64/photo.jpg",
      "userId": "11633758183366338150"
     },
     "user_tz": -120
    },
    "id": "b37NFyC-4lMB",
    "outputId": "537e85fb-f5e0-4f05-f4fb-efc37dc41085"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, training on CPU; consider making n_epochs very small.\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mrVryIjJ4lMC"
   },
   "source": [
    "## Define a LSTM network as for note-to-note melody generation\n",
    "We now build a network to implement note-to-note melody generation using LSTMs units as well as drop-out of the output. This network is highly inspired from the Tutorial 08 of the class, by replacing characters by notes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IPccoLL84lMD"
   },
   "outputs": [],
   "source": [
    "# Declaring the model\n",
    "class NoteRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_notes, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.n_notes = n_notes \n",
    "        #define the LSTM\n",
    "        self.lstm = nn.LSTM(self.n_notes, n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        #define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        #define the final, fully-connected output layer\n",
    "        self.fc = nn.Linear(n_hidden, self.n_notes)\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "                \n",
    "        #get the outputs and the new hidden state from the lstm\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        #pass through a dropout layer\n",
    "        out = self.dropout(r_output)\n",
    "        \n",
    "        # Stack up LSTM outputs using view\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        \n",
    "        #put x through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GxUeYhz24lME"
   },
   "source": [
    "## Training Code\n",
    "\n",
    "Time for training! We declare a function, where we define an optimizer (Adam) and loss (cross entropy). We then create the training and validation data and initialize the hidden state of the RNN. \n",
    "We loop over the training set, each time encoding the data into one-hot vectors, performing forward and backpropagation, and updating the network parameters.\n",
    "\n",
    "Every once a while, we generate some loss statistics (training loss and validation loss) to let us know if the model is training correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ySgtX9jT4lME"
   },
   "outputs": [],
   "source": [
    "# Declaring the train method\n",
    "def train(net, data, data2=None, mode=\"melody_generation\", epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: NoteRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "        seq_length: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    if mode == \"harmonization\":\n",
    "        data2, val_data2 = data2[:val_idx], data2[val_idx:]\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_notes = net.n_notes \n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        if mode == \"melody_generation\":\n",
    "            batch_generator = midi_utils.get_pianoroll_batches(data, batch_size, seq_length)\n",
    "        elif mode == \"harmonization\":\n",
    "            batch_generator = midi_utils.get_pianoroll_batches_harmonization(data, data2, batch_size, seq_length)\n",
    "            \n",
    "        for x, y in batch_generator:\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = midi_utils.one_hot_encode_batch(x, n_notes)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, targets.contiguous().view(batch_size*seq_length).long())\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                \n",
    "                if mode == \"melody_generation\":\n",
    "                    batch_generator_val = midi_utils.get_pianoroll_batches(val_data, batch_size, seq_length)\n",
    "                elif mode == \"harmonization\":\n",
    "                    batch_generator_val = midi_utils.get_pianoroll_batches_harmonization(val_data, val_data2, batch_size, seq_length)\n",
    "                for x, y in batch_generator_val:\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = midi_utils.one_hot_encode_batch(x, n_notes)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.contiguous().view(batch_size*seq_length).long())\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                net.train() # reset to train mode after iterationg through validation data\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6841,
     "status": "ok",
     "timestamp": 1557344202867,
     "user": {
      "displayName": "Nicolas Deperrois",
      "photoUrl": "https://lh5.googleusercontent.com/-Jfkdb6w84hk/AAAAAAAAAAI/AAAAAAAACRA/bqUAiv0shAM/s64/photo.jpg",
      "userId": "11633758183366338150"
     },
     "user_tz": -120
    },
    "id": "_Gr3LZSO4lMF",
    "outputId": "70735989-f441-42e9-a47a-0b938611056e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NoteRNN(\n",
      "  (lstm): LSTM(28, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (fc): Linear(in_features=256, out_features=28, bias=True)\n",
      ")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-048ac15a5b2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_pianorolls_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-65-3afb9128c846>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, data, data2, mode, epochs, batch_size, seq_length, lr, clip, val_frac, print_every)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;31m# calculate the loss and perform backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0;31m# `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/BERN/ATML/myenv/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/BERN/ATML/myenv/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define and print the net\n",
    "n_hidden=256\n",
    "n_layers=2\n",
    "\n",
    "net = NoteRNN(n_notes, n_hidden, n_layers)\n",
    "print(net)\n",
    "\n",
    "# Declaring the hyperparameters\n",
    "batch_size = 16\n",
    "seq_length = 100\n",
    "n_epochs = 20 # start smaller if you are just testing initial behavior\n",
    "\n",
    "# train the model\n",
    "train(net, all_pianorolls_scaled, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nAAp3PAX4lMH"
   },
   "outputs": [],
   "source": [
    "\"\"# Defining a method to generate the next character\n",
    "def predict(net, note, h=None):\n",
    "        ''' Given a note, predict the next note.\n",
    "            Returns the predicted note and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[note]])\n",
    "        x = midi_utils.one_hot_encode_batch(x, net.n_notes)\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "        \n",
    "        note_range = np.arange(net.n_notes)\n",
    "        # select the likely next note with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        note = np.random.choice(note_range, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return note, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UWOVgrS64lMI"
   },
   "outputs": [],
   "source": [
    "# Declaring a method to generate new melody\n",
    "def sample(net, size, prime=[10,10,12,12]):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    notes = [no for no in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for no in prime:\n",
    "        note, h = predict(net, no, h)\n",
    "    notes.append(note)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        note, h = predict(net, notes[-1], h)\n",
    "        notes.append(note)\n",
    "\n",
    "    return np.array(notes)\n",
    "\n",
    "\n",
    "# Declaring a method to generate new melody\n",
    "def sample_harmonization(net, seq):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    size = len(seq)\n",
    "    notes = []\n",
    "    h = net.init_hidden(1)\n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        note, h = predict(net, seq[ii], h)\n",
    "        notes.append(note)\n",
    "\n",
    "    return np.array(notes)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1011,
     "status": "ok",
     "timestamp": 1557344248338,
     "user": {
      "displayName": "Nicolas Deperrois",
      "photoUrl": "https://lh5.googleusercontent.com/-Jfkdb6w84hk/AAAAAAAAAAI/AAAAAAAACRA/bqUAiv0shAM/s64/photo.jpg",
      "userId": "11633758183366338150"
     },
     "user_tz": -120
    },
    "id": "dDlgi2OM4lMJ",
    "outputId": "c7c217c2-6ca2-4174-ac26-938e7eb85b03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sample :\n",
      "[ 1  1  1  1  3  6  9 13 12 15 26 20 18  8 21 22 15 17 14 13 21 11 24  0\n",
      " 19 16 12 14 10 19 11 23 16 15  8  6 13  8 10 17 18 18 10 22 16 16 14 15\n",
      " 10  0 15  8  9 17 14 15 15  9 21 16 18 15 14 13  9  0  0 21 14 10 17 13\n",
      " 14 14 15 10 17 17 13  9 12 10 15  8 12 22 13  7  8 14 13 15 13 17 21 12\n",
      "  4  0 11 14 13 14 10 18 15]\n",
      "Generated sample in the pitch range :\n",
      "[51 51 51 51 53 56 59 63 62 65 76 70 68 58 71 72 65 67 64 63 71 61 74  0\n",
      " 69 66 62 64 60 69 61 73 66 65 58 56 63 58 60 67 68 68 60 72 66 66 64 65\n",
      " 60  0 65 58 59 67 64 65 65 59 71 66 68 65 64 63 59  0  0 71 64 60 67 63\n",
      " 64 64 65 60 67 67 63 59 62 60 65 58 62 72 63 57 58 64 63 65 63 67 71 62\n",
      " 54  0 61 64 63 64 60 68 65]\n"
     ]
    }
   ],
   "source": [
    "# Generating new melody\n",
    "notes = sample(net, 100, prime=[1,1,1,1])\n",
    "print(\"Generated sample :\")\n",
    "print(notes)\n",
    "normal_pianoroll = midi_utils.unscale_pianoroll(notes, global_lower) # go back to the track range\n",
    "print(\"Generated sample in the pitch range :\")\n",
    "print(normal_pianoroll)\n",
    "# create a one_hot_pianoroll\n",
    "one_hot_pianoroll = midi_utils.one_hot_encode_pianoroll(normal_pianoroll, 128)*90 # 90 for the velocity\n",
    "# store it a in a track object\n",
    "new_track = Track(pianoroll=one_hot_pianoroll, name='new track')\n",
    "# create a multitrack made of the generated track object\n",
    "new_multitrack = Multitrack(tracks=[new_track], tempo = 90, beat_resolution=2)\n",
    "#write to midi file \n",
    "pypianoroll.write(new_multitrack, home_dir + \"/results/RNN_track\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4950,
     "status": "error",
     "timestamp": 1557344169303,
     "user": {
      "displayName": "Nicolas Deperrois",
      "photoUrl": "https://lh5.googleusercontent.com/-Jfkdb6w84hk/AAAAAAAAAAI/AAAAAAAACRA/bqUAiv0shAM/s64/photo.jpg",
      "userId": "11633758183366338150"
     },
     "user_tz": -120
    },
    "id": "n-06icmc4lMK",
    "outputId": "38bb75d2-c86b-41fb-e472-5d5f6c1c3131"
   },
   "outputs": [],
   "source": [
    "modified_midi_filename = home_dir + \"/results/RNN_track.mid\"\n",
    "pygame.init()\n",
    "pygame.mixer.music.load(modified_midi_filename)\n",
    "pygame.mixer.music.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MMWx1qQs4lML"
   },
   "outputs": [],
   "source": [
    "pygame.mixer.music.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train for harmonization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine boundaries in all tracks\n",
    "lower_notes = []\n",
    "upper_notes = []\n",
    "list_pianorolls = []\n",
    "list_pianorolls2 = []\n",
    "os.chdir(home_dir + \"/data/raw/bach\")  # go to a folder relative to home dir\n",
    "for midi_file in glob.glob(\"*.mid\"):\n",
    "    ## load midi file\n",
    "    csv_text = midi_utils.load_to_csv(midi_file)\n",
    "    # get semitones to C major \n",
    "    semitones,_ = midi_utils.get_semitones_to_C(csv_text)\n",
    "    # convert to multitrack object\n",
    "    multitrack = pypianoroll.parse(midi_file, beat_resolution=4)\n",
    "    # get the soprano track object and transpose it to C major \n",
    "    track = pypianoroll.transpose(multitrack.tracks[0], -semitones)\n",
    "    track2 = pypianoroll.transpose(multitrack.tracks[1], -semitones)\n",
    "    # get pitch range \n",
    "    pitch_range = track.get_active_pitch_range()\n",
    "    lower_notes.append(pitch_range[0])\n",
    "    upper_notes.append(pitch_range[1])\n",
    "    pitch_range2 = track2.get_active_pitch_range()\n",
    "    lower_notes.append(pitch_range2[0])\n",
    "    upper_notes.append(pitch_range2[1])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # get the flattened representation of pianoroll\n",
    "    pianoroll_flattened = midi_utils.flatten_one_hot_pianoroll(track.pianoroll)\n",
    "    pianoroll_flattened2 = midi_utils.flatten_one_hot_pianoroll(track2.pianoroll)\n",
    "    # add it to the global list of all tracks\n",
    "    list_pianorolls.append(pianoroll_flattened)\n",
    "    list_pianorolls2.append(pianoroll_flattened2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global lower note : 51\n",
      "Global upper note : 83\n",
      "Number of notes : 34\n"
     ]
    }
   ],
   "source": [
    "# convert into an array \n",
    "all_pianorolls = np.concatenate(list_pianorolls)\n",
    "all_pianorolls2 = np.concatenate(list_pianorolls2)\n",
    "# get lower and upper bounds \n",
    "global_lower = min(lower_notes)\n",
    "global_upper = max(upper_notes)\n",
    "global_diff = global_upper - global_lower\n",
    "n_notes = global_diff + 2 # we include the silence \n",
    "print('Global lower note : '+ str(global_lower))\n",
    "print('Global upper note : '+ str(global_upper))\n",
    "print('Number of notes : '+ str(n_notes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voice to train on :\n",
      "[ 0  0  0  0 70 70 70  0 70 70 70 70 68 68 68 68 66 66 66 66 65 65 65 65\n",
      " 65 65 65 65 63 63 63 63 63 63 63 63 70 70 70  0 70 70 70 70 72 72 72 72\n",
      " 74 74 74 74 75 75 75 75 75 75 75 75 74 74 74 74 74 74 74 74 75 75 75 75\n",
      " 78 78 78 78 77 77 77 77 77 77 75  0 75 75 75 75 75 75 75 75 75 75 75 75\n",
      " 75 75 75 75]\n",
      "Voice to target :\n",
      "[ 0  0  0  0 66 66 66  0 66 66 65 65 63 63 63  0 63 63 63  0 63 63 63 63\n",
      " 62 62 62 62 58 58 58 58 58 58 58 58 66 66 66  0 66 66 66 66 68 68 66 66\n",
      " 65 65 65 65 63 63 63 63 65 65 65  0 65 65 65 65 65 65 65 65 66 66 68 68\n",
      " 70 70 70  0 70 70 70 70 68 68 68 68 66 66 66 66 66 66 66 66 66 66 66 66\n",
      " 66 66 66  0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Voice to train on :\")\n",
    "print(all_pianorolls[0:100])\n",
    "print(\"Voice to target :\")\n",
    "print(all_pianorolls2[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled voice to train on :\n",
      "[ 0  0  0  0 20 20 20  0 20 20 20 20 18 18 18 18 16 16 16 16 15 15 15 15\n",
      " 15 15 15 15 13 13 13 13 13 13 13 13 20 20 20  0 20 20 20 20 22 22 22 22\n",
      " 24 24 24 24 25 25 25 25 25 25 25 25 24 24 24 24 24 24 24 24 25 25 25 25\n",
      " 28 28 28 28 27 27 27 27 27 27 25  0 25 25 25 25 25 25 25 25 25 25 25 25\n",
      " 25 25 25 25]\n",
      "Scaled voice to target :\n",
      "[ 0  0  0  0 16 16 16  0 16 16 15 15 13 13 13  0 13 13 13  0 13 13 13 13\n",
      " 12 12 12 12  8  8  8  8  8  8  8  8 16 16 16  0 16 16 16 16 18 18 16 16\n",
      " 15 15 15 15 13 13 13 13 15 15 15  0 15 15 15 15 15 15 15 15 16 16 18 18\n",
      " 20 20 20  0 20 20 20 20 18 18 18 18 16 16 16 16 16 16 16 16 16 16 16 16\n",
      " 16 16 16  0]\n"
     ]
    }
   ],
   "source": [
    "# scale pianoroll to 0 \n",
    "all_pianorolls_scaled = midi_utils.scale_pianoroll(all_pianorolls, global_lower)\n",
    "print(\"Scaled voice to train on :\")\n",
    "print(all_pianorolls_scaled[0:100])\n",
    "all_pianorolls_scaled2 = midi_utils.scale_pianoroll(all_pianorolls2, global_lower)\n",
    "print(\"Scaled voice to target :\")\n",
    "print(all_pianorolls_scaled2[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NoteRNN(\n",
      "  (lstm): LSTM(28, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (fc): Linear(in_features=256, out_features=28, bias=True)\n",
      ")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-df189782393a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_harmonization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_pianorolls_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_pianorolls_scaled2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"harmonization\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-65-3afb9128c846>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, data, data2, mode, epochs, batch_size, seq_length, lr, clip, val_frac, print_every)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;31m# get the output from the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;31m# calculate the loss and perform backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/BERN/ATML/myenv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-64-0da7193cdfc3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, hidden)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m#get the outputs and the new hidden state from the lstm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mr_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m#pass through a dropout layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/BERN/ATML/myenv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/BERN/ATML/myenv/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             result = _impl(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 179\u001b[0;31m                            self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             result = _impl(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define and print the net\n",
    "n_hidden=256\n",
    "n_layers=2\n",
    "\n",
    "net_harmonization = NoteRNN(n_notes, n_hidden, n_layers)\n",
    "print(net)\n",
    "\n",
    "# Declaring the hyperparameters\n",
    "batch_size = 16\n",
    "seq_length = 100\n",
    "n_epochs = 20 # start smaller if you are just testing initial behavior\n",
    "\n",
    "# train the model\n",
    "train(net_harmonization, all_pianorolls_scaled, all_pianorolls_scaled2, mode=\"harmonization\", epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aZW3uAKC4lMM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semitones to C major : 2\n",
      "Soprano pianoroll :\n",
      "[ 0  0  0  0 69 69 71 71 72 72 72 72 71 71 71 71 69 69 69  0 69 69 71 71\n",
      " 72 72 72 72 71 71 71 71 69 69 69 69 76 76 76 76 74 74 74 74 72 72 72 72\n",
      " 71 71 71  0 71 71 71 71 72 72 72  0 72 72 72 72 74 74 74  0 74 74 74 74\n",
      " 76 76 76  0 76 76 76 76 74 74 74 74 72 72 72 72 71 71 71 71 71 71 71 71\n",
      " 69 69 69 69 76 76 76 76 74 74 74 74 72 72 72 72 71 71 71 71 76 76 76 76\n",
      " 74 74 74 74 72 72 72 72 71 71 71  0 71 71 71 71 72 72 72  0 72 72 72 72\n",
      " 74 74 74  0 74 74 74 74 76 76 76  0 76 76 76 76 74 74 74 74 72 72 72 72\n",
      " 71 71 71 71 71 71 71 71 69 69 69 69]\n",
      "Alto pianoroll :\n",
      "[51 51 51 51 53 56 59 63 62 65 76 70 68 58 71 72 65 67 64 63 71 61 74  0\n",
      " 69 66 62 64 60 69 61 73 66 65 58 56 63 58 60 67 68 68 60 72 66 66 64 65\n",
      " 60  0 65 58 59 67 64 65 65 59 71 66 68 65 64 63 59  0  0 71 64 60 67 63\n",
      " 64 64 65 60 67 67 63 59 62 60 65 58 62 72 63 57 58 64 63 65 63 67 71 62\n",
      " 54  0 61 64 63 64 60 68 65]\n"
     ]
    }
   ],
   "source": [
    "# Generating harmonization\n",
    "\n",
    "# get one voice from the dataset \n",
    "midi_filename = 'bwv110.7.mid'\n",
    "csv_text = midi_utils.load_to_csv(midi_filename)\n",
    "# get semitones to C major \n",
    "semitones,_ = midi_utils.get_semitones_to_C(csv_text)\n",
    "print('Semitones to C major : ' +str(semitones))\n",
    "# get multitrack object from midi \n",
    "multitrack = pypianoroll.parse(midi_filename, beat_resolution=4)\n",
    "# get the soprano track\n",
    "soprano_track = multitrack.tracks[0]\n",
    "# transpose the track to C Major \n",
    "soprano_track = pypianoroll.transpose(soprano_track,-semitones)\n",
    "soprano_pianoroll = midi_utils.flatten_one_hot_pianoroll(transposed_track_soprano.pianoroll)\n",
    "print('Soprano pianoroll :')\n",
    "print(soprano_pianoroll)\n",
    "scaled_soprano_pianoroll = midi_utils.scale_pianoroll(soprano_pianoroll, global_lower)\n",
    "# now we can predict the second voice \n",
    "scaled_alto_pianoroll = sample_harmonization(net, scaled_soprano_pianoroll)\n",
    "\n",
    "alto_pianoroll = midi_utils.unscale_pianoroll(notes, global_lower)\n",
    "print('Alto pianoroll :')\n",
    "print(alto_pianoroll)\n",
    "one_hot_alto = midi_utils.one_hot_encode_pianoroll(alto_pianoroll, 128)*90\n",
    "alto_track = Track(pianoroll=one_hot_alto, name='alto gernerated track')\n",
    "# create a multitrack made of the initial soprano track and the generated alto\n",
    "new_multitrack = Multitrack(tracks=[soprano_track, alto_track], tempo = 90, beat_resolution=4)\n",
    "#write to midi file \n",
    "pypianoroll.write(new_multitrack, home_dir + \"/results/RNN_harmonization_track\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_midi_filename = home_dir + \"/results/RNN_harmonization_track.mid\"\n",
    "pygame.init()\n",
    "pygame.mixer.music.load(modified_midi_filename)\n",
    "pygame.mixer.music.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.mixer.music.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "train_RNN_pianorolls.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
