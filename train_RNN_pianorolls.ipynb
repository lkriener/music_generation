{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mEpS7y23emOr"
   },
   "outputs": [],
   "source": [
    "#rm -r *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7455,
     "status": "ok",
     "timestamp": 1557735083676,
     "user": {
      "displayName": "Nicolas Deperrois",
      "photoUrl": "https://lh5.googleusercontent.com/-Jfkdb6w84hk/AAAAAAAAAAI/AAAAAAAACRA/bqUAiv0shAM/s64/photo.jpg",
      "userId": "11633758183366338150"
     },
     "user_tz": -120
    },
    "id": "kfpjADwL4oUx",
    "outputId": "04fa8a9c-f4c5-4955-e304-c50d8bc64c91"
   },
   "outputs": [],
   "source": [
    "# Uncomment the line below and run this cell to get your data from github into colab (only runnable in colab, not ordinary jupyter notebook):\n",
    "#! git clone https://github.com/lkriener/music_generation.git && mv music_generation/* . && rm music_generation -r "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8741,
     "status": "ok",
     "timestamp": 1557735096663,
     "user": {
      "displayName": "Nicolas Deperrois",
      "photoUrl": "https://lh5.googleusercontent.com/-Jfkdb6w84hk/AAAAAAAAAAI/AAAAAAAACRA/bqUAiv0shAM/s64/photo.jpg",
      "userId": "11633758183366338150"
     },
     "user_tz": -120
    },
    "id": "sLhyMMRj4tNz",
    "outputId": "383a73d9-1275-45aa-f84b-39404255841a"
   },
   "outputs": [],
   "source": [
    "# Uncomment line to install requirements\n",
    "#! pip install -r colab_requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EH9lnM84OBTV"
   },
   "source": [
    "# Music generation using note-to-note LSTMs \n",
    "In this part, we will try to generate melodies from the same dataset (Bach Chorals) using recurrent neural networks (RNN) with long short term memory units (LSTM) as in the tutorial 08 of the class, based on the two following blog posts:\n",
    "\n",
    "- http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "- https://towardsdatascience.com/writing-like-shakespeare-with-machine-learning-in-pytorch-d77f851d910c\n",
    "\n",
    "The idea would be to extend the character-to-character LSTM network to midi files. We will also use another representation of the data, namely 'pianorolls', where a single melody is stored in a matrix with the x-axis for time and the y-axis for the pitch. This is one of the most frequent representations used, although it has some limitations. For instance, there is no way to differentiate between a long note (1 half-note) and 4 short-ones (4 eight-notes). However, it brings a considerable advantage compared to the previous used representations as it can store rhythm, depending on the chosen time-step. \n",
    "\n",
    "Moreover, each possible note is considered as a distinct element of a vocabulary. We will then use $N$ input nodes, where $N$ is the size of the vocabulary (i.e., the number of distinct notes, including silences). \n",
    "Thus, these pianorolls arrays will be converted into one-hot vectors containing the corresponding pitch (or silence, represented by 0) for each time step, and will be fed to the RNN network defined below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HygKqpcn-_Ih"
   },
   "source": [
    "## Test of pypianoroll library\n",
    "We will first use the pypianoroll library - https://salu133445.github.io/pypianoroll/ - to convert a midi file into a multitrack object, get the soprano track, transpose it to C major, create a new multitrack object out of it, and finally write it to a new midi file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4917,
     "status": "ok",
     "timestamp": 1557735104467,
     "user": {
      "displayName": "Nicolas Deperrois",
      "photoUrl": "https://lh5.googleusercontent.com/-Jfkdb6w84hk/AAAAAAAAAAI/AAAAAAAACRA/bqUAiv0shAM/s64/photo.jpg",
      "userId": "11633758183366338150"
     },
     "user_tz": -120
    },
    "id": "0DaVKNWw4lLr",
    "outputId": "32aa011e-9184-4b4f-9b0d-1b9143f8ad16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pygame\n",
    "from pypianoroll import Multitrack, Track\n",
    "import pypianoroll\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, training on CPU; consider making n_epochs very small.\n"
     ]
    }
   ],
   "source": [
    "from src.RNN_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IugBxMoP4lLy"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    home_dir\n",
    "except NameError:\n",
    "    home_dir = os.getcwd()\n",
    "\n",
    "os.chdir(home_dir + \"/data/raw/bach\")  # go to a folder relative to home dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BrAiT6ot4lL2"
   },
   "outputs": [],
   "source": [
    "from src.dataset_utils import TrackDataset, get_dataset_representation_from_tracks\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "feature_scaler = StandardScaler()\n",
    "label_scaler = StandardScaler()\n",
    "\n",
    "# iterate over all midi files of folder\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    home_dir\n",
    "except NameError:\n",
    "    home_dir = os.getcwd()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 6137
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7721,
     "status": "ok",
     "timestamp": 1557736557625,
     "user": {
      "displayName": "Nicolas Deperrois",
      "photoUrl": "https://lh5.googleusercontent.com/-Jfkdb6w84hk/AAAAAAAAAAI/AAAAAAAACRA/bqUAiv0shAM/s64/photo.jpg",
      "userId": "11633758183366338150"
     },
     "user_tz": -120
    },
    "id": "8EnWFvKA4lL7",
    "outputId": "66c9c145-8d86-42c4-d8ce-ece64a7b1c26"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deperrois/Documents/BERN/ATML/myenv/lib/python3.7/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global lower note : 57\n",
      "Global upper note : 83\n",
      "Number of notes : 28\n"
     ]
    }
   ],
   "source": [
    "# get lower and upper bounds \n",
    "voice = 0 \n",
    "all_pianorolls = get_all_pianorolls(voice, home_dir, beat_resolution=4)\n",
    "global_lower, global_upper, n_notes = get_extremum_pitches([all_pianorolls])\n",
    "print('Global lower note : '+ str(global_lower))\n",
    "print('Global upper note : '+ str(global_upper))\n",
    "print('Number of notes : '+ str(n_notes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3759,
     "status": "ok",
     "timestamp": 1557735893582,
     "user": {
      "displayName": "Nicolas Deperrois",
      "photoUrl": "https://lh5.googleusercontent.com/-Jfkdb6w84hk/AAAAAAAAAAI/AAAAAAAACRA/bqUAiv0shAM/s64/photo.jpg",
      "userId": "11633758183366338150"
     },
     "user_tz": -120
    },
    "id": "P3Tts_gs4lL9",
    "outputId": "a58a687c-5199-4f46-c51b-d8e8de534107"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pianoroll sequence of soprano :\n",
      "[ 0  0  0  0 70 70 70  0 70 70 70 70 68 68 68 68 66 66 66 66 65 65 65 65\n",
      " 65 65 65 65 63 63 63 63 63 63 63 63 70 70 70  0 70 70 70 70 72 72 72 72\n",
      " 74 74 74 74 75 75 75 75 75 75 75 75 74 74 74 74 74 74 74 74 75 75 75 75\n",
      " 78 78 78 78 77 77 77 77 77 77 75  0 75 75 75 75 75 75 75 75 75 75 75 75\n",
      " 75 75 75 75]\n"
     ]
    }
   ],
   "source": [
    "print(\"Pianoroll sequence of soprano :\")\n",
    "print(all_pianorolls[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rzfuMYvgZ6if"
   },
   "source": [
    "In order to minimize the size of one-hot encode vectors, we restrict the pitches to values next to 0, using the minimum pitch found in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 453,
     "status": "ok",
     "timestamp": 1557735895623,
     "user": {
      "displayName": "Nicolas Deperrois",
      "photoUrl": "https://lh5.googleusercontent.com/-Jfkdb6w84hk/AAAAAAAAAAI/AAAAAAAACRA/bqUAiv0shAM/s64/photo.jpg",
      "userId": "11633758183366338150"
     },
     "user_tz": -120
    },
    "id": "ayfyARad4lL_",
    "outputId": "c0cdc6c4-5f48-4250-94ed-c067ac0f7c57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pianoroll sequence of soprano scaled to 0:\n",
      "[ 0  0  0  0 14 14 14  0 14 14 14 14 12 12 12 12 10 10 10 10  9  9  9  9\n",
      "  9  9  9  9  7  7  7  7  7  7  7  7 14 14 14  0 14 14 14 14 16 16 16 16\n",
      " 18 18 18 18 19 19 19 19 19 19 19 19 18 18 18 18 18 18 18 18 19 19 19 19\n",
      " 22 22 22 22 21 21 21 21 21 21 19  0 19 19 19 19 19 19 19 19 19 19 19 19\n",
      " 19 19 19 19]\n"
     ]
    }
   ],
   "source": [
    "# scale pianoroll to 0 \n",
    "print(\"Pianoroll sequence of soprano scaled to 0:\")\n",
    "all_pianorolls_scaled = scale_pianoroll(all_pianorolls, global_lower)\n",
    "print(all_pianorolls_scaled[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "42JsbRVtN7L-"
   },
   "source": [
    "## Generation of a single voice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7nbdxBCkXycc"
   },
   "source": [
    "As a first attempt to generate a melody using LSTMs with pianorolls, we will simply try to generate a single voice (soprano, alto, tenor or bass), training the network on the whole midi file dataset concatenated into a long sequence of notes all translated to the same tonality (here, C Major or its relative A minor). The later operation will particularly facilitate learning on such a small dataset as all melodies will be rescaled to the same tonality, thus preventing abrupt changes among melody batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 430,
     "status": "ok",
     "timestamp": 1557735897116,
     "user": {
      "displayName": "Nicolas Deperrois",
      "photoUrl": "https://lh5.googleusercontent.com/-Jfkdb6w84hk/AAAAAAAAAAI/AAAAAAAACRA/bqUAiv0shAM/s64/photo.jpg",
      "userId": "11633758183366338150"
     },
     "user_tz": -120
    },
    "id": "b37NFyC-4lMB",
    "outputId": "15259288-dadf-4c5a-fb68-7a75f0037d9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, training on CPU; consider making n_epochs very small.\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mrVryIjJ4lMC"
   },
   "source": [
    "### Define a LSTM network as for note-to-note melody generation\n",
    "We now build a network to implement note-to-note melody generation using LSTMs units as well as drop-out of the output. This network is highly inspired from the Tutorial 08 of the class, by replacing characters by notes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GxUeYhz24lME"
   },
   "source": [
    "## Training Code\n",
    "\n",
    "For training the model, we declare a function, where we define an optimizer (Adam) and loss (cross entropy). Then, training and validation data are separated and the hidden state of the RNN is initaliazed. Looping over the training melody batches, we use the functions `get_pianoroll_batches` and `one_hot_encode_batch` in order to build batches and feed them to the network input. This function is also adapted to harmonization that will be defined below. \n",
    "\n",
    "Every once a while, we generate some loss statistics (training loss and validation loss) to let us know if the model is training correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 44868,
     "status": "ok",
     "timestamp": 1557751901275,
     "user": {
      "displayName": "Nicolas Deperrois",
      "photoUrl": "https://lh5.googleusercontent.com/-Jfkdb6w84hk/AAAAAAAAAAI/AAAAAAAACRA/bqUAiv0shAM/s64/photo.jpg",
      "userId": "11633758183366338150"
     },
     "user_tz": -120
    },
    "id": "_Gr3LZSO4lMF",
    "outputId": "92e9db52-63b5-4a4a-e9c2-5d55c1597df2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NoteRNN(\n",
      "  (lstm): LSTM(28, 128, num_layers=3, batch_first=True, dropout=0.2)\n",
      "  (dropout): Dropout(p=0.2)\n",
      "  (fc): Linear(in_features=128, out_features=28, bias=True)\n",
      ")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-222778c705f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_pianorolls_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/BERN/ATML/music_generation/src/RNN_utils.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, data, data2, mode, epochs, batch_size, seq_length, lr, clip, val_frac, print_every)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;31m# get the output from the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0;31m# calculate the loss and perform backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/BERN/ATML/myenv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/BERN/ATML/music_generation/src/RNN_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, hidden)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;31m#get the outputs and the new hidden state from the lstm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mr_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m         \u001b[0;31m#pass through a dropout layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/BERN/ATML/myenv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/BERN/ATML/myenv/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             result = _impl(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 179\u001b[0;31m                            self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             result = _impl(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define and print the net\n",
    "n_hidden=128\n",
    "n_layers=3\n",
    "\n",
    "net = NoteRNN(n_notes, n_hidden, n_layers)\n",
    "print(net)\n",
    "\n",
    "# Declaring the hyperparameters\n",
    "batch_size = 32\n",
    "seq_length = 100\n",
    "n_epochs = 20 # start smaller if you are just testing initial behavior\n",
    "\n",
    "# train the model\n",
    "train(net, all_pianorolls_scaled, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Un2-EJ8kyZyy"
   },
   "source": [
    "### Test of the network: generation of single voice \n",
    "We will now test the trained network to predict potential melodies starting from a short sequence of notes. Here again, the functions `predict` and `sample` are highly inspired from the Tutorial 8, and slightly modified towards pianorolls data processing. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "om0ch5hlz7oC"
   },
   "source": [
    "The `NoteRNN` network outputs scaled pianorolls. We thus need to send back the obtained pitches to the associated voice range, using the function `unscale_pianoroll`. We then store the pianoroll to a track object, and to a multitrack object, which can directly be used to write a midi file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 507,
     "status": "ok",
     "timestamp": 1557736047963,
     "user": {
      "displayName": "Nicolas Deperrois",
      "photoUrl": "https://lh5.googleusercontent.com/-Jfkdb6w84hk/AAAAAAAAAAI/AAAAAAAACRA/bqUAiv0shAM/s64/photo.jpg",
      "userId": "11633758183366338150"
     },
     "user_tz": -120
    },
    "id": "dDlgi2OM4lMJ",
    "outputId": "50a82c65-5ec3-479a-c084-4608bbc80279"
   },
   "outputs": [],
   "source": [
    "# Generating new melody\n",
    "start = 72 - global_lower + 1 # starting with a C \n",
    "notes = sample(net, 200, prime=[start,start,start+2,start+2])\n",
    "print(\"Generated sample :\")\n",
    "print(notes)\n",
    "normal_pianoroll = unscale_pianoroll(notes, global_lower) # go back to the track range\n",
    "print(\"Generated sample in the pitch range :\")\n",
    "print(normal_pianoroll)\n",
    "# create a one_hot_pianoroll\n",
    "one_hot_pianoroll = one_hot_encode_pianoroll(normal_pianoroll, 128)*90 # 90 for the velocity\n",
    "# store it a in a track object\n",
    "new_track = Track(pianoroll=one_hot_pianoroll, name='new track')\n",
    "# create a multitrack made of the generated track object\n",
    "new_multitrack = Multitrack(tracks=[new_track], tempo = 90, beat_resolution=4)\n",
    "#write to midi file \n",
    "pypianoroll.write(new_multitrack, home_dir + \"/results/RNN_track\")\n",
    "modified_midi_filename = home_dir + \"/results/RNN_track.mid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a1zoH626TqKk"
   },
   "outputs": [],
   "source": [
    "!apt install fluidsynth\n",
    "!cp /usr/share/sounds/sf2/FluidR3_GM.sf2 ./font.sf2\n",
    "!pip install midi2audio\n",
    "from midi2audio import FluidSynth\n",
    "from IPython.display import Audio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52,
     "output_embedded_package_id": "17Q9VVEcvm_RFRKv4uriNqz1J7Bo_1P1O"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7806,
     "status": "ok",
     "timestamp": 1557736059023,
     "user": {
      "displayName": "Nicolas Deperrois",
      "photoUrl": "https://lh5.googleusercontent.com/-Jfkdb6w84hk/AAAAAAAAAAI/AAAAAAAACRA/bqUAiv0shAM/s64/photo.jpg",
      "userId": "11633758183366338150"
     },
     "user_tz": -120
    },
    "id": "t7_AvsYET_TX",
    "outputId": "6206eddd-1f29-4dd7-ccf7-d2f2ac5d2ac2"
   },
   "outputs": [],
   "source": [
    "FluidSynth(\"font.sf2\").midi_to_audio(modified_midi_filename, 'test.wav')\n",
    "Audio(\"test.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n-06icmc4lMK"
   },
   "outputs": [],
   "source": [
    "\n",
    "pygame.init()\n",
    "pygame.mixer.music.load(modified_midi_filename)\n",
    "pygame.mixer.music.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 511,
     "status": "error",
     "timestamp": 1557504269655,
     "user": {
      "displayName": "Nicolas Deperrois",
      "photoUrl": "https://lh5.googleusercontent.com/-Jfkdb6w84hk/AAAAAAAAAAI/AAAAAAAACRA/bqUAiv0shAM/s64/photo.jpg",
      "userId": "11633758183366338150"
     },
     "user_tz": -120
    },
    "id": "MMWx1qQs4lML",
    "outputId": "80d34808-4e3b-41d1-a467-6e0af08633bd"
   },
   "outputs": [],
   "source": [
    "pygame.mixer.music.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vEo6s6GV-_I3"
   },
   "source": [
    "## Train for harmonization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qkoVhdWWEIsz"
   },
   "source": [
    "We observed above that it is possible to generate a coherent melody using a slightly modified version of the original character-to-character RNN. Here, we will investigate whether such a network can perform more complex task, such as harmonizing a given voice. For instance, from the soprano sequence, we would like to generate the associated alto voice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NQSTEDZrV70O"
   },
   "outputs": [],
   "source": [
    "beat_resolution = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KK_JGdyb-_I4"
   },
   "outputs": [],
   "source": [
    "\n",
    "all_pianorolls_soprano = get_all_pianorolls(0, home_dir, beat_resolution=beat_resolution)\n",
    "all_pianorolls_alto = get_all_pianorolls(1, home_dir, beat_resolution=beat_resolution)\n",
    "all_pianorolls_tenor = get_all_pianorolls(2, home_dir, beat_resolution=beat_resolution)\n",
    "all_pianorolls_bass = get_all_pianorolls(3, home_dir, beat_resolution=beat_resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 28431,
     "status": "ok",
     "timestamp": 1557750317882,
     "user": {
      "displayName": "Nicolas Deperrois",
      "photoUrl": "https://lh5.googleusercontent.com/-Jfkdb6w84hk/AAAAAAAAAAI/AAAAAAAACRA/bqUAiv0shAM/s64/photo.jpg",
      "userId": "11633758183366338150"
     },
     "user_tz": -120
    },
    "id": "A1XAhpRy-_I4",
    "outputId": "5316e472-6a4a-4741-d95e-50ab3cd2a2d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global lower note : 35\n",
      "Global upper note : 83\n",
      "Number of notes : 50\n"
     ]
    }
   ],
   "source": [
    "list_pianorolls = [all_pianorolls_soprano, all_pianorolls_alto, all_pianorolls_tenor, all_pianorolls_bass]\n",
    "global_lower, global_upper, n_notes = get_extremum_pitches(list_pianorolls)\n",
    "\n",
    "\n",
    "print('Global lower note : '+ str(global_lower))\n",
    "print('Global upper note : '+ str(global_upper))\n",
    "print('Number of notes : '+ str(n_notes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2wBxcVc3VCOn"
   },
   "source": [
    "We now rescale the pianorolls to values next to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SvlOqRm3-_I7"
   },
   "outputs": [],
   "source": [
    "# scale pianoroll to 0 \n",
    "all_pianorolls_soprano = scale_pianoroll(all_pianorolls_soprano, global_lower)\n",
    "all_pianorolls_alto = scale_pianoroll(all_pianorolls_alto, global_lower)\n",
    "all_pianorolls_tenor = scale_pianoroll(all_pianorolls_tenor, global_lower)\n",
    "all_pianorolls_bass = scale_pianoroll(all_pianorolls_bass, global_lower)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G5JMisjMIm2A"
   },
   "source": [
    "It is now time to train the network to perform harmonization. Setting the mode to `\"harmonization\"`, we will call the function `get_pianoroll_batches_harmonization` that generates one-hot vectors of both input and target melodies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 66642,
     "status": "ok",
     "timestamp": 1557755548460,
     "user": {
      "displayName": "Nicolas Deperrois",
      "photoUrl": "https://lh5.googleusercontent.com/-Jfkdb6w84hk/AAAAAAAAAAI/AAAAAAAACRA/bqUAiv0shAM/s64/photo.jpg",
      "userId": "11633758183366338150"
     },
     "user_tz": -120
    },
    "id": "bAWv5b1A-_I8",
    "outputId": "bdbbd85e-3c34-462a-93e0-eb05758d1db7"
   },
   "outputs": [],
   "source": [
    "# Define and print the net\n",
    "n_hidden=512\n",
    "n_layers=2\n",
    "\n",
    "net_harmonization_alto = NoteRNN(n_notes, n_hidden, n_layers)\n",
    "net_harmonization_tenor = NoteRNN(n_notes, n_hidden, n_layers)\n",
    "net_harmonization_bass = NoteRNN(n_notes, n_hidden, n_layers)\n",
    "\n",
    "\n",
    "\n",
    "# Declaring the hyperparameters\n",
    "batch_size = 64\n",
    "seq_length = 50\n",
    "n_epochs = 100 # start smaller if you are just testing initial behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 66642,
     "status": "ok",
     "timestamp": 1557755548460,
     "user": {
      "displayName": "Nicolas Deperrois",
      "photoUrl": "https://lh5.googleusercontent.com/-Jfkdb6w84hk/AAAAAAAAAAI/AAAAAAAACRA/bqUAiv0shAM/s64/photo.jpg",
      "userId": "11633758183366338150"
     },
     "user_tz": -120
    },
    "id": "bAWv5b1A-_I8",
    "outputId": "bdbbd85e-3c34-462a-93e0-eb05758d1db7"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-892dac401e82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_harmonization_alto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_pianorolls_soprano\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_pianorolls_alto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"harmonization\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/BERN/ATML/music_generation/src/RNN_utils.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, data, data2, mode, epochs, batch_size, seq_length, lr, clip, val_frac, print_every)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;31m# calculate the loss and perform backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0;31m# `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/BERN/ATML/myenv/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/BERN/ATML/myenv/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "train(net_harmonization_alto, data=all_pianorolls_soprano, data2=all_pianorolls_alto, mode=\"harmonization\", epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.005, print_every=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 131586,
     "status": "ok",
     "timestamp": 1557755614518,
     "user": {
      "displayName": "Nicolas Deperrois",
      "photoUrl": "https://lh5.googleusercontent.com/-Jfkdb6w84hk/AAAAAAAAAAI/AAAAAAAACRA/bqUAiv0shAM/s64/photo.jpg",
      "userId": "11633758183366338150"
     },
     "user_tz": -120
    },
    "id": "ZkYgqAfIeBJb",
    "outputId": "b096aae6-9fcd-4afa-8b9b-7a298f48f17c"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-2a0d3fe20d76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_harmonization_tenor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_pianorolls_soprano\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_pianorolls_tenor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"harmonization\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/BERN/ATML/music_generation/src/RNN_utils.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, data, data2, mode, epochs, batch_size, seq_length, lr, clip, val_frac, print_every)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;31m# calculate the loss and perform backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0;31m# `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/BERN/ATML/myenv/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/BERN/ATML/myenv/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(net_harmonization_tenor, data=all_pianorolls_soprano, data2=all_pianorolls_tenor, mode=\"harmonization\", epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.005, print_every=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 196940,
     "status": "ok",
     "timestamp": 1557755680363,
     "user": {
      "displayName": "Nicolas Deperrois",
      "photoUrl": "https://lh5.googleusercontent.com/-Jfkdb6w84hk/AAAAAAAAAAI/AAAAAAAACRA/bqUAiv0shAM/s64/photo.jpg",
      "userId": "11633758183366338150"
     },
     "user_tz": -120
    },
    "id": "SoQKIHoPeJDU",
    "outputId": "ffb364e1-dca6-471b-a54e-8aa49fa08e29"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-57a27d3c0fe7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_harmonization_bass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_pianorolls_soprano\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_pianorolls_bass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"harmonization\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/BERN/ATML/music_generation/src/RNN_utils.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, data, data2, mode, epochs, batch_size, seq_length, lr, clip, val_frac, print_every)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;31m# calculate the loss and perform backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0;31m# `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/BERN/ATML/myenv/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/BERN/ATML/myenv/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(net_harmonization_bass, data=all_pianorolls_soprano, data2=all_pianorolls_bass, mode=\"harmonization\", epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.005, print_every=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aZW3uAKC4lMM"
   },
   "outputs": [],
   "source": [
    " # Generating harmonization\n",
    "\n",
    "os.chdir(home_dir + \"/data/raw/bach\")  # go to a folder relative to home dir\n",
    "# get one voice from the dataset \n",
    "#midi_filename = 'bwv368.mid'\n",
    "midi_filename = 'bwv155.5.mid'\n",
    "#midi_filename = 'pkgsc_azalea.mid'\n",
    "soprano_track = get_track(midi_filename, 0, beat_resolution=beat_resolution, transpose=True)\n",
    "alto_track_real = get_track(midi_filename, 1, beat_resolution=beat_resolution, transpose=True)\n",
    "tenor_track_real = get_track(midi_filename, 2, beat_resolution=beat_resolution, transpose=True)\n",
    "bass_track_real = get_track(midi_filename, 3, beat_resolution=beat_resolution, transpose=True)\n",
    "\n",
    "soprano_pianoroll = scale_pianoroll(flatten_one_hot_pianoroll(soprano_track.pianoroll), global_lower)\n",
    "alto_pianoroll_real = scale_pianoroll(flatten_one_hot_pianoroll(alto_track_real.pianoroll), global_lower)\n",
    "tenor_pianoroll_real = scale_pianoroll(flatten_one_hot_pianoroll(tenor_track_real.pianoroll), global_lower)\n",
    "bass_pianoroll_real = scale_pianoroll(flatten_one_hot_pianoroll(bass_track_real.pianoroll), global_lower)\n",
    "\n",
    "\n",
    "# now we can predict the second voice \n",
    "alto_pianoroll = sample_harmonization(net_harmonization_alto, soprano_pianoroll, prime = alto_pianoroll_real[:4])\n",
    "\n",
    "tenor_pianoroll = sample_harmonization(net_harmonization_tenor, soprano_pianoroll, prime = tenor_pianoroll_real[:4])\n",
    "bass_pianoroll = sample_harmonization(net_harmonization_bass, soprano_pianoroll, prime = bass_pianoroll_real[:4])\n",
    "\n",
    "# go back to the pitch range \n",
    "alto_pianoroll = unscale_pianoroll(alto_pianoroll, global_lower)\n",
    "tenor_pianoroll = unscale_pianoroll(tenor_pianoroll, global_lower)\n",
    "bass_pianoroll = unscale_pianoroll(bass_pianoroll, global_lower)\n",
    "\n",
    "# convert to one-hot representation for track object\n",
    "alto_onehot = one_hot_encode_pianoroll(alto_pianoroll, 128)*90\n",
    "tenor_onehot = one_hot_encode_pianoroll(tenor_pianoroll, 128)*90\n",
    "bass_onehot = one_hot_encode_pianoroll(bass_pianoroll, 128)*90\n",
    "\n",
    "alto_track = Track(pianoroll=alto_onehot, name='alto gernerated track')\n",
    "tenor_track = Track(pianoroll=tenor_onehot, name='tenor gernerated track')\n",
    "bass_track = Track(pianoroll=bass_onehot, name='bass gernerated track')\n",
    "# create a multitrack made of the initial soprano track and the generated alto\n",
    "new_multitrack = Multitrack(tracks=[soprano_track, alto_track, tenor_track, bass_track], tempo = 90, beat_resolution=beat_resolution)\n",
    "\n",
    "multitrack_real = Multitrack(tracks=[soprano_track, alto_track_real, tenor_track_real, bass_track_real], tempo = 90, beat_resolution=beat_resolution)\n",
    "#write to midi file \n",
    "pypianoroll.write(new_multitrack, home_dir + \"/results/RNN_harmonization_track\")\n",
    "pypianoroll.write(multitrack_real, home_dir + \"/results/RNN_harmonization_track_real\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52,
     "output_embedded_package_id": "1OeU-pRYyjtywIyK-aeRIVgI6klQjpSLI"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9583,
     "status": "ok",
     "timestamp": 1557755842254,
     "user": {
      "displayName": "Nicolas Deperrois",
      "photoUrl": "https://lh5.googleusercontent.com/-Jfkdb6w84hk/AAAAAAAAAAI/AAAAAAAACRA/bqUAiv0shAM/s64/photo.jpg",
      "userId": "11633758183366338150"
     },
     "user_tz": -120
    },
    "id": "78YbppxNUxt-",
    "outputId": "23ab0bf2-dcdb-477e-9d1e-a99200ad25f3"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FluidSynth' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-8a940a9a9c91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodified_midi_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhome_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/results/RNN_harmonization_track.mid\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mFluidSynth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"font.sf2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmidi_to_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodified_midi_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test_fake.wav'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mAudio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test_fake.wav\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'FluidSynth' is not defined"
     ]
    }
   ],
   "source": [
    "modified_midi_filename = home_dir + \"/results/RNN_harmonization_track.mid\"\n",
    "FluidSynth(\"font.sf2\").midi_to_audio(modified_midi_filename, 'test_fake.wav')\n",
    "Audio(\"test_fake.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52,
     "output_embedded_package_id": "1Dg-ggnXAcJypf4RWoa0K1A3yyD_6JUYt"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12527,
     "status": "ok",
     "timestamp": 1557755819568,
     "user": {
      "displayName": "Nicolas Deperrois",
      "photoUrl": "https://lh5.googleusercontent.com/-Jfkdb6w84hk/AAAAAAAAAAI/AAAAAAAACRA/bqUAiv0shAM/s64/photo.jpg",
      "userId": "11633758183366338150"
     },
     "user_tz": -120
    },
    "id": "zAfF0mMcbHDr",
    "outputId": "556f4fea-d5cf-4627-b162-4c68d324d9c1"
   },
   "outputs": [],
   "source": [
    "modified_midi_filename_real = home_dir + \"/results/RNN_harmonization_track_real.mid\"\n",
    "FluidSynth(\"font.sf2\").midi_to_audio(modified_midi_filename_real, 'test_real.wav')\n",
    "Audio(\"test_real.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 64606,
     "status": "error",
     "timestamp": 1557736711266,
     "user": {
      "displayName": "Nicolas Deperrois",
      "photoUrl": "https://lh5.googleusercontent.com/-Jfkdb6w84hk/AAAAAAAAAAI/AAAAAAAACRA/bqUAiv0shAM/s64/photo.jpg",
      "userId": "11633758183366338150"
     },
     "user_tz": -120
    },
    "id": "D9c_NQ5q-_I-",
    "outputId": "a75b4d5a-cfe0-4aa2-b29d-f55b1b66927d"
   },
   "outputs": [],
   "source": [
    "modified_midi_filename = home_dir + \"/results/RNN_harmonization_track.mid\"\n",
    "pygame.init()\n",
    "pygame.mixer.music.load(modified_midi_filename)\n",
    "pygame.mixer.music.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 663,
     "status": "error",
     "timestamp": 1557504611177,
     "user": {
      "displayName": "Nicolas Deperrois",
      "photoUrl": "https://lh5.googleusercontent.com/-Jfkdb6w84hk/AAAAAAAAAAI/AAAAAAAACRA/bqUAiv0shAM/s64/photo.jpg",
      "userId": "11633758183366338150"
     },
     "user_tz": -120
    },
    "id": "MpStKEvO-_I-",
    "outputId": "354ac2d5-19ad-4310-8e4a-a920b4a68dcc"
   },
   "outputs": [],
   "source": [
    "pygame.mixer.music.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "562Tt2rR-_I_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "train_RNN_pianorolls_2.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
